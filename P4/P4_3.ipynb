{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NOTEBOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"./figures/LogoOpenclassrooms.png\">\n",
    "<font size=\"4\">\n",
    "<p>\n",
    "Cette étude a été réalisée dans le cadre du 4eme projet de ma formation Datascientist dispensée en MOOC par \n",
    "**<font color='blus'>Openclassrooms / écoles Centrale-Supélec</font>**.\n",
    "</p>    \n",
    "\n",
    "<p>\n",
    "Ce notebook présente un modèle de prédiction des retards d'avions. \n",
    "</p>\n",
    "<p>\n",
    "Le modèle se base sur les données fournies par le site :\n",
    "</p>\n",
    "<p>\n",
    "https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time\n",
    "</p>\n",
    "<p>\n",
    "</p>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blus'> Synthèse du 14/06/2018</font>\n",
    "* Heure de départ :implémentation de la périodicité sur [0,2PI] (configurable)\n",
    "* Mois :implémentation de la périodicité sur [0,2PI] (configurable)\n",
    "* Impémentation d'un estimateur par route et validation : ~ 4000 routes\n",
    "* Pour chaque route : implémentation des modèles suivant :\n",
    "    ** Lasso\n",
    "    ** Ridge\n",
    "    ** LinearRegression\n",
    "    ** SGDRegressor avec un GridSearchCV\n",
    "* Optimisation de la mémoire en compressant en RAM (zlib)\n",
    "* Chargement et test du dataset sur 8 mois.    \n",
    "* Traitement du features à l'origine de data-leakage\n",
    "* Heure de départ (CRS_DEP_TIME) : implementation d'un dataframe réduit avec les informations strictement nécessaires pour faire la régression linéaire et renvoyer la prédiction.\n",
    "\n",
    "## <font color='blue'>Déploiement</font>\n",
    "* Composant P4_ModelBuilder : fabrication du composant LinearDelayPredictor\n",
    "* Implémentation du composant LinearDelayPredictor pour déploiement. Test et validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blus'> Synthèse du 07/06/2018</font>\n",
    "* Heure de départ : indication avant midi et après midi\n",
    "* Data leakage : enlever distance et arr_time dans un modele par route.\n",
    "* Utilisation des modèles fins : en production oui pour les modèles fins mais attention pour l'exercice.\n",
    "* Variables de temps : tranches de 15mn\n",
    "* Modele par route?\n",
    "* Sans route : encodage des aeroports: par probabilité d'apparition dans le dataset; a partir des analyses exploratoires, calculer le retard moyen par aeroport et diviser par quartile. En fonction des quartiles : 0, 0.25, 0.75, 1.0; calculer  Faire un apply surl la colonne avec les \n",
    "\n",
    "\n",
    "**Ce modèle est fortement non linéaire; on a donc une approximation.**\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'max_iter':[5, 10, 20, 40], 'penalty':[None, \"l1\", \"l2\", \"elasticnet\"], \"alpha\":[0.0001, 0.001, 0.01], \"l1_ratio\": [0.15, 0.5, 0.85] }\n",
    "model = SGDRegressor()\n",
    "reg = GridSearchCV(model, parameters, cv=3, scoring='neg_mean_absolute_error')\n",
    "reg.fit(X_train, y_train)\n",
    "best_model = reg.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_pred)**(1/2))\n",
    "mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "## <font color='blue'>Modeles de regression</font>\n",
    "* Dummy\n",
    "* LinearRegression\n",
    "* Ridge\n",
    "* Lasso\n",
    "* Utilisation de LassoCV\n",
    "* Utilisation de RidgeCV\n",
    "* Utilisation de GridSearcv avec Lasso, Ridge, ElasticNet\n",
    "## <font color='blue'>Déploiement</font>\n",
    "* Composant P4DataModel\n",
    "* Implementation du modele par route et par transporteur\n",
    "* Implementation du modele avec retards et avances différentiés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blus'> Synthèse du 31/05/2018</font>\n",
    "\n",
    "## <font color='blue'>Data preparation</font>\n",
    "* Rework : choix des variables pour l'analyse exploratoire\n",
    "* Data préparation en vue d'abtenir des modèles de regression linéaires performants.\n",
    "\n",
    "## <font color='blue'>Analyse exploratoire</font>\n",
    "* **<font color='blue'>Hypothèse : la frequentation relative est indépendante du mois</font>**\n",
    "* Analyse dela distribution des retards\n",
    "* Analyse de la variation des retards pour une route donnée, en fonction de différentes variables\n",
    "    * Jour dans le mois\n",
    "    * Transporteurs\n",
    "    * Jour de la semaine\n",
    "    * Date du vol\n",
    "* <font color='red'>Justification du choix des variables dans le modèle de prédiction</font>\n",
    "\n",
    "## <font color='blue'>Modèle de régression linéaire</font>\n",
    "* Choix des variables : justification\n",
    "* Algorithmes de regression linéaires : SVR, ElasticNet, ... : pas de résultat probant\n",
    "* Mise e évidence de la nature gaussiennne du bruit (hypothese de regression linéaire)\n",
    "* Data leak: ??\n",
    "* Incidences de la correlation des variables sur les performances du modèle.\n",
    "\n",
    "## <font color='blue'>Session de mentorat</font>\n",
    "SGDR Regression : méthode de déscente dans le cas de gros modeles numériques.\n",
    "Convergence iterative vers le vecteur beta.\n",
    "Lecture d'un fichier --> partial_fit()\n",
    "Le modele fait converger le beta pendant l'entraînement\n",
    "Permet de sortir des scores intermédiaires.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "\n",
    "Utiliser MAE et MSE et R2 en complément.\n",
    "\n",
    "Utiliser MAE avec  R2 qui est plus frequement utilisé que R2\n",
    "MSE : moyenne des diff au carré.\n",
    "\n",
    "Utiliser CRS_ARR en place de CRS_ELAPSED_TIME\n",
    "\n",
    "Attetion au OneHotEncoding : utiliser le mm objet pour encoder tous les data issues des différents fichiers.\n",
    "Sinon : risque de pb d'indexes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blus'> Synthèse du 24/05/2018</font>\n",
    "\n",
    "## <font color='blue'>Nature du pb à résoudre</font>\n",
    "* Problème de prédiction.\n",
    "* Orientation vers un algorithme de régression tels que vue en cours : ElasticNet.\n",
    "\n",
    "## <font color='blue'>Acquisition des données</font>\n",
    "* Problème de débordement mémoire\n",
    "\n",
    "## <font color='blue'>Préparation des données</font>\n",
    "* Traitement des valeurs a Nan\n",
    "\n",
    "## <font color='blue'>Analyse exploratoire</font>\n",
    "Objectifs: appréhender la nature du problème à résoudre pour s'orienter vers des modèles de prédictions.\n",
    "* Correlations\n",
    "* Analyse en composantes principales :  quelles sont les variables qui expliquent le plus la variance? Réduction de dimension possible?\n",
    "\n",
    "## <font color='blue'>Evaluation des modèles de prédiction</font>\n",
    "* Modèle linéaire : basé sur une combinaison linéaire des variables.__Comment justifier l'hyopthèse de linéarité?__\n",
    "* Choix des hyper-paramètres : Grid search\n",
    "* Entraînement des modèles : validation croisée\n",
    "* Prédiction des retards avec un score : prédiction de la valeur du retard\n",
    "* Prédiction binaire du retard : en retard / pas en retard\n",
    "* Comparaison des performances avec des modèles naïfs.\n",
    "* Augmentation de la performance des algorithmes évalués\n",
    "\n",
    "## <font color='blue'>Choix du modèle de prédiction</font>\n",
    "\n",
    "\n",
    "* Anticiper les retards : retard = heure atterissage - heure prévue.\n",
    "* Créer la colonne avant la séparation et supprimer la colonne dont elle dépend. Sinon phénomène de __DATA LEAKAGE__. *En effet, si on utilise deux variables pour en créer une 3eme, alors lorsque l'on va prédire, à un instant dans le futur, une valeur pour cette 3eme variable, les valeurs des 2 variables utilisées ne seront plus valables au moment de la prédiction. Les variables ayant servies à créer cette nouvelle variable doivent être **exclues du modèle**.*.\n",
    "\n",
    "* Les étiquettes sont a valeurs dans les réels. On utilisera donc une regression linéaire pour la prédiction des retards.\n",
    "\n",
    "* Après modélisation, si la mtrice X.T.dot(X) est inversible, alors le pb admet une solution unique et explicite, issue de la maximisation de la vraissemblance du modèle, i.e, la maximisation de la probabilité de calculer la valeur de la cible (ici, pour ce problème, les retards).\n",
    "\n",
    "* Technique courante sur les lignes : définir les indices das un vecteur modele réduit Etudier l'impact de la taille des données d'entraînement sur la qualité de la prédiction. 20%, 40%, 30% de l'entraînement et regarder la performance.\n",
    "* Supprimer les colones qui n'apporent rien a la prediction\n",
    "* Regression : foncton de out a optimiser va inclure les coeff de Ridge de et Lasso Initialement, sans regularisation. Par la suite, essayer la recherche d'hyper-paramètres l1 et l2 qui controlent Ridge et Lasso. __Coder une seule loss__ qui jauge l'effet de la régularisation L1 et L2. __==> utiliser la regression elestic net.__\n",
    "\n",
    "* Conclusion : jouer le jeux de test et évaluer l'erreur quadratique.\n",
    "\n",
    "* __Le pb est formulé en prédiction__ : entraîner un modele sur Janv--> mars, essayer sur Avril. Ce, pour répondre a la probléamatique : évaluer les retards dans le futur.\n",
    "* Entraîner surr 15 premiers jours et tester sur les derniers jours.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blus'> Synthèse du 19/05/2018</font>\n",
    "\n",
    "## <font color='blue'>Acquisition des données</font>\n",
    "* Exploration des données et modèle sur un mois\n",
    "\n",
    "## <font color='blue'>Préparation des données</font>\n",
    "* Traitement des valeurs a Nan remplacées par la valeur moyenne dans le vecteur étiquette \n",
    "* Mises a 0 sur les autres données.\n",
    "\n",
    "## <font color='blue'>Analyse exploratoire</font>\n",
    "Objectifs: appréhender la nature du problème à résoudre pour s'orienter vers des modèles de prédictions.\n",
    "* Correlations : données corrélées sur les retards ==> élimination des colonnes corrélées?\n",
    "\n",
    "## <font color='blue'>Evaluation d'un modèle de prédiciton binaire : KNN</font>\n",
    "* Choix des hyper-paramètres : Grid search\n",
    "* Entraînement des modèles : validation croisée\n",
    "* Affichage des performances du modèle : MSE et matrice de confusion\n",
    "\n",
    "## <font color='blue'>Modèle de prédiction par regression : SVR </font>\n",
    "* Données d'entraînement : sur un mois, les 3 premières semaines.\n",
    "* Regression linéaire par vecteur de support\n",
    "\n",
    "\n",
    "## Commentaires de la session\n",
    "* Bruit Gaussien : pour valider les hypothèses de la mise en oeuvre de la regression linéaire, faire un histogramme a posteriori en utilisant le calcul de l'erreur entre les données mesurées et la valeur d'une regression linéaire simple.\n",
    "\n",
    "* Indépendance : vérifier que les colonnes ne sont pas indépendantes. Au sens conceptuel du terme : heure dans deux colonnes pas exeemple. \n",
    "* Data leakage : retard au depart et retard a l'arrivée. Predire a partir des informations indépendantes du retard de départ. Enlever le retard de départ; cela ne se justifie pas par l'IID. Dans la réaalité pas accès à cette information.\n",
    "\n",
    "* Scikitlearn : entrainer le modele et remplacer les données par des nvx modeles .Partial feed Voir http://scikit-learn.org/stable/modules/scaling_strategies.html\n",
    "* Se focaliser sur la regression linéaire Ridge / Lasso\n",
    "* Utiliser les regression KNN et Linear SVR en termes de références, baseline ou pour approfondir l'étude.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blues'>Paramètres globaux du modèle</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_df_with_route = False\n",
    "is_route_in_model  = False\n",
    "is_carrier_model = False\n",
    "is_delay_outlier_removed = True\n",
    "\n",
    "dict_delay_splitted = {'past':0,'futur':0,'present':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blues'>Acquisition des données</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loading Python lib used for project__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn import model_selection\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from P4_ModelBuilder import *\n",
    "from p3_util import *\n",
    "from p3_util_plot import *\n",
    "from p4_util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data loading from a fixed month__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../outside_p4/data/Dataset+Projet+4_2/\"\n",
    "\n",
    "list_col_keep = [\n",
    " 'CRS_DEP_TIME'\n",
    ",'ORIGIN_AIRPORT_ID'\n",
    ",'DEST_AIRPORT_ID'          \n",
    ",'ORIGIN_CITY_NAME'\n",
    ",'DEST_CITY_NAME'             \n",
    ",'ARR_DELAY'\n",
    ",'AIRLINE_ID'    \n",
    ",'MONTH'\n",
    ",'DAY_OF_MONTH'\n",
    ",'DAY_OF_WEEK'\n",
    ",'CARRIER'\n",
    ",'CRS_ARR_TIME'    \n",
    ",'CRS_ELAPSED_TIME'    \n",
    ",'DISTANCE'\n",
    ",'ORIGIN_STATE_ABR'\n",
    ",'DEST_STATE_ABR'\n",
    "]\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# These features are used for user test \n",
    "#-------------------------------------------------------------------------\n",
    "list_for_user_test =['MONTH','DAY_OF_MONTH','DAY_OF_WEEK'\\\n",
    "      , 'CRS_DEP_TIME','AIRLINE_ID','ORIGIN_CITY_NAME','DEST_CITY_NAME'\\\n",
    "      ,'ARR_DELAY','CARRIER','CRS_ARR_TIME','FL_NUM']\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# These features are excluded from data model\n",
    "#-------------------------------------------------------------------------\n",
    "list_excluded = ['WEEK_OF_MONTH','CARRIER','CRS_ARR_TIME','FL_NUM']\n",
    "\n",
    "#list_month = ['01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "list_month = ['01','02','03','04']\n",
    "list_month = ['01']\n",
    "\n",
    "list_skip_rows_04 = [121011,386248,388290,389370,389547,453857]\n",
    "dict_skip_rows = dict()\n",
    "dict_skip_rows['01'] = None\n",
    "dict_skip_rows['02'] = None\n",
    "dict_skip_rows['03'] = None\n",
    "dict_skip_rows['04'] = list_skip_rows_04\n",
    "dict_skip_rows['05'] = None\n",
    "dict_skip_rows['06'] = None\n",
    "dict_skip_rows['07'] = None\n",
    "dict_skip_rows['08'] = None\n",
    "dict_skip_rows['09'] = None\n",
    "dict_skip_rows['10'] = None\n",
    "dict_skip_rows['11'] = None\n",
    "dict_skip_rows['12'] = None\n",
    "list_for_cleaning = ['MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK'\\\n",
    "                     ,'AIRLINE_ID','ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID','CRS_DEP_TIME', 'ARR_DELAY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #----------------------------------------------------------------------------\n",
    "   #\n",
    "   #----------------------------------------------------------------------------\n",
    "   def clean(_df, list_to_keep) :\n",
    "      '''Clean all columns outside from list_to_keep\n",
    "      Drop raws from which values are Nan.\n",
    "      '''\n",
    "      #-------------------------------------------------------------------------\n",
    "      # Build list to drop from list given as parameter\n",
    "      #-------------------------------------------------------------------------\n",
    "      list_col_drop = list()\n",
    "      for col in _df.columns:\n",
    "          if col not in list_to_keep :\n",
    "              list_col_drop.append(col)\n",
    "\n",
    "      _df = df_drop_list_column(_df,list_col_drop)        \n",
    "\n",
    "      ser_col_nan = _df.isnull().any()\n",
    "      list_col_nan = list()\n",
    "      for col, status in ser_col_nan.iteritems() :\n",
    "          if status is  True :\n",
    "              list_col_nan.append(col)\n",
    "              \n",
    "      print(\"Columns targeted for nan : \"+str(list_col_nan))\n",
    "\n",
    "      _df.dropna(axis=0, how='any', inplace=True)   \n",
    "      return _df\n",
    "   #----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #----------------------------------------------------------------------------\n",
    "   #\n",
    "   #----------------------------------------------------------------------------\n",
    "   def _clean_value_from_list(_df, list_for_cleaning):\n",
    "       ''' Drop raws considering criterias applied on values for \n",
    "       each feature given as list in parameter.\n",
    "       Criterias apply as following :\n",
    "         --> If any value is not digit, raw containing this feature is deleted.\n",
    "         --> All values are casted into integer type.\n",
    "    '''\n",
    "        \n",
    "       if 0>= len(list_for_cleaning)  or list_for_cleaning is None :\n",
    "         return\n",
    "\n",
    "       for column_for_cleaning in list_for_cleaning :\n",
    "           # mark as -1 fields that are not digit\n",
    "           print(\"\\n\"+str(column_for_cleaning)+\" : ...\")\n",
    "           #----------------------------------------------------------------------\n",
    "           # Clean rows from Nan values\n",
    "           #----------------------------------------------------------------------\n",
    "           #_df.dropna(axis=1, how='all',inplace=True)\n",
    "           #_df.dropna(axis=0, how='all',inplace=True)\n",
    "           print(_df.shape)\n",
    "           _df[column_for_cleaning].dropna(axis=0, inplace=True)\n",
    "           print(_df.shape)\n",
    "\n",
    "\n",
    "           #--------------------------------------------------------------------\n",
    "           # Printing for debug\n",
    "           #--------------------------------------------------------------------\n",
    "           if column_for_cleaning != 'ARR_DELAY' \\\n",
    "           and column_for_cleaning != 'ORIGIN_AIRPORT_ID' \\\n",
    "           and column_for_cleaning != 'DEST_AIRPORT_ID':\n",
    "               print(_df[column_for_cleaning].unique())\n",
    "\n",
    "           #--------------------------------------------------------------------\n",
    "           # Mark none digit values in order raw holding this value to be deleted.\n",
    "           #--------------------------------------------------------------------\n",
    "           _df[column_for_cleaning] = \\\n",
    "           _df[column_for_cleaning].apply(p4_mark_none_digit)\n",
    "\n",
    "           #--------------------------------------------------------------------\n",
    "           # Printing for debug\n",
    "           #--------------------------------------------------------------------\n",
    "           if column_for_cleaning != 'ARR_DELAY' \\\n",
    "           and column_for_cleaning != 'ORIGIN_AIRPORT_ID' \\\n",
    "           and column_for_cleaning != 'DEST_AIRPORT_ID':\n",
    "               print(_df[column_for_cleaning].unique())\n",
    "\n",
    "           #--------------------------------------------------------------------\n",
    "           # All digit are casted into integer\n",
    "           #--------------------------------------------------------------------\n",
    "           try :\n",
    "               _df[column_for_cleaning] = \\\n",
    "               _df[column_for_cleaning].apply(lambda x: int(x))\n",
    "           except  ValueError as valueError:\n",
    "               print(\"*** ERROR : column= \"+str(column_for_cleaning)+\" error= \"+str(valueError))\n",
    "           #--------------------------------------------------------------------\n",
    "           # Printing for debug\n",
    "           #--------------------------------------------------------------------\n",
    "           if column_for_cleaning != 'ARR_DELAY' \\\n",
    "           and column_for_cleaning != 'ORIGIN_AIRPORT_ID' \\\n",
    "           and column_for_cleaning != 'DEST_AIRPORT_ID':\n",
    "               print(_df[column_for_cleaning].unique())\n",
    "\n",
    "           if column_for_cleaning != 'ARR_DELAY' :\n",
    "               #----------------------------------------------------------------\n",
    "               # Excepted values from feature ARR_DELAY\n",
    "               # raws with strictly negative value are deleted.\n",
    "               #----------------------------------------------------------------\n",
    "               _df = _df[_df[column_for_cleaning]>=0] \n",
    "\n",
    "\n",
    "           #--------------------------------------------------------------------\n",
    "           # Printing for debug\n",
    "           #--------------------------------------------------------------------\n",
    "           if column_for_cleaning != 'ARR_DELAY' \\\n",
    "           and column_for_cleaning != 'ORIGIN_AIRPORT_ID' \\\n",
    "           and column_for_cleaning != 'DEST_AIRPORT_ID':\n",
    "               print(_df[column_for_cleaning].unique())\n",
    "\n",
    "           #--------------------------------------------------------------------\n",
    "           # Months belongs to interval [1, 12]\n",
    "           #--------------------------------------------------------------------\n",
    "           if column_for_cleaning == 'MONTH' :\n",
    "               _df = _df[_df[column_for_cleaning]<=12] \n",
    "\n",
    "\n",
    "           print(column_for_cleaning+\" : Done!\\n\")\n",
    "       return _df\n",
    "   #-------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #-------------------------------------------------------------------------\n",
    "   #\n",
    "   #-------------------------------------------------------------------------\n",
    "   def load_and_clean(list_month,list_to_keep,list_for_cleaning,\\\n",
    "   dict_skip_rows=None) :\n",
    "      #----------------------------------------------------------------------\n",
    "      # Tempory dataframe initialization; used for data agrregation \n",
    "      # issued formm files  \n",
    "      #----------------------------------------------------------------------\n",
    "      df_concat = pd.DataFrame()\n",
    "      #----------------------------------------------------------------------\n",
    "      # Files will be read and dumped every nb_file_dumped\n",
    "      #----------------------------------------------------------------------\n",
    "      nb_file_dumped = 0\n",
    "      is_partial_file = True\n",
    "      t0 = time.time()            \n",
    "      \n",
    "      if len(list_month) < self._modulo_month :\n",
    "         is_partial_file = False   \n",
    "\n",
    "      for month in list_month :\n",
    "         #-------------------------------------------------------------------\n",
    "         # Read file and store it into a dataframe\n",
    "         # Some rows, depending on month-file, leading to read errors, are skipped .\n",
    "         #-------------------------------------------------------------------\n",
    "         if dict_skip_rows is not None :\n",
    "            list_skip_rows = dict_skip_rows[month]\n",
    "         else :\n",
    "            list_skip_rows = None         \n",
    "      \n",
    "         self._df , list_col_notdigit = \\\n",
    "         p4_df_read_from_list_month(self._path_to_data+self.year, month,\\\n",
    "         list_skip_rows=list_skip_rows)\n",
    "\n",
    "         #-------------------------------------------------------------------\n",
    "         # Clean columns regarding column list in list_to_keep\n",
    "         #-------------------------------------------------------------------\n",
    "         self._df = self._df[list_to_keep]   \n",
    "         \n",
    "         #-------------------------------------------------------------------\n",
    "         # Drop rows with undefined values\n",
    "         #-------------------------------------------------------------------\n",
    "         self._df.dropna(inplace=True, axis=0)\n",
    "         self.clean(list_to_keep)\n",
    "\n",
    "         #-------------------------------------------------------------------\n",
    "         # Aggregation : cleaned dataframe is aggregated into df_concat\n",
    "         #-------------------------------------------------------------------\n",
    "         df_concat = pd.concat([df_concat, self._df ],axis=0)\n",
    "\n",
    "         #-------------------------------------------------------------------\n",
    "         # Clear dataframe memory before coyping concatened dataframe in it.\n",
    "         #-------------------------------------------------------------------\n",
    "         self._df = pd.DataFrame()\n",
    "\n",
    "         #-------------------------------------------------------------------\n",
    "         # Dataframe serialisation every 4 read files\n",
    "         #-------------------------------------------------------------------\n",
    "         if is_partial_file :\n",
    "            if 0 == int(month) % 4 :\n",
    "               #------------------------------------------------------------\n",
    "               # Copy back aggregated dataframe\n",
    "               #------------------------------------------------------------\n",
    "               self._df = pd.DataFrame()\n",
    "               self._df = df_concat.copy()\n",
    "\n",
    "               #----------------------------------------------------------------------\n",
    "               # Clean values from list when these values do not match some criterias.\n",
    "               # This criteria depends on features into list_for_cleaning.\n",
    "               #----------------------------------------------------------------------\n",
    "               self._clean_value_from_list(list_for_cleaning)\n",
    "\n",
    "               #----------------------------------------------------------------------\n",
    "               # resources used in this method are released.\n",
    "               #----------------------------------------------------------------------\n",
    "               del(df_concat)\n",
    "\n",
    "\n",
    "               nb_file_dumped += 1\n",
    "               fileName = self._path_to_data+\"partFile_\"+str(nb_file_dumped)\n",
    "               with open(fileName,\"wb\") as dumpedFile :\n",
    "                 oPickler = pickle.Pickler(dumpedFile)\n",
    "                 oPickler.dump(self._df)\n",
    "               print(\"\\nPartial file = \"+str(fileName)+\" dumped!\\n\")    \n",
    "               #----------------------------------------------------------------------\n",
    "               # Clear concatened dataframe memory \n",
    "               #----------------------------------------------------------------------\n",
    "               df_concat =pd.DataFrame()    \n",
    "            else :\n",
    "               pass\n",
    "         \n",
    "         else :\n",
    "            #----------------------------------------------------------------\n",
    "            # Copy back aggregated dataframe\n",
    "            #----------------------------------------------------------------\n",
    "            self._df = pd.DataFrame()\n",
    "            self._df = df_concat.copy()\n",
    "\n",
    "            #----------------------------------------------------------------\n",
    "            # Clean values from list when these values do not match some criterias.\n",
    "            # This criteria depends on features into list_for_cleaning.\n",
    "            #----------------------------------------------------------------\n",
    "            self._clean_value_from_list(list_for_cleaning)\n",
    "\n",
    "            #----------------------------------------------------------------\n",
    "            # resources used in this method are released.\n",
    "            #----------------------------------------------------------------\n",
    "            del(df_concat)\n",
    "\n",
    "      #----------------------------------------------------------------------\n",
    "      # Read all dumped files\n",
    "      #----------------------------------------------------------------------\n",
    "      \n",
    "      if is_partial_file :\n",
    "         self._df = pd.DataFrame()\n",
    "         for nb_file in range(1,nb_file_dumped+1,1) :\n",
    "            fileName = self._path_to_data+\"/\"+\"partFile_\"+str(nb_file)\n",
    "            try:\n",
    "               with open(fileName,\"rb\") as dataFile:\n",
    "                  oUnpickler = pickle.Unpickler(dataFile)\n",
    "                  #----------------------------------------------------------------\n",
    "                  # Aggregation : cleaned dataframe is aggregated with df_concat\n",
    "                  #----------------------------------------------------------------\n",
    "                  self._df = pd.concat([self._df, oUnpickler.load() ],axis=0)\n",
    "            except FileNotFoundError: \n",
    "               print(\"\\n*** ERROR : file not found : \"+fileName)\n",
    "      else :\n",
    "         pass\n",
    "\n",
    "      #-------------------------------------------------------------------------\n",
    "      # Fraction rows from the whole dataframe\n",
    "      #-------------------------------------------------------------------------\n",
    "      self._fract_whole_data()\n",
    "      t1 = time.time()\n",
    "      \n",
    "      print(\"\\n*** Time for reading and fractioning all data files: %0.3F\" %(t1-t0))\n",
    "   #----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 01 loaded!\n",
      "Columns targeted for nan : []\n",
      "Time for reading data : 7.147\n"
     ]
    }
   ],
   "source": [
    "df_delay_digit = pd.DataFrame()\n",
    "df_delay_digit =  load_and_clean(df_delay_digit, str(2016), list_month,list_col_keep,list_for_cleaning,dict_skip_rows=dict_skip_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_delay_digit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'ORIGIN_STATE_ABR'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0090b9fd3cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_delay_digit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mORIGIN_STATE_ABR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#list_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3614\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ORIGIN_STATE_ABR'"
     ]
    }
   ],
   "source": [
    "list_state = df_delay_digit.ORIGIN_STATE_ABR.unique().tolist()\n",
    "#list_state"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(df_delay_digit.shape)\n",
    "print(df_delay_digit.columns)\n",
    "\n",
    "#print(df_delay_digit.isnull().sum())\n",
    "#df_delay_digit.dropna(axis=1, how='any', inplace=True)\n",
    "#print(df_delay_digit.isnull().sum())\n",
    "#print(df_delay_digit.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Modele climatique</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_climate = dict()\n",
    "list_state = df_delay_digit._df.ORIGIN_STATE_ABR.unique().tolist()\n",
    "\n",
    "# Climat tempéré océanique : 1\n",
    "dict_climate['WA']=1\n",
    "dict_climate['OR']=1\n",
    "\n",
    "# Climat tempéré continental sec : 2\n",
    "list_climate_2 = ['ID','MT','WY','UT','CO','ND','SD','NE','KS','OK']\n",
    "for state in list_climate_2 :\n",
    "    dict_climate[state]=2\n",
    "    \n",
    "# Climat tempéré continental pacifique : 3\n",
    "\n",
    "# Climat sub-tropical : 4\n",
    "list_climate_4 = ['VA','NC','GA','FL','AL','TN','MS','LA','AR']\n",
    "for state in list_climate_4 :\n",
    "    dict_climate[state]= 4\n",
    "\n",
    "# Climat Aride : 5\n",
    "list_climate_5 = ['AZ','TX','NM','NV']\n",
    "for state in list_climate_5 :\n",
    "    dict_climate[state]= 5\n",
    "\n",
    "# Climat tempéré méditéranéen : 6\n",
    "list_climate_6 = ['CA']\n",
    "for state in list_climate_6 :\n",
    "    dict_climate[state]= 6\n",
    "\n",
    "for state in list_state :\n",
    "    if state not in dict_climate.keys() :\n",
    "        dict_climate[state] = 3\n",
    "print(len(dict_climate))\n",
    "#print(dict_climate)\n",
    "\n",
    "def state_climate_build(state) :\n",
    "    return dict_climate[state]\n",
    "\n",
    "df_delay_digit['ORIGIN_CLIMAT'] = df_delay_digit.ORIGIN_STATE_ABR.apply(state_climate_build)\n",
    "df_delay_digit['DEST_CLIMAT'] = df_delay_digit.DEST_STATE_ABR.apply(state_climate_build)\n",
    "\n",
    "#del(df_delay_digit['ORIGIN_STATE_ABR'])\n",
    "df_delay_digit.sample(2)\n",
    "df_delay_digit_save = df_delay_digit.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #-------------------------------------------------------------------------\n",
    "   #\n",
    "   #-------------------------------------------------------------------------\n",
    "   def load_and_clean(self, list_month,list_to_keep,list_for_cleaning,\\\n",
    "   dict_skip_rows=None) :\n",
    "      #----------------------------------------------------------------------\n",
    "      # Tempory dataframe initialization; used for data agrregation \n",
    "      # issued formm files  \n",
    "      #----------------------------------------------------------------------\n",
    "      df_concat = pd.DataFrame()\n",
    "      #----------------------------------------------------------------------\n",
    "      # Files will be read and dumped every nb_file_dumped\n",
    "      #----------------------------------------------------------------------\n",
    "      nb_file_dumped = 0\n",
    "      is_partial_file = True\n",
    "      t0 = time.time()            \n",
    "      \n",
    "      if len(list_month) < self._modulo_month :\n",
    "         is_partial_file = False   \n",
    "\n",
    "      for month in list_month :\n",
    "         #-------------------------------------------------------------------\n",
    "         # Read file and store it into a dataframe\n",
    "         # Some rows, depending on month-file, leading to read errors, are skipped .\n",
    "         #-------------------------------------------------------------------\n",
    "         if dict_skip_rows is not None :\n",
    "            list_skip_rows = dict_skip_rows[month]\n",
    "         else :\n",
    "            list_skip_rows = None         \n",
    "      \n",
    "         self._df , list_col_notdigit = \\\n",
    "         p4_df_read_from_list_month(self._path_to_data+self.year, month,\\\n",
    "         list_skip_rows=list_skip_rows)\n",
    "\n",
    "         #-------------------------------------------------------------------\n",
    "         # Clean columns regarding column list in list_to_keep\n",
    "         #-------------------------------------------------------------------\n",
    "         self._df = self._df[list_to_keep]   \n",
    "         \n",
    "         #-------------------------------------------------------------------\n",
    "         # Drop rows with undefined values\n",
    "         #-------------------------------------------------------------------\n",
    "         self._df.dropna(inplace=True, axis=0)\n",
    "         self.clean(list_to_keep)\n",
    "\n",
    "         #-------------------------------------------------------------------\n",
    "         # Aggregation : cleaned dataframe is aggregated into df_concat\n",
    "         #-------------------------------------------------------------------\n",
    "         df_concat = pd.concat([df_concat, self._df ],axis=0)\n",
    "\n",
    "         #-------------------------------------------------------------------\n",
    "         # Clear dataframe memory before coyping concatened dataframe in it.\n",
    "         #-------------------------------------------------------------------\n",
    "         self._df = pd.DataFrame()\n",
    "\n",
    "         #-------------------------------------------------------------------\n",
    "         # Dataframe serialisation every 4 read files\n",
    "         #-------------------------------------------------------------------\n",
    "         if is_partial_file :\n",
    "            if 0 == int(month) % 4 :\n",
    "               #------------------------------------------------------------\n",
    "               # Copy back aggregated dataframe\n",
    "               #------------------------------------------------------------\n",
    "               self._df = pd.DataFrame()\n",
    "               self._df = df_concat.copy()\n",
    "\n",
    "               #----------------------------------------------------------------------\n",
    "               # Clean values from list when these values do not match some criterias.\n",
    "               # This criteria depends on features into list_for_cleaning.\n",
    "               #----------------------------------------------------------------------\n",
    "               self._clean_value_from_list(list_for_cleaning)\n",
    "\n",
    "               #----------------------------------------------------------------------\n",
    "               # resources used in this method are released.\n",
    "               #----------------------------------------------------------------------\n",
    "               del(df_concat)\n",
    "\n",
    "\n",
    "               nb_file_dumped += 1\n",
    "               fileName = self._path_to_data+\"partFile_\"+str(nb_file_dumped)\n",
    "               with open(fileName,\"wb\") as dumpedFile :\n",
    "                 oPickler = pickle.Pickler(dumpedFile)\n",
    "                 oPickler.dump(self._df)\n",
    "               print(\"\\nPartial file = \"+str(fileName)+\" dumped!\\n\")    \n",
    "               #----------------------------------------------------------------------\n",
    "               # Clear concatened dataframe memory \n",
    "               #----------------------------------------------------------------------\n",
    "               df_concat =pd.DataFrame()    \n",
    "            else :\n",
    "               pass\n",
    "         \n",
    "         else :\n",
    "            #----------------------------------------------------------------\n",
    "            # Copy back aggregated dataframe\n",
    "            #----------------------------------------------------------------\n",
    "            self._df = pd.DataFrame()\n",
    "            self._df = df_concat.copy()\n",
    "\n",
    "            #----------------------------------------------------------------\n",
    "            # Clean values from list when these values do not match some criterias.\n",
    "            # This criteria depends on features into list_for_cleaning.\n",
    "            #----------------------------------------------------------------\n",
    "            self._clean_value_from_list(list_for_cleaning)\n",
    "\n",
    "            #----------------------------------------------------------------\n",
    "            # resources used in this method are released.\n",
    "            #----------------------------------------------------------------\n",
    "            del(df_concat)\n",
    "\n",
    "      #----------------------------------------------------------------------\n",
    "      # Read all dumped files\n",
    "      #----------------------------------------------------------------------\n",
    "      \n",
    "      if is_partial_file :\n",
    "         self._df = pd.DataFrame()\n",
    "         for nb_file in range(1,nb_file_dumped+1,1) :\n",
    "            fileName = self._path_to_data+\"/\"+\"partFile_\"+str(nb_file)\n",
    "            try:\n",
    "               with open(fileName,\"rb\") as dataFile:\n",
    "                  oUnpickler = pickle.Unpickler(dataFile)\n",
    "                  #----------------------------------------------------------------\n",
    "                  # Aggregation : cleaned dataframe is aggregated with df_concat\n",
    "                  #----------------------------------------------------------------\n",
    "                  self._df = pd.concat([self._df, oUnpickler.load() ],axis=0)\n",
    "            except FileNotFoundError: \n",
    "               print(\"\\n*** ERROR : file not found : \"+fileName)\n",
    "      else :\n",
    "         pass\n",
    "\n",
    "      #-------------------------------------------------------------------------\n",
    "      # Fraction rows from the whole dataframe\n",
    "      #-------------------------------------------------------------------------\n",
    "      self._fract_whole_data()\n",
    "      t1 = time.time()\n",
    "      \n",
    "      print(\"\\n*** Time for reading and fractioning all data files: %0.3F\" %(t1-t0))\n",
    "   #----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit.sample(3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    nb_file_dumped = 3\n",
    "    df_delay_digit = pd.DataFrame()\n",
    "    for nb_file in range(1,nb_file_dumped+1,1) :\n",
    "        fileName = path_to_data+\"partFile_\"+str(nb_file)\n",
    "        try:\n",
    "          with open(fileName,\"rb\") as dataFile:\n",
    "             oUnpickler = pickle.Unpickler(dataFile)\n",
    "             #----------------------------------------------------------------------\n",
    "             # Aggregation : cleaned dataframe is aggregated with df_concat\n",
    "             #----------------------------------------------------------------------\n",
    "             df_delay_digit = pd.concat([df_delay_digit, oUnpickler.load() ],axis=0)\n",
    "        except FileNotFoundError: \n",
    "            print(\"\\n*** ERROR : file not found : \"+fileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit = df_delay_digit_save.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_delay_digit_save.ORIGIN_AIRPORT_ID.unique()), len(df_delay_digit_save.ORIGIN_CITY_NAME.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blues'>Data preparation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit = df_delay_digit_save.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Identification des colonnes non numériques : <font color='blue'>list_col_notdigit</font>__\n",
    "\n",
    "*Les colonnes en gras sont à encoder*\n",
    "\n",
    "* **FL_DATE : date du vol**\n",
    "* UNIQUE_CARRIER : identification unique du transporteur\n",
    "* CARRIER : identification du transporteur\n",
    "* TAIL_NUM : marquage sur la carliingue de l'avion\n",
    "* ORIGIN : aeroport de départ\n",
    "* **ORIGIN_WAC : aeroport de départ, en code international**\n",
    "* **ORIGIN_STATE_FIPS : origine encdée par le bureau fédéral**\n",
    "* ORIGIN_CITY_NAME : ville a laquelle l'aeroport d'origine est rattachée\n",
    "* ORIGIN_STATE_ABR : abbréviation de l'état d'origine du vol\n",
    "* ORIGIN_STATE_NM : état d'origine du vol\n",
    "* DEST : aeroport de destination\n",
    "* **DEST_WAC : aeroport de destination en code international**\n",
    "* DEST_CITY_NAME : nom de la ille de destination\n",
    "* DEST_STATE_ABR : abbréviation de l'état de destination du vol\n",
    "* DEST_STATE_NM : état de destination du vol\n",
    "* **DEP_TIME_BLK : fenêtres temporelles de départ**\n",
    "* **ARR_TIME_BLK : fenêtres temporelles d'arrivée**\n",
    "* AIR_TIME : temps de vol avant atterissage et après décolage\n",
    "* CANCELLATION_CODE : code d'annulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Correspondance bi-univoque entre CARRIER et  AIRLINE_ID</font>\n",
    "\n",
    "* Le nom des compagnies aériennes est plus parlant pour un utilisateur que les codes transporteur.\n",
    "* On fait donc l'hypothèse qu'une compagnie est aérienne est assimilable à un transporteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_delay_digit.shape)\n",
    "df_delay_digit.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "frac = 1/100\n",
    "rows_frac = int(df_delay_digit.shape[0]*frac)\n",
    "print(rows_frac)\n",
    "df_delay_digit = df_delay_digit.sample(rows_frac)\n",
    "print(df_delay_digit.shape)\n",
    "print(df_delay_digit.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nb CARRIER = \"+str(len(df_delay_digit_save.CARRIER.unique())))\n",
    "print(\"Nb AIRLINE_ID = \"+str(len(df_delay_digit_save.AIRLINE_ID.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Data LEAKAGE</font>\n",
    "\n",
    "* Purge des variables qui contiennent de l'information sur les retards à prédire.\n",
    "\n",
    "Comme il est possible de prédire les retards avec les variables CRS_ARR_TIME et ARR_TIME, cett dernière \n",
    " est purgée du modèle.\n",
    "\n",
    "Toutes les variables signant un retard sont purgées du modèle pour les mêmes raisons. Il s'agit : \n",
    "* DEP_DELAY\n",
    "* DEP_DELAY_NEW\n",
    "* ARR_DELAY_NEW\n",
    "* ARR_DEL15\n",
    "* ARR_DELAY_GROUP\n",
    "* CARRIER_DELAY\n",
    "* WEATHER_DELAY\n",
    "* NAS_DELAY\n",
    "* SECURITY_DELAY\n",
    "* LATE_AIRCRAFT_DELAY\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Purge de variables</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Liste des variables à conserver__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable <font color='blue'>FL_DATE</font> est substituable par le groupe  de variables <font color='blue'>(MONTH,DAY_OF_MONTH,DAY_OF_MONTH)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purge des variables indésirables : celles qui ne sont pas dans <font color='blue'>list_col_keep</font>__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list_col_drop = list()\n",
    "for col in df_delay_digit.columns:\n",
    "    if col not in list_col_keep :\n",
    "        list_col_drop.append(col)\n",
    "\n",
    "print(df_delay_digit.shape)\n",
    "df_delay_digit = df_drop_list_column(df_delay_digit,list_col_drop)        \n",
    "print(df_delay_digit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Purge des lignes dont il existe des valeurs Nan</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_col_nan = df_delay_digit.isnull().any()\n",
    "list_col_nan = list()\n",
    "for col, status in ser_col_nan.iteritems() :\n",
    "    if status is  True :\n",
    "        list_col_nan.append(col)\n",
    "\n",
    "print(list_col_nan)\n",
    "rows_before = df_delay_digit.shape[0]\n",
    "print(df_delay_digit.shape)\n",
    "df_delay_digit.dropna(axis=0, how='any', inplace=True)\n",
    "print(df_delay_digit.shape)\n",
    "rows_after = df_delay_digit.shape[0]\n",
    "print(\"Percent drop rows = %0.2F\" %(100*np.abs(rows_before-rows_after)/rows_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blues'>Exploration des données</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Représentation des mois dans l'échantillon</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_delay_digit['MONTH'].hist(color='k', alpha=0.5, bins=50)\n",
    "\n",
    "z_ = ax.set(xlabel=\"Mois de l\\'année 2016\")\n",
    "z_ = ax.set(ylabel='Echantillons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Analyse de la correlation des variables dépendantes du temps</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df_delay_digit[['CRS_DEP_TIME', 'ARR_DELAY', 'DISTANCE','CRS_ARR_TIME','CRS_ELAPSED_TIME']].corr()\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "_z = sns.heatmap(df_corr, annot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette analyse montre, sans surprise, une forte corrélation entre les variables:\n",
    "* <font color='blue'>DISTANCE</font> et <font color='blue'>CRS_ELAPSED_TIME</font>.\n",
    "* <font color='blue'>CRS_ARR_TIME</font> et <font color='blue'>CRS_ELAPSED_TIME</font>.\n",
    "\n",
    "Il n'est pas nécesssaire de garder dans le modèle ces deux variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit = df_drop_list_column(df_delay_digit,['CRS_ELAPSED_TIME','CRS_ARR_TIME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Liste des colonnes résultantes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='blue'>Analyse de la distribution des retards</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_delay_digit['ARR_DELAY'].boxplot()\n",
    "df_boxplot_list_display(df_delay_digit, ['ARR_DELAY'], show_outliers=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boxplot_list_display(df_delay_digit, ['ARR_DELAY'], show_outliers=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure()\n",
    "\n",
    "ax = (df_delay_digit[df_delay_digit['ARR_DELAY']<=100])['ARR_DELAY'].hist(color='k', alpha=0.5, bins=150)\n",
    "\n",
    "z_ = ax.set(xlabel='Retards (mn)')\n",
    "z_ = ax.set(ylabel='Fréquence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_delay_digit['ARR_DELAY'].hist(color='k', alpha=0.5, bins=150)\n",
    "\n",
    "z_ = ax.set(xlabel='Retards (mn)')\n",
    "z_ = ax.set(ylabel='Fréquence')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.distplot(X_delays_std, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_filter = df_delay_digit['ARR_DELAY']>200                                                       \n",
    "ser  = df_delay_digit['ARR_DELAY']\n",
    "ser_most = ser[ser_filter]\n",
    "z_=ser_most.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_delay_digit.ORIGIN_CITY_NAME.unique()),len(df_delay_digit.DEST_CITY_NAME.unique())\n",
    "df_delay_digit.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='blue'>Frequentation des aéroports</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "flight_frequency_threshold = 0\n",
    "ser_most = p4_df_get_feature_count_from_threshold(df_delay_digit,'ORIGIN_AIRPORT_ID',flight_frequency_threshold)\n",
    "df = pd.DataFrame(ser_most, columns=['ORIGIN_AIRPORT_ID'])\n",
    "df_boxplot_list_display(df, ['ORIGIN_AIRPORT_ID'], show_outliers=False )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_boxplot_list_display(df, ['ORIGIN_AIRPORT_ID'], show_outliers=True )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "z_=df.hist(bins=100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "q1,q3,zmin,zmax = df_boxplot_limits(df , 'ORIGIN_AIRPORT_ID')\n",
    "q1,q3,zmin,zmax"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_most1 = df[df['ORIGIN_AIRPORT_ID']>zmax]\n",
    "df_most1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='blue'>Distribution des retards selon les routes</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'HROUTE' in df_delay_digit.columns :\n",
    "    del(df_delay_digit['HROUTE'])\n",
    "df_delay_digit['HROUTE'] = df_delay_digit['ORIGIN_CITY_NAME'] +\" / \" +df_delay_digit['DEST_CITY_NAME'] \\\n",
    "          +df_delay_digit['ORIGIN_AIRPORT_ID'].apply(lambda x: str(x))\\\n",
    "          +df_delay_digit['DEST_AIRPORT_ID'].apply(lambda x: str(x))\n",
    "\n",
    "    \n",
    "df_delay_digit['HROUTE'] = df_delay_digit['HROUTE'].apply(lambda val: (\"0x\"+hashlib.md5(val.encode()).hexdigest()))\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_delay_digit['HROUTE'])\n",
    "df_delay_digit['HROUTE'] = le.transform(df_delay_digit['HROUTE'])\n",
    "\n",
    "#df_route['ORIGIN_CITY_NAME'] = df_delay_digit['ORIGIN_CITY_NAME'].copy()\n",
    "#df_route['DEST_CITY_NAME'] = df_delay_digit['DEST_CITY_NAME'].copy()\n",
    "#df_route['ARR_DELAY'] = df_delay_digit['ARR_DELAY'].copy()\n",
    "\n",
    "#df_delay_digit['HROUTE'] = df_route['HROUTE'].copy()\n",
    "#print(df_route.shape)\n",
    "print(df_delay_digit.shape)\n",
    "print(df_delay_digit.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_delay_digit.HROUTE.value_counts()\n",
    "df_boxplot_list_display(df_delay_digit, ['HROUTE'], show_outliers=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_route_count(route) :\n",
    "    \"\"\"Returns the number of routes for a given route\"\"\"\n",
    "    ser_route = df_delay_digit[df_delay_digit['HROUTE']==route].HROUTE.value_counts()\n",
    "    \n",
    "    return ser_route[route]\n",
    "    \n",
    "\n",
    "\n",
    "#print(df_delay_digit['HROUTE'].value_counts().shape)\n",
    "list_route = df_delay_digit['HROUTE'].unique().tolist()\n",
    "nb_route = len(list_route)\n",
    "route_id = 0\n",
    "for route in list_route :\n",
    "    route_id += 1\n",
    "    print(\"Route= \"+str(route)+\" \"+str(route_id)+\"/\"+str(nb_route)+\": \")\n",
    "    df_delay_digit['NBROUTE'] = df_delay_digit[df_delay_digit['HROUTE']==route].HROUTE.apply(cb_route_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_route_drop = list()\n",
    "len(list_route_drop)        \n",
    "df_delay_digit[df_delay_digit['HROUTE']==548].NBROUTE.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure()\n",
    "\n",
    "ax = df_delay_digit['HROUTE'].hist(color='k', alpha=0.5, bins=150)\n",
    "\n",
    "z_ = ax.set(xlabel='Route ID')\n",
    "z_ = ax.set(ylabel='Fréquence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_route.HROUTE.value_counts()\n",
    "print(\"Nb de routes : \"+str(len(df_route.HROUTE.unique())))\n",
    "df_delay_digit.HROUTE.value_counts()\n",
    "df_delay_digit.loc[0]\n",
    "i = 0\n",
    "for route in df_delay_digit.HROUTE.value_counts().index.tolist():\n",
    "    if i == 30 :\n",
    "        break\n",
    "    else :\n",
    "        print(\"\\nRoute= \"+str(route))\n",
    "        print(df_delay_digit[df_delay_digit.HROUTE==route].iloc[0])\n",
    "    i+=1\n",
    "#for i in range(0,10,1)\n",
    "#len(df_delay_digit[df_delay_digit.HROUTE==216])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"New York, NY\"\n",
    "df_delay_digit[df_delay_digit.DEST_CITY_NAME==city].sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution des retards par route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def display_route_distrib(df, list_route) :\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for route in list_route :\n",
    "        df_route = df[df.HROUTE==route]\n",
    "        df_route = df_route[df_route['ARR_DELAY'] <=60]\n",
    "        origin_city_name = df_route.iloc[0]['ORIGIN_CITY_NAME']\n",
    "        dest_city_name = df_route.iloc[0]['DEST_CITY_NAME']\n",
    "        label = str(route)+\" : \"+origin_city_name+\" --> \"+dest_city_name\n",
    "        pyplot.hist(df_route.ARR_DELAY, bins=48, alpha=0.8, label=label)\n",
    "        print(\"Route \"+str(route)+\" Skew = %0.2F\" %df_route.ARR_DELAY.skew())\n",
    "\n",
    "    pyplot.legend(loc='upper left')\n",
    "    pyplot.title(\"Fréquende des retards\", color='b', fontsize=14)\n",
    "    pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_route = [216,1601]\n",
    "display_route_distrib(df_delay_digit, list_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_route = [2217,36]\n",
    "display_route_distrib(df_delay_digit, list_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_route = [1971,2217]\n",
    "display_route_distrib(df_delay_digit, list_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_route = [1971,3353]\n",
    "display_route_distrib(df_delay_digit, list_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_route = pd.concat([df_route, df_delay_digit.ORIGIN_CITY_NAME],axis=0)\n",
    "df_delay_digit = df_delay_digit[df_delay_digit['ARR_DELAY']<60]\n",
    "df_delay_digit.sample(3)\n",
    "for index_route in range(0,300,100) :\n",
    "    hroute_max = df_delay_digit.HROUTE.value_counts().index[index_route]\n",
    "    print(\"HROUTE : max route frequentations = \"+str(hroute_max))\n",
    "    df_route_max = df_delay_digit[df_delay_digit.HROUTE==hroute_max]\n",
    "    \n",
    "    origin_city_name = df_route_max.ORIGIN_CITY_NAME.unique()[0]\n",
    "    dest_city_name = df_route_max.DEST_CITY_NAME.unique()[0]\n",
    "    print( origin_city_name+\" --> \"+dest_city_name)\n",
    "    title = \"Retards en fonction des routes\"\n",
    "    axes = df_route_max.ARR_DELAY.hist(bins=50,label=title)\n",
    "\n",
    "    axes.set_title(\"Retards par route\")\n",
    "type(axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='blue'>Analyse de l'activité</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import LinearDelayPredictor\n",
    "from matplotlib import pyplot as py\n",
    "df_crs_dep_time = pd.DataFrame()\n",
    "df_crs_dep_time['CRS_DEP_TIME'] = df_delay_digit.CRS_DEP_TIME.copy()\n",
    "df_crs_dep_time = df_crs_dep_time.CRS_DEP_TIME.apply(LinearDelayPredictor.cb_convert_floathour_to_mn)\n",
    "print(df_crs_dep_time.min(),df_crs_dep_time.max())\n",
    "if True :\n",
    "    title = \"Activité au départ\"\n",
    "    xlabel = \"Heure de départ :\"\n",
    "    ylabel = \"Fréquence des départs\"\n",
    "\n",
    "    ser_hist(df_crs_dep_time,title,xlabel, ylabel,param_bins=50, param_font_size=12, param_rotation=90, figsize=(10,10))\n",
    "else : \n",
    "    df_crs_dep_time.hist(bins=150, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_crs_dep_time.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crs_dep_time = df_crs_dep_time.apply(LinearDelayPredictor.convert_mn_to_hour)\n",
    "print(df_crs_dep_time.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_crs_dep_time.hist(bins=48, figsize=(10,10))\n",
    "title = \"Activité au départ\"\n",
    "xlabel = \"Heure de départ :\"\n",
    "ylabel = \"Fréquence des départs\"\n",
    "\n",
    "ser_hist(df_crs_dep_time,title,xlabel, ylabel,param_bins=48, param_font_size=12, param_rotation=0, figsize=(10,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boxplot_list_display(pd.DataFrame(df_crs_dep_time), ['CRS_DEP_TIME'], show_outliers=True ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ser_list = df_delay_digit.HROUTE.sample(3)\n",
    "xlabel = \"Heure de départ :\"\n",
    "ylabel = \"Fréquence des départs\"\n",
    "for index in ser_list.index :\n",
    "    route = ser_list.loc[index]\n",
    "    origin = df_delay_digit.ORIGIN_CITY_NAME.loc[index]\n",
    "    dest = df_delay_digit.DEST_CITY_NAME.loc[index]\n",
    "    ser = df_delay_digit[df_delay_digit['HROUTE']==route].CRS_DEP_TIME\n",
    "    ser = ser.apply(LinearDelayPredictor.cb_convert_floathour_to_mn)\n",
    "    ser = ser.apply(LinearDelayPredictor.convert_mn_to_hour)\n",
    "    title = \"Activité au départ de \"+str(origin)+\" --> \"+str(dest)+\" : \"\n",
    "    ser_hist(ser,title,xlabel, ylabel,param_bins=15, param_font_size=12, param_rotation=0, figsize=(10,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_list = df_delay_digit.HROUTE.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.figure(figsize=(10,10))\n",
    "xlabel = \"Heure de départ\"\n",
    "ylabel = \"Fréquence des départs\"\n",
    "for index in ser_list.index :\n",
    "    route = ser_list.loc[index]\n",
    "    origin = df_delay_digit.ORIGIN_CITY_NAME.loc[index]\n",
    "    dest = df_delay_digit.DEST_CITY_NAME.loc[index]\n",
    "    ser = df_delay_digit[df_delay_digit['HROUTE']==route].CRS_DEP_TIME\n",
    "    ser = ser.apply(LinearDelayPredictor.cb_convert_floathour_to_mn)\n",
    "    ser = ser.apply(LinearDelayPredictor.convert_mn_to_hour)\n",
    "    \n",
    "    label = \"Route \"+str(route)+\" de \"+str(origin)+\" --> \"+str(dest)+\" : \"\n",
    "    #ser_hist(ser,title,xlabel, ylabel,param_bins=15, param_font_size=12, param_rotation=0, figsize=(10,10))\n",
    "    pyplot.hist(ser, bins=48, alpha=0.8, label=label)\n",
    "pyplot.legend(loc='upper left')\n",
    "\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.figure(figsize=(10,10))\n",
    "xlabel = \"Heure de départ\"\n",
    "ylabel = \"Fréquence des départs\"\n",
    "for index in ser_list.index :\n",
    "    route = ser_list.loc[index]\n",
    "    origin = df_delay_digit.ORIGIN_CITY_NAME.loc[index]\n",
    "    dest = df_delay_digit.DEST_CITY_NAME.loc[index]\n",
    "    ser = df_delay_digit[df_delay_digit['HROUTE']==route].ARR_DELAY\n",
    "    #ser = ser.apply(LinearDelayPredictor.cb_convert_floathour_to_mn)\n",
    "    #ser = ser.apply(LinearDelayPredictor.convert_mn_to_hour)\n",
    "    \n",
    "    label = \"Route \"+str(route)+\" de \"+str(origin)+\" --> \"+str(dest)+\" : \"\n",
    "    #ser_hist(ser,title,xlabel, ylabel,param_bins=15, param_font_size=12, param_rotation=0, figsize=(10,10))\n",
    "    pyplot.hist(ser, bins=48, alpha=0.8, label=label)\n",
    "pyplot.legend(loc='upper left')\n",
    "\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_display_linear_regression_from_list(pd.DataFrame(ser), 'ARR_DELAY',['CRS_DEP_TIME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='blue'>Analyse de la fréquentation aéroports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flight_frequency_threshold = zmax\n",
    "ser_most = p4_df_get_feature_count_from_threshold(df_delay_digit,'ORIGIN_AIRPORT_ID',flight_frequency_threshold)\n",
    "#----------------------------------------------------------------------------\n",
    "# Recuperation du nom de l'aeroport en fonction de son code.\n",
    "#----------------------------------------------------------------------------\n",
    "x= ser_most.index.values\n",
    "x_list_origin = p4_df_get_listName_from_listCode(df_delay_digit,x,ref_code='ORIGIN_AIRPORT_ID')\n",
    "\n",
    "y = ser_most.values\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title(\"Aeroports de plus de \"+str(int(flight_frequency_threshold))+\" vols/mois\")\n",
    "plt.ylabel(\"Nb vols par mois\")\n",
    "plt.bar(x_list_origin, height= y)\n",
    "plt.xticks(x_list_origin,rotation=90);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='blue'>Aeroports les moins frequentés</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_frequency_threshold = q1\n",
    "ser_most = p4_df_get_feature_count_from_threshold(df_delay_digit,'ORIGIN_AIRPORT_ID',flight_frequency_threshold, direction=-1)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Recuperation du nom de l'aeroport en fonction de son code.\n",
    "#----------------------------------------------------------------------------\n",
    "x= ser_most.index.values\n",
    "x_list_origin = p4_df_get_listName_from_listCode(df_delay_digit,x,ref_code='ORIGIN_AIRPORT_ID')\n",
    "\n",
    "y = ser_most.values\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title(\"Aeroports de plus de \"+str(flight_frequency_threshold)+\" vols/mois\")\n",
    "plt.ylabel(\"Nb vols par mois\")\n",
    "plt.bar(x_list_origin, height= y)\n",
    "plt.xticks(x_list_origin,rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='blue'>Retard moyen par aeroport les plus fréquentés</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# Aggregation dans un dataframe des Series des aeroports les plus frequentés\n",
    "#----------------------------------------------------------------------------\n",
    "df_most= pd.DataFrame()\n",
    "for col in ser_most.index.tolist() :\n",
    "    df_most = pd.concat([df_most,df_delay_digit[df_delay_digit['ORIGIN_AIRPORT_ID']==col]],axis=0)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Les données sont groupées par aeroport, le retard moyen est calculé par \n",
    "# aeroport.\n",
    "#----------------------------------------------------------------------------\n",
    "df_most.groupby('ORIGIN_AIRPORT_ID')\n",
    "ser = df_most.groupby(['ORIGIN_AIRPORT_ID'])['ARR_DELAY'].mean()\n",
    "#print(ser)\n",
    "ser.sort_values(inplace=True)\n",
    "#print(ser)\n",
    "x= ser.index.values\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Recuperation du nom de l'aeroport en fonction de son code.\n",
    "#----------------------------------------------------------------------------\n",
    "x_list_origin = p4_df_get_listName_from_listCode(df_most,x,ref_code='ORIGIN_AIRPORT_ID')\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Affichage du diagramme\n",
    "#----------------------------------------------------------------------------\n",
    "y = ser.values\n",
    "#y = ser.sort_values(inplace=False).values\n",
    "#print(y)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Retards moyens pour les aéroports les plus frequentés\")\n",
    "plt.bar(x_list_origin, height= y)\n",
    "plt.xlabel(\"Aeroports\")\n",
    "plt.xlabel(\"Retard moyen / vol en mn\")\n",
    "plt.xticks(x_list_origin,rotation=90);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='blue'>Distribution des retards selon les aeroports</font>\n",
    " * Calcul, par aéroport, des retards moyens\n",
    " * Selection des données en fonction des valeurs remarquables de la boxplot de la moyenne des retards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# Calcul de la moyenne des retards sur le jeux de données\n",
    "#----------------------------------------------------------------------------\n",
    "delay_mean = df_delay_digit['ARR_DELAY'].mean()\n",
    "print(\"Retard moyen global = \"+str(delay_mean))\n",
    "q1,q3,zmin, zmax = df_boxplot_limits(df_delay_digit , 'ARR_DELAY')\n",
    "print(\"Valeur moustache inférieure = \"+str(zmin))\n",
    "print(\"Valeur quantile 1 = \"+str(q1))\n",
    "print(\"Valeur quantile 3 = \"+str(q3))\n",
    "print(\"Valeur moustache supérieure = \"+str(zmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = q3\n",
    "#----------------------------------------------------------------------------\n",
    "# Les données sont groupées par aeroport, le retard moyen est calculé par \n",
    "# aeroport.\n",
    "#----------------------------------------------------------------------------\n",
    "ser = df_delay_digit.groupby(['ORIGIN_AIRPORT_ID'])['ARR_DELAY'].mean()\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Calcul du filtre\n",
    "#----------------------------------------------------------------------------\n",
    "ser_filetered = ser>criteria\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Application du filtre\n",
    "#----------------------------------------------------------------------------\n",
    "ser = ser[ser_filetered]\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Affichage sous forme d'histogramme\n",
    "#----------------------------------------------------------------------------\n",
    "plt.figure(figsize=(10,10))\n",
    "parameter_rotation = 90\n",
    "x_code = ser.index.values\n",
    "\n",
    "x_label = p4_df_get_listName_from_listCode(df_delay_digit,list(x_code),ref_code='ORIGIN_AIRPORT_ID')\n",
    "y = ser.values\n",
    "plt.bar(x_label, height= y)\n",
    "plt.ylabel(\"Retards en mn\")\n",
    "plt.xticks(x_label,rotation=parameter_rotation);\n",
    "z_=plt.title(\"Aeroports dont le retard en moyenne est > 3eme Quantile\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = q1\n",
    "#----------------------------------------------------------------------------\n",
    "# Les données sont groupées par aeroport, le retard moyen est calculé par \n",
    "# aeroport.\n",
    "#----------------------------------------------------------------------------\n",
    "ser = df_delay_digit.groupby(['ORIGIN_AIRPORT_ID'])['ARR_DELAY'].mean()\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Calcul du filtre\n",
    "#----------------------------------------------------------------------------\n",
    "ser_filetered = ser<criteria\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Application du filtre\n",
    "#----------------------------------------------------------------------------\n",
    "ser = ser[ser_filetered]\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Affichage sous forme d'histogramme\n",
    "#----------------------------------------------------------------------------\n",
    "plt.figure(figsize=(10,10))\n",
    "parameter_rotation = 90\n",
    "x_code = ser.index.values\n",
    "\n",
    "x_label = p4_df_get_listName_from_listCode(df_delay_digit,list(x_code),ref_code='ORIGIN_AIRPORT_ID')\n",
    "y = ser.values\n",
    "plt.bar(x_label, height= y)\n",
    "plt.ylabel(\"Retards en mn\")\n",
    "plt.xticks(x_label,rotation=parameter_rotation);\n",
    "z_=plt.title(\"Aeroports dont le retard en moyenne est < 1er quantile\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = df_delay_digit['ARR_DELAY'].median()\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Les données sont groupées par aeroport, le retard moyen est calculé par \n",
    "# aeroport.\n",
    "#----------------------------------------------------------------------------\n",
    "ser = df_delay_digit.groupby(['ORIGIN_AIRPORT_ID'])['ARR_DELAY'].mean()\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Calcul du filtre\n",
    "#----------------------------------------------------------------------------\n",
    "ser_filetered = ser<criteria\n",
    "#----------------------------------------------------------------------------\n",
    "# Application du filtre\n",
    "#----------------------------------------------------------------------------\n",
    "ser = ser[ser_filetered]\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Affichage sous forme d'histogramme\n",
    "#----------------------------------------------------------------------------\n",
    "plt.figure(figsize=(10,10))\n",
    "parameter_rotation = 90\n",
    "x_code = ser.index.values\n",
    "\n",
    "x_label = p4_df_get_listName_from_listCode(df_delay_digit,list(x_code),ref_code='ORIGIN_AIRPORT_ID')\n",
    "y = ser.values\n",
    "plt.bar(x_label, height= y)\n",
    "plt.ylabel(\"Retards en mn\")\n",
    "plt.xticks(x_label,rotation=parameter_rotation);\n",
    "z_=plt.title(\"Aeroports dont le retard en moyenne est < 1er quantile\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='blue'>Analyse des routes dans le modèle</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from decimal import Decimal\n",
    "\n",
    "df_route = df_delay_digit[['ORIGIN_CITY_NAME','DEST_CITY_NAME']]\n",
    "df_route['HROUTE'] = df_delay_digit['ORIGIN_CITY_NAME'] + df_delay_digit['DEST_CITY_NAME']\n",
    "\n",
    "df_route['HROUTE'] = df_route['HROUTE'].apply(lambda val: (\"0x\"+hashlib.md5(val.encode()).hexdigest()))\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df_route['HROUTE'])\n",
    "df_route['HROUTE'] = le.transform(df_route['HROUTE'])\n",
    "\n",
    "if is_df_with_route is True :\n",
    "    df_delay_digit['HROUTE'] = df_route['HROUTE'].copy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "xlabel = ''\n",
    "ylabel = ''\n",
    "ser_hist(df_route.HROUTE,'Routes',xlabel, ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(df_route['HROUTE'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(\"Nombre de routes du modèle= \"+str(len(df_route.HROUTE.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hroute_freq_max = df_route.HROUTE.value_counts(ascending=False).index[0]\n",
    "hroute_freq_min = df_route.HROUTE.value_counts(ascending=True).index[0]\n",
    "hroute_freq_mean = df_route.HROUTE.value_counts(ascending=True).index[1500]\n",
    "hroute_freq_min,hroute_freq_max,hroute_freq_mean\n",
    "\n",
    "origin_max_city_name = df_route[df_route.HROUTE==hroute_freq_max].ORIGIN_CITY_NAME.unique()\n",
    "dest_max_city_name = df_route[df_route.HROUTE==hroute_freq_max].DEST_CITY_NAME.unique()\n",
    "\n",
    "origin_min_city_name = df_route[df_route.HROUTE==hroute_freq_min].ORIGIN_CITY_NAME.unique()\n",
    "dest_min_city_name = df_route[df_route.HROUTE==hroute_freq_min].DEST_CITY_NAME.unique()\n",
    "\n",
    "origin_mean_city_name = df_route[df_route.HROUTE==hroute_freq_mean].ORIGIN_CITY_NAME.unique()\n",
    "dest_mean_city_name = df_route[df_route.HROUTE==hroute_freq_mean].DEST_CITY_NAME.unique()\n",
    "\n",
    "\n",
    "print(\"Route la plus fréquentée : \"+origin_max_city_name+\" --> \"+dest_max_city_name)\n",
    "print(\"Route la moins fréquentée : \"+origin_min_city_name+\" --> \"+dest_min_city_name)\n",
    "print(\"Route moyenemant fréquentée : \"+origin_mean_city_name+\" --> \"+dest_mean_city_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_route.HROUTE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_frequency_threshold = 0\n",
    "ser_most_origin = \\\n",
    "p4_df_get_feature_count_from_threshold(df_delay_digit,'ORIGIN_AIRPORT_ID'\\\n",
    "                                       ,flight_frequency_threshold, direction=1)\n",
    "ser_most_dest = \\\n",
    "p4_df_get_feature_count_from_threshold(df_delay_digit,'DEST_AIRPORT_ID',flight_frequency_threshold, direction=1)\n",
    "ser_most_origin.sort_values(ascending=False, inplace=True)\n",
    "ser_most_dest.sort_values(ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='blue'>Variation des retards en fonction d'une route et d'un paramètre additionnel</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_route.HROUTE.unique())\n",
    "route=2544,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Variation des retards en fonction des transporteurs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4_df_plot_delay_route_parameter(df_delay_digit,route,'AIRLINE_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Variation des retards en fonction des dates de vol</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p4_df_plot_delay_route_parameter(df_delay_digit,route,'FL_DATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Variation des retards en fonction du jour de la semaine</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code,Description :**\n",
    "\n",
    "* \"1\",\"Monday\"\n",
    "* \"2\",\"Tuesday\"\n",
    "* \"3\",\"Wednesday\"\n",
    "* \"4\",\"Thursday\"\n",
    "* \"5\",\"Friday\"\n",
    "* \"6\",\"Saturday\"\n",
    "* \"7\",\"Sunday\"\n",
    "* \"9\",\"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4_df_plot_delay_route_parameter(df_delay_digit,route,'DAY_OF_WEEK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Variation des retards en fonction du jour dans le mois</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4_df_plot_delay_route_parameter(df_delay_digit,route,'DAY_OF_MONTH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Variation des retards en fonction l'heure de départ programmé</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4_df_plot_delay_route_parameter(df_delay_digit,route,'CRS_DEP_TIME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <font color='blue'>Analyse de la distribution des variables</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_parameter = dict()\n",
    "list_col_time = ['DAY_OF_MONTH', 'DAY_OF_WEEK', 'AIRLINE_ID',\n",
    "       'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID',\n",
    "       'CRS_DEP_TIME', 'CRS_ARR_TIME', 'ARR_DELAY',\n",
    "       'CRS_ELAPSED_TIME', 'DISTANCE']\n",
    "\n",
    "\n",
    "if 'DISTANCE' in df_delay_digit :\n",
    "        dict_parameter['DISTANCE'] = ['Variable DISTANCE', 'Distances','Fréquence']\n",
    "    \n",
    "if 'CRS_ELAPSED_TIME' in df_delay_digit :\n",
    "    dict_parameter['CRS_ELAPSED_TIME'] = ['Variable CRS_ELAPSED_TIME','Temps de vol prévue (mn)','Fréquence']\n",
    "\n",
    "if 'CRS_DEP_TIME' in df_delay_digit :\n",
    "    dict_parameter['CRS_DEP_TIME'] = ['Variable CRS_DEP_TIME','Heures de départ prévue','Fréquence']\n",
    "\n",
    "if 'CRS_DEP_TIME' in df_delay_digit :\n",
    "    dict_parameter['DAY_OF_MONTH'] = ['Variable DAY_OF_MONTH','Jours du mois','Fréquence']\n",
    "\n",
    "if 'DAY_OF_WEEK' in df_delay_digit :\n",
    "    dict_parameter['DAY_OF_WEEK'] = ['Variable DAY_OF_WEEK','Jours de la semaine','Fréquence']\n",
    "\n",
    "if 'ARR_DELAY' in df_delay_digit :\n",
    "    dict_parameter['ARR_DELAY'] = ['Variable ARR_DELAY','Retards a l\\'arrivée','Fréquence']\n",
    "\n",
    "if 'AIRLINE_ID' in df_delay_digit :\n",
    "    dict_parameter['AIRLINE_ID'] = ['Variable AIRLINE_ID','Compagnies','Fréquence']\n",
    "\n",
    "if 'ORIGIN_AIRPORT_ID' in df_delay_digit :\n",
    "    dict_parameter['ORIGIN_AIRPORT_ID'] = ['Variable ORIGIN_AIRPORT_ID','Aéroports de départ','Fréquence']\n",
    "\n",
    "if 'DEST_AIRPORT_ID' in df_delay_digit :\n",
    "    dict_parameter['DEST_AIRPORT_ID'] = ['Variable DEST_AIRPORT_ID','Aéroports d\\'arrivée','Fréquence']\n",
    "\n",
    "for key, list_param_display in dict_parameter.items() :\n",
    "    ser_hist(df_delay_digit[key],list_param_display[0],list_param_display[1], list_param_display[2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "list_data_analysis= ['MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'AIRLINE_ID',\n",
    "        'ORIGIN_CITY_NAME', 'DEST_CITY_NAME', 'CRS_DEP_TIME', 'ARR_DELAY', 'CRS_ELAPSED_TIME']\n",
    "if True :\n",
    "    pos = 0\n",
    "    for item in list_data_analysis :\n",
    "        ax = fig.add_subplot(5,4, (pos+1))\n",
    "        X = df_delay_digit[item]\n",
    "        h = ax.hist(X, bins=50, color='steelblue', edgecolor='none', normed=True)\n",
    "        ax.set_title(item, fontsize=8)\n",
    "        pos +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarités entre les distributions des variables obervées : **\n",
    "\n",
    "* *ORIGIN_AIRPORT_ID et DEST_AIRPORT_ID*\n",
    "* *CRS_DEP_TIME et CRS_ARR_TIME*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> PCA  </font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_scaled_ = df_pca_all_plot(df_delay_digit)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get standardized data\n",
    "X_scaled = df_get_std_scaled_values(df_delay_digit)\n",
    "\n",
    "#Build PCA algorithme.\n",
    "nb_components = 2\n",
    "pca = PCA(n_components=nb_components)\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "X_projected = pca.transform(X_scaled)\n",
    "print(X_projected.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_pcs2_plot(df_delay_digit, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit_corr = df_delay_digit.corr()\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "_z = sns.heatmap(df_delay_digit_corr, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'FL_DATE' in df_delay_digit :\n",
    "    df_delay_digit.FL_DATE.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blues'>Regressions linéaires</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Choix des variables</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variable cible (y):**\n",
    "* ARR_DELAY\n",
    "\n",
    "**Choix des variables (X):**\n",
    "* MONTH\n",
    "* DAY_OF_MONTH\n",
    "* DAY_OF_WEEK\n",
    "* AIRLINE_ID\n",
    "* ORIGIN_AIRPORT_ID\n",
    "* DEST_AIRPORT_ID\n",
    "* CRS_DEP_TIME\n",
    "* CRS_ELAPSED_TIME\n",
    "\n",
    "**Variables quantitatives:**\n",
    "* CRS_DEP_TIME\n",
    "* CRS_ARR_TIME pour rendre transparent les fuseaux horraires.\n",
    "\n",
    "**Encodage des variables :**\n",
    "* MONTH\n",
    "* AIRLINE_ID\n",
    "* DAY_OF_MONTH\n",
    "* DAY_OF_WEEK\n",
    "* ORIGIN_AIRPORT_ID\n",
    "* DEST_AIRPORT_ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Hypothéses sous-jacentes à la regression linéaire</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Hypothèse IID </font>\n",
    "* Les couples (Xi,yi) sont susceptibles d'être indépendants les uns des autres\n",
    "* Toutes les variables ont une probabilité de distribution identiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le mois \n",
    "list_var = ['MONTH','AIRLINE_ID','DAY_OF_MONTH', 'DAY_OF_WEEK', 'ORIGIN_AIRPORT_ID'\\\n",
    "                      , 'DEST_AIRPORT_ID','CRS_DEP_TIME','DISTANCE','ARR_DELAY']\n",
    "df_corr = df_delay_digit[list_var].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "_z = sns.heatmap(df_corr, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit_save = df_delay_digit.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Construction du modèle numérique des données</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_delay_digit = df_delay_digit_save.copy()\n",
    "df_delay_digit_save.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discrimination du retard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_delay_digit.shape)\n",
    "\n",
    "for key in dict_delay_splitted.keys() :\n",
    "    if dict_delay_splitted[key] == 1 :\n",
    "        if key == 'past' :\n",
    "            df_delay_digit = df_delay_digit[df_delay_digit['ARR_DELAY'] <=0]\n",
    "            break\n",
    "        if key == 'futur' :\n",
    "            df_delay_digit = df_delay_digit[df_delay_digit['ARR_DELAY'] >0]\n",
    "            break\n",
    "\n",
    "print(df_delay_digit.shape)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modele par route**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if is_route_in_model is True :\n",
    "    origin_city_name = origin_max_city_name[0]\n",
    "    dest_city_name = dest_max_city_name[0]\n",
    "    printmd(\"Modele pour la route \"+origin_city_name+\" --> \"+ dest_city_name)\n",
    "\n",
    "    print(df_delay_digit.shape)\n",
    "    df_delay_digit = df_delay_digit[df_delay_digit['ORIGIN_CITY_NAME']==origin_city_name]\n",
    "    df_delay_digit = df_delay_digit[df_delay_digit['DEST_CITY_NAME']==dest_city_name]\n",
    "    print(df_delay_digit.shape)\n",
    "else :\n",
    "   print(df_delay_digit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit.AIRLINE_ID.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtrage par transporteur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_carrier_model is True :\n",
    "    \n",
    "    print(df_delay_digit.shape)\n",
    "    df_delay_digit = df_delay_digit[df_delay_digit['AIRLINE_ID']==19790]\n",
    "    del(df_delay_digit['AIRLINE_ID'])\n",
    "    print(df_delay_digit.shape)\n",
    "else :\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtrage des outliers sur les retards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end = 0\n",
    "if is_delay_outlier_removed is True :\n",
    "    zmin, zmax = df_boxplot_min_max(df_delay_digit , 'ARR_DELAY')\n",
    "\n",
    "    print(df_delay_digit.shape)\n",
    "    start = df_delay_digit.shape[0]\n",
    "    df_delay_digit = df_delay_digit[df_delay_digit['ARR_DELAY']<zmax]\n",
    "    df_delay_digit = df_delay_digit[df_delay_digit['ARR_DELAY']>zmin]\n",
    "    print(df_delay_digit.shape)\n",
    "    end = df_delay_digit.shape[0]\n",
    "    print(\"Pourcent valeurs outliers écrêtées : %0.2f\" %((start-end)*100/start))\n",
    "else :\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Vecteur cible</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Variable etiquette\n",
    "# --------------------------------------------------\n",
    "list_target = ['ARR_DELAY']\n",
    "y = df_delay_digit[list_target].values\n",
    "y.min(),y.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Variables de référence</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ref = ['ORIGIN_CITY_NAME','DEST_CITY_NAME']\n",
    "#df_ref = df_delay_digit[list_ref]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Variables quantitatives</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_quantitative = ['CRS_DEP_TIME']\n",
    "# ------------------------------------------------\n",
    "# Recuperation du tableau de données quantitatives\n",
    "# ------------------------------------------------\n",
    "X_quantitative = df_delay_digit[list_quantitative].values\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Conversion du type entier en float\n",
    "# ------------------------------------------------\n",
    "X_quantitative = X_quantitative.astype(float)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Nombre de colonnes pour les données qualitatives\n",
    "# ------------------------------------------------\n",
    "col_quant_count = X_quantitative.shape[1]\n",
    "printmd(col_quant_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_quantitative[:,0:col_quant_count].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Standardisation des données numériques</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scale       = preprocessing.StandardScaler().fit(X_quantitative)\n",
    "X_quantitative_std    = std_scale.transform(X_quantitative)\n",
    "X_quantitative_std.min(),X_quantitative_std.max()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### <font color='blue'>Vecteur cible standardisé</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y = X_quantitative_std[:,col_quant_count-1:col_quant_count]\n",
    "X_quantitative_std= X_quantitative_std[:,0:col_quant_count-1]\n",
    "X_quantitative_std.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Variables qualitatives et encodage</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construction de la liste qualitative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_qualitative_excluded = list()\n",
    "#----------------------------------------------------------------------------\n",
    "# Les aeroports d'origine et de destination sont exclus du modèle car \n",
    "# remplacés par la variable HROUTE\n",
    "#----------------------------------------------------------------------------\n",
    "if 'HROUTE' in df_delay_digit.columns :\n",
    "    list_qualitative_excluded = ['ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID']\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Les aeroports d'origine et de destination sont exclus du modèle car \n",
    "# le dataframe a été filtré par ORIGIN_AIRPORT_ID et DEST_AIRPORT_ID de \n",
    "# route.\n",
    "#----------------------------------------------------------------------------\n",
    "if is_route_in_model is True :\n",
    "    list_qualitative_excluded = ['ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construction de la liste des variables qualitatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------\n",
    "# Construction de la liste des variables qualitatives.\n",
    "#----------------------------------------------------------------------------\n",
    "list_qualitative_excluded.append('DISTANCE')\n",
    "list_qualitative_excluded.append('FL_NUM')\n",
    "list_qualitative_excluded.append('CARRIER')\n",
    "list_qualitative = list()\n",
    "for col in df_delay_digit.columns :\n",
    "    if col not in list_target :\n",
    "        if col not in list_quantitative:\n",
    "            if col not in list_ref :\n",
    "                if col not in list_qualitative_excluded :\n",
    "                    list_qualitative.append(col)\n",
    "\n",
    "print(list_ref)\n",
    "print(list_qualitative)\n",
    "print(list_target)\n",
    "print(list_quantitative)\n",
    "print(list_qualitative_excluded)\n",
    "print(df_delay_digit.columns)\n",
    "print(df_delay_digit.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encodage de la liste qualitative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder() \n",
    "sparse_col_encoded = encoder.fit_transform(df_delay_digit[list_qualitative].values)\n",
    "sparse_col_encoded.shape, sparse_col_encoded.min(),sparse_col_encoded.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Agrégation du modèle de données : variables quantitatives et qualitatives encodées</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Transformation des données standardisées en sparse\n",
    "# --------------------------------------------------\n",
    "sparse_X = sparse.csr_matrix(X_quantitative_std)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Aggregation des structures sparses\n",
    "# --------------------------------------------------\n",
    "if True :\n",
    "    X_std = sparse.hstack((sparse_X, sparse_col_encoded))\n",
    "else :\n",
    "    X_std =  sparse_col_encoded.copy()\n",
    "    X_std =  sparse_X.copy()\n",
    "    \n",
    "X_std.min(),X_std.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Jeux de données d'entraînement et de test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std, X_test_std, y_train, y_test = \\\n",
    "model_selection.train_test_split(X_std, y, test_size=0.3)\n",
    "\n",
    "#X_train_std = X_train_std.toarray()\n",
    "#X_test_std = X_test_std.toarray()\n",
    "y_train.shape,X_train_std.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blues'>Regressions du package sklearn.linear_model</color>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Prédictions de l'estimateur Dummy </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import dummy\n",
    "dum = dummy.DummyRegressor(strategy='mean')\n",
    "\n",
    "# Entraînement\n",
    "dum.fit(X_train_std, y_train)\n",
    "\n",
    "# Prédiction sur le jeu de test\n",
    "y_predict = dum.predict(X_test_std)\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "\n",
    "print(\"Route in model = {}\".format(is_route_in_model))\n",
    "print(\"Carrier in model = {}\".format(is_carrier_model))\n",
    "print(\"Delay outlier removed = {}\".format(is_delay_outlier_removed))\n",
    "print(\"Delay splitting config.= {}\".format(dict_delay_splitted))\n",
    "\n",
    "print(\"\")\n",
    "mse = mean_squared_error(y_test, y_predict)\n",
    "mae = mean_absolute_error(y_test, y_predict)\n",
    "score_r2 = dum.score(X_test_std, y_test)\n",
    "\n",
    "print(\"Regression Dummy R2  = %0.6f\" %score_r2)\n",
    "print(\"Regression Dummy MSE = %0.6f\" %mse)\n",
    "print(\"Regression Dummy MAE = %0.6f\" %mae)\n",
    "print(\"Model train size = {}\".format(X_train_std.shape))\n",
    "print(\"Model test size = {}\".format(X_test_std.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Prédictions de l'estimateur LinearRegression </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(normalize=False)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "# Entraînement du modèle\n",
    "#-------------------------------------------------------------\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "# Calcul des prédictions\n",
    "#-------------------------------------------------------------\n",
    "y_predict = lr.predict(X_test_std)\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "# Calcul des scores R2, MAE et MSE\n",
    "#-------------------------------------------------------------\n",
    "mse = mean_squared_error(y_test, y_predict)\n",
    "mae = mean_absolute_error(y_test, y_predict)\n",
    "score_r2 = lr.score(X_test_std, y_test)\n",
    "\n",
    "print(\"Route in model = {}\".format(is_route_in_model))\n",
    "print(\"Carrier in model = {}\".format(is_carrier_model))\n",
    "print(\"Delay outlier removed = {}\".format(is_delay_outlier_removed))\n",
    "print(\"Delay splitting config.= {}\".format(dict_delay_splitted))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Regression linéaire R2  = %0.6f\" %score_r2)\n",
    "print(\"Regression linéaire MSE = %0.6f\" %mse)\n",
    "print(\"Regression linéaire MAE = %0.6f\" %mae)\n",
    "print(\"Model train size = {}\".format(X_train_std.shape))\n",
    "print(\"Model test size = {}\".format(X_test_std.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, y_predict, edgecolors=(0, 0, 0))\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2,color='r')\n",
    "#plt.title(\"Route \"+origin_city_name+\" --> \"+origin_city_name+\" : retards prédits fonction des retards observés\")\n",
    "ax.set_xlabel('Observations')\n",
    "ax.set_ylabel('Prédictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le nombre de points superposés vient du fait des valeurs de retards sans décimale.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = {} # clé : coordonnées ; valeur : nombre de points à ces coordonnées\n",
    "for (yt, yp) in zip(list(y_test), list(y_predict)):\n",
    "    \n",
    "    if (yt[0],yp[0]) in sizes.keys() :\n",
    "        sizes[(yt[0], yp[0])] += 1\n",
    "        print(sizes[(yt[0], yp[0])])\n",
    "    else:\n",
    "        sizes[(yt[0], yp[0])] = 1\n",
    "\n",
    "keys = sizes.keys()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter([k[0] for k in keys], # valeurs observées (axe X)\n",
    "[k[1] for k in keys], # valeurs predites (axe Y)\n",
    "s=[sizes[k] for k in keys], # taille du marqueur\n",
    "color='coral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_error = [ (y_test[j]-y_predict[j])**2  for j in range(0,y_predict.shape[0])]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mse_error = mean_squared_error(y_test, y_predict)\n",
    "mae_error = mean_absolute_error(y_test, y_predict)\n",
    "list_mae_error = np.abs(y_test- y_predict)\n",
    "list_mse_error = (y_test- y_predict)**2\n",
    "print(\"Erreur RMSE : %0.3F\"%np.sqrt(mse_error))\n",
    "print(\"Erreur MAE : %0.3F\"%mae_error)\n",
    "\n",
    "list_error = np.sqrt(list_mse_error)\n",
    "\n",
    "ax = plt.gca()\n",
    "# Affichage de dexu courbes : \n",
    "# --> Courbe 1 : list_alpha,list_error_ridge\n",
    "# --> Courbe 2 : [10**-5, 10**5],[baseline_error,baseline_error]\n",
    "z_ = ax.plot(range(0,y_predict.shape[0]),list_error )\n",
    "ax.set_xscale('log')\n",
    "#ax.set_yscale('log')\n",
    "plt.show()\n",
    "\n",
    "## <font color='blue'>Prédictions avec l'estimateur SGDRegressor</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "import numpy as np\n",
    "n_alphas = 100\n",
    "\n",
    "SGD_params = {'alpha': np.logspace(-6,0)} \n",
    "#print(SGD_params)\n",
    "SGD_model = GridSearchCV(SGDRegressor(random_state = 0), param_grid=SGD_params\\\n",
    "                         , scoring = 'mean_absolute_error', cv = 5, verbose=False)\n",
    "SGD_model.fit(X_train_std, y_train)\n",
    "\n",
    "print(\"Meilleur paramètre = {}\".format(SGD_model.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Prédictions avec l'estimateur RIDGE</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "n_alphas = 50\n",
    "list_alphas = np.logspace(-5, 5, n_alphas)\n",
    "\n",
    "ridge = Ridge()\n",
    "\n",
    "list_coefs = list()\n",
    "list_mse_error = list()\n",
    "list_r2_error = list()\n",
    "list_msa_error = list()\n",
    "\n",
    "t0 = time.time()\n",
    "iter = 0\n",
    "for a in list_alphas:\n",
    "    ridge.set_params(alpha=a)\n",
    "    ridge.fit(X_train_std, y_train)\n",
    "    \n",
    "    list_coefs.append(ridge.coef_)\n",
    "    \n",
    "    y_predict = ridge.predict(X_test_std)\n",
    "    list_mse_error.append(np.mean(( y_predict - y_test) ** 2))\n",
    "    list_r2_error.append(r2_score(y_test, y_predict))\n",
    "    list_msa_error.append(mean_absolute_error(y_test, y_predict))\n",
    "    iter +=1\n",
    "    if iter%10 == 0 :\n",
    "        print(\"Iter status: {}/{} Alpha = {}\".format(iter, len(list_alphas), a))\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"RIDGE :Moyenne R2 = %0.6f\" %np.mean(list_r2_error))\n",
    "print(\"RIDGE :Moyenne MSE = %0.6f\" %np.mean(list_mse_error))\n",
    "print(\"RIDGE :Moyenne MAE = %0.6f\" %np.mean(list_msa_error))\n",
    "printmd(\"RIDGE :Temps de résolution = %0.6f\" %(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_index_mse = np.argmin(list_mse_error)\n",
    "min_index_msa = np.argmin(list_msa_error)\n",
    "\n",
    "alpha_best = list_alphas[min_index_mse]\n",
    "\n",
    "printmd(\"Valeur optimale de alpha = %0.3f\" %alpha_best)\n",
    "printmd(\"Valeur optimale MSE = %0.3f\" %list_mse_error[min_index_mse])\n",
    "printmd(\"Valeur optimale MAE = %0.3f\" %list_msa_error[min_index_msa])\n",
    "\n",
    "\n",
    "ridge.set_params(alpha = alpha_best)\n",
    "ridge.fit(X_train_std, y_train)\n",
    "y_ridge_best_predict = ridge.predict(X_test_std)\n",
    "\n",
    "printmd(\"Erreur MSE optimale = %0.3f\" %(np.mean(( y_ridge_best_predict - y_test) ** 2)))\n",
    "printmd(\"Erreur MAE optimale = %0.3f\" %(mean_absolute_error(y_test, y_ridge_best_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chemin de régularisation RIDGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raws = list_alphas.shape[0]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.gca()\n",
    "arr_coef_reshaped =  np.array(list_coefs).reshape(raws,-1)\n",
    "ax.plot(list_alphas,arr_coef_reshaped)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Coef. de régularisation')\n",
    "plt.ylabel('Poids')\n",
    "plt.title('Poids fonction des coefficients de régularisation',fontsize=14,color='b')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Erreur fonction des coefficients de RIDGE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raws = list_alphas.shape[0]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.gca()\n",
    "ax.plot(list_alphas,list_mse_error)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Coefficients')\n",
    "plt.ylabel('Erreur MSE')\n",
    "plt.title('Erreur fonction des coefficients de régularisation',fontsize=14,color='b')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raws = list_alphas.shape[0]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.gca()\n",
    "ax.plot(list_alphas,list_msa_error)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Coefficients')\n",
    "plt.ylabel('Erreur MSA')\n",
    "plt.title('Erreur fonction des coefficients de régularisation',fontsize=14,color='b')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Prédictions avec l'estimateur LASSO</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "n_alphas = 50\n",
    "list_alphas = np.logspace(-4, 4, n_alphas)\n",
    "\n",
    "lasso = Lasso()\n",
    "\n",
    "list_lasso_coefs = list()\n",
    "list_lasso_mse_error = list()\n",
    "list_lasso_r2_error = list()\n",
    "list_lasso_mae_error = list()\n",
    "\n",
    "iter = 0\n",
    "t0 = time.time()\n",
    "\n",
    "for a in list_alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X_train_std, y_train)\n",
    "    \n",
    "    list_lasso_coefs.append(lasso.coef_)\n",
    "    \n",
    "    y_predict = lasso.predict(X_test_std)\n",
    "    list_lasso_mse_error.append(mean_squared_error(y_test, y_predict))\n",
    "    list_lasso_r2_error.append(r2_score(y_test, y_predict))\n",
    "    list_lasso_mae_error.append(mean_absolute_error(y_test, y_predict))\n",
    "    iter +=1\n",
    "    if iter%10 == 0 :\n",
    "        print(\"Iter status: {}/{} \".format(iter, len(list_alphas))+\" Alpha = %0.6f\" %a)\n",
    "    \n",
    "t1= time.time()\n",
    "print(\"LASSO : Moyenne R2 = %0.6f\" %np.mean(list_lasso_r2_error))\n",
    "print(\"LASSO : Moyenne MSE = %0.6f\" %np.mean(list_lasso_mse_error))\n",
    "print(\"LASSO : Moyenne MAE = %0.6f\" %np.mean(list_lasso_mae_error))\n",
    "printmd(\"LASSO : Temps de résolution= %0.6f\" %(t1-t0))\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# Utilisation du meilleur score pour produire le modele LASSO\n",
    "#-------------------------------------------------------------------\n",
    "min_index_mse = np.argmin(list_lasso_mse_error)\n",
    "lasso_best_alpha = list_alphas[min_index_mse]\n",
    "lasso.set_params(alpha=lasso_best_alpha)\n",
    "\n",
    "lasso.fit(X_train_std, y_train)\n",
    "y_predict = lasso.predict(X_test_std)\n",
    "\n",
    "print(\"LASSO : Coeff = %0.6f\" %lasso_best_alpha)\n",
    "print(\"LASSO : R2 = %0.6f\" %r2_score(y_test, y_predict))\n",
    "print(\"LASSO : MSE = %0.6f\" %mean_squared_error(y_test, y_predict))\n",
    "print(\"LASSO : MAE = %0.6f\" %mean_absolute_error(y_test, y_predict))\n",
    "\n",
    "\n",
    "min_lasso_index_error = np.argmin(list_lasso_mae_error)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(y_test, y_predict, edgecolors=(0, 0, 0))\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2,color='r')\n",
    "#plt.title(\"Route \"+origin_city_name+\" --> \"+origin_city_name+\" : retards prédits fonction des retards observés\")\n",
    "ax.set_xlabel('Observations')\n",
    "ax.set_ylabel('Prédictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chemin de régularisation LASSO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raws = list_alphas.shape[0]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.gca()\n",
    "arr_coef_reshaped =  np.array(list_coefs).reshape(raws,-1)\n",
    "ax.plot(list_alphas,arr_coef_reshaped)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Coeff. régularisation')\n",
    "plt.ylabel('Poids')\n",
    "plt.title('LASSO : poids fonction des coefficients de régularisation',fontsize=14,color='b')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raws = list_alphas.shape[0]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.gca()\n",
    "ax.plot(list_alphas,list_lasso_mse_error)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Coefficients')\n",
    "plt.ylabel('Erreur MSA')\n",
    "plt.title('Erreur fonction des coefficients de régularisation',fontsize=14,color='b')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raws = list_alphas.shape[0]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.gca()\n",
    "ax.plot(list_alphas,list_lasso_mae_error)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Coefficients')\n",
    "plt.ylabel('Erreur MSA')\n",
    "plt.title('Erreur fonction des coefficients de régularisation',fontsize=14,color='b')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raws = list_alphas.shape[0]\n",
    "list_lasso_r2_error = [1-error for error in list_lasso_r2_error]\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.gca()\n",
    "ax.plot(list_alphas,list_lasso_r2_error)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('Coefficients')\n",
    "plt.ylabel('Erreur 1-R2')\n",
    "plt.title('Erreur fonction des coefficients de régularisation',fontsize=14,color='b')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Prédictions avec l'estimateur ElasticNet</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 50\n",
    "\n",
    "param_grid = {'l1_ratio':np.logspace(-3, 1, n_alphas),'alpha':np.logspace(-3, 3, n_alphas)}\n",
    "\n",
    "enet = ElasticNet()\n",
    "\n",
    "enet_gscv = GridSearchCV(enet, param_grid=param_grid, refit=True, scoring='neg_mean_squared_error', cv=5)\n",
    "t0 = time.time()\n",
    "enet_gscv.fit(X_train_std, y_train)\n",
    "print(\"Recherche des hyper-paramètres en %0.3fs\" % (time.time() - t0))\n",
    "print(\"Meilleur(s) hyper-paramètre(s) pour le classifieur ElacticNet: \"+str(enet_gscv.best_params_))\n",
    "y_predict = enet_gscv.predict(X_test_std)\n",
    "\n",
    "#print(\"LASSO : Moyenne R2 = %0.6f\" %np.mean(list_r2_error))\n",
    "#print(\"LASSO : Moyenne MSE = %0.6f\" %np.mean(list_mse_error))\n",
    "#print(\"LASSO : Moyenne MSA = %0.6f\" %np.mean(list_msa_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_regr.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 50\n",
    "alphas = np.logspace(-2, 0, n_alphas)\n",
    "param_grid = {'l1_ratio':np.logspace(-2, 2, n_alphas)}\n",
    "\n",
    "enetcv = ElasticNetCV(l1_ratio=alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enetcv.fit(X_train_std.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enetcv.score(X_test_std,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = enetcv.path(X_test_std.toarray(),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(path[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(path[0], path[2])\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('ElasticNet coefficients as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = []\n",
    "errors = []\n",
    "enet = ElasticNet()\n",
    "\n",
    "for a in alphas:\n",
    "    enet.set_params(l1_ratio=a)\n",
    "    enet.fit(X_train_std, y_train)\n",
    "    coefs.append(enet.coef_)\n",
    "    #errors.append([ np.mean((enet.predict(X_test_std) - y_test) ** 2)])\n",
    "    errors.append([ np.mean(r2_score(y_test, enet.predict(X_test_std)))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas,coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('L1')\n",
    "plt.ylabel('Poids')\n",
    "plt.title('ElasticNet: poids fonction de la regularisation')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas, errors)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('L1')\n",
    "plt.ylabel('Erreurs R2')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blues'>Classification binaire KNN </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Creation de la classe binaire : avec retard (-1) ou sans retard (1)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delay_digit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = np.where(arr_delay>0,1,-1)\n",
    "print(np.where(y_labels>0)[0].shape[0], np.where(y_labels<0)[0].shape[0])\n",
    "print(np.where(y_labels>0)[0].shape[0] + np.where(y_labels<0)[0].shape[0],y_labels.shape[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Création des jeux de données d'entraînement et standardisation des données__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# La colonne separator_column est utilisée pour séparer le jeu de données de test\n",
    "# du jeu de données d'entraînement.\n",
    "# Le critere de séparation sont les valeurs de part et d'autre de la valeur \n",
    "# train_limit\n",
    "# -----------------------------------------------------------------------------\n",
    "separator_column = 'FL_DATE'\n",
    "train_limit = 20\n",
    "\n",
    "df_delay_digit, X_train_std, X_test_std, y_train, y_test =  p4_train_test_split_from_column(df_delay_digit\\\n",
    "                                                                    , separator_column\\\n",
    "                                                                    , train_limit\\\n",
    "                                                                    , y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Verification sommaire__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(y_test>0)[0].shape[0]+np.where(y_test<0)[0].shape[0], y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Recherche du nb de voisins optimum du KNN par validation croisée</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_neighbors, list_neighbors, list_score = knn_cv_search(X_train_std.toarray(), y_train\\\n",
    "                                                           , scoring_parameter='r2'\\\n",
    "                                                          ,limit_list=(13,22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot misclassification error vs k\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(list_neighbors, list_score)\n",
    "plt.xlabel('Nombre de voisins K')\n",
    "plt.ylabel('Erreur R2 de prédiction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Prédictions avec un classifieur KNN optimal</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "knr_clf = KNeighborsRegressor(n_neighbors=best_neighbors)\n",
    "t0 = time.time()\n",
    "knr_clf.fit(X_train_std, y_train)\n",
    "t1 = time.time()\n",
    "print(\"Apprentissage KNN réalisé en %0.3Fs\" % (t1-t0))\n",
    "t0=t1\n",
    "y_pred_knn = knr_clf.predict(X_test_std)\n",
    "t1 = time.time()\n",
    "print(\"Meilleur score pour la classification KNN : %0.4F\" % accuracy_score(y_test, y_pred_knn))\n",
    "print(\"Prédictions KNN réalisées en %0.3Fs\" % (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Affichage des scores du classificateur binaire__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.shape)\n",
    "class_names=['RETARD','SANS RETARD']\n",
    "print(classification_report(y_test, y_pred_knn,target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Affichage graphique de la matrice de confusions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(y_test, y_pred_knn )\n",
    "#print(conf)\n",
    "plot_confusion_matrix(conf, class_names,\n",
    "                          normalize=True,\n",
    "                          title='Matrice de confusion',\n",
    "                          cmap=plt.cm.Reds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blues'>Clustering DBSCAN </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_min_samples = 5\n",
    "\n",
    "    \n",
    "#for eps_param in np.logspace(-1,0,10) :\n",
    "for dbscan_min_samples in range(5,25,1) :\n",
    "    for eps_param in np.linspace(0.1,1.5,15) :\n",
    "\n",
    "        dbscan = DBSCAN(eps=eps_param, min_samples=dbscan_min_samples).fit(X_std)\n",
    "        clustering_dbscan = dbscan.fit(X_std)\n",
    "        clustering_name = clustering_dbscan.labels_\n",
    "        dbscan_nclusters = len(set(clustering_name)) - (1 if -1 in clustering_name else 0)\n",
    "\n",
    "        if dbscan_nclusters <= 1 :\n",
    "            #print(\"\\nERROR : DBSCAN cluster bruité!\\n\")\n",
    "            pass\n",
    "        else :\n",
    "            print(\"DBSCAN : estimation du nombre de clusters min_sample= {} eps= {}: {} \"\\\n",
    "                  .format(dbscan_min_samples, eps_param, dbscan_nclusters))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sample= 5 \n",
    "eps= 1.4\n",
    "dbscan = DBSCAN(eps=eps_param, min_samples=dbscan_min_samples).fit(X_std)\n",
    "clustering_dbscan = dbscan.fit(X_std)\n",
    "clustering_name = clustering_dbscan.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
