# <center><h1>TAG engine proposal for StackOverFlow platform</h1></center>
This project carried out on behalf of the Master 2 Centrale Sup√©lec / Openclassrooms training in datasience
<hr>

<br><br><br>
<h3>Abstract</h3>
<hr>

Stackoverflow allows its users to post problems that they encounter in the implementation of solutions in the field of information technologies. Helpers answer these questions. Questions and Answers form a knowledge base.

<br>

* Project artefacts are located under <a href="URL">http://bit.ly/FBangui_Datascience_NLP</a>
<br>

* The slides present the overall approach of the study : <a href="URL">
        OC_Datascientist/P6/Soutenance_P6_v3/report/Openclassrooms_ParcoursDatascientist_P6-V1.pdf
      </a>
<br>

* Jupyter notebook : <a href="URL">http://bit.ly/NLP_JupyterNotebook</a>
<br>

* Python source code : <a href="URL">http://bit.ly/NLP_Python</a>
<br>

* Project report with detailed exploratory analysis : <a href="URL">http://bit.ly/FBangui_Datascience_slides_TAG</a>
<br>

Description of task to be accomplished along with asumptions are schematized below : 

<center><img src="./P6_Mission.png" alt="Drawing" style="width: 200px;"/></center>

<br>

The problem posed is formulated as that of classifying questions according to the tags which are attributed to them. This classification is then used to suggest the most relevant tags to new questions. 
<br>

Because a question can be assigned with several tags, the classification is approached here from the angle of a **multi-label classification**.
<br>

<h3>Study Overview :</h3>
<hr>

 
* Exploratory analysis leads to build a standardized data model. Scheme below shows how the exploratory analysis has been conducted. It was focused both on Tags and Tokens.

<center><img src="./P6_TAGExploratoryAnalysis.png" alt="Drawing" style="width: 200px;"/></center>

<br>
Tags and tokens show same curve for occurencies.
<center><img src="./P6_TAGandTokenDistribution.png" alt="Drawing" style="width: 200px;"/></center>

<br>
While Kolmogorov/Smirnov statistical inference test over assigned tags lead us to make decision over Gaussian asumption distribution.
<center><img src="./P6_StatInference.png" alt="Drawing" style="width: 200px;"/></center>
<br>

* Standardization process
<br>
<center><img src="./P6_TokenizationProcess.png" alt="Drawing" style="width: 200px;"/></center>

The left side of the scheme above shows applied functions to lead to corpus standardization.
<br>

The right side of the scheme above provides an exemple of a tokenized post issued from standardization process.
<br>

<u>It is shown that while using NLP algorithms, we've to mainly deal with tokens that are derived from naturel language but do not belongs to naturel language.</u>

<br>
Then, using lemmatization and stems may lead to a bias while training Machine Learning algorithms.

<br>

* Corpus digitalization

All along this study, multiple algorithms have been used for standardized corpus digitalization : counting tokens, TFIDF, co-occurencies.

<br>
Results lead to a set of features along which, each one of the POST from corpus will be assigned with a set of weights.


<br>
<center><img src="./P6_CorpusDigitalization.png" alt="Drawing" style="width: 200px;"/></center>
<br>



* Benchmarking scheme : multiple Machine Learning algorithms have been benchmarked in order to select most apropriate one for deployment.
<br>

In addition to M.L. algorithms, statistical algorithms have been added in order to provide a baseline of  results. Scheme below shows the global picture of such benchmark.
<br>
<center><img src="./P6_BenchmarkScope.png" alt="Drawing" style="width: 200px;"/></center>
<br>

**Word2Vec** algorihtm has been regarded here as an expert system. Some tests have shown that **TWord2Vec** was able to predict "better" tags for a given post then the one suggested by users.
<br>

* Statisticals methods
<br>
These methods provide a baseline of results. A measure of accuracy has been defined as showned on scheme below. Multiple digitalization algorithms has been applied such as TF-IDF, token occurencies, N-Grams occurency.


<br>
<center><img src="./P6_StatisticalMethod.png" alt="Drawing" style="width: 200px;"/></center>

<br>
    Results shows accuracy versus digitalization algorithms :
<br>
<center><img src="./P6_StatisticalMethodBenchmark.png" alt="Drawing" style="width: 200px;"/></center>
Best result is reached for  TF-IDF(1,1) with an average accuracy of around 10%.
<br>

* Unsupervized methods : LDA
<br>
<center><img src="./P6_LDA.png" alt="Drawing" style="width: 200px;"/></center>
<br>Latent Dirichlet Allocation process leads to build 2 matrix : topics distribution over tokens (beta matrix) and POST distribution ovre topics. The former matrix is used for feeding matching algorithm for providing suggested TAGs. The resulted mapped beta matrix shows how same TAGs may belong to different topics. An interpretation may be delivered as follow : combining TAGs lead to cover differents topics. Then (*Linux, C++)* combination may reference developpement of C++ application over Linux platform while *(IOCTL, Linux)* combination probably references developpment of drivers inside Linux kernel.
<br>
<center><img src="./P6_LDAResult.png" alt="Drawing" style="width: 200px;"/></center>
<br>Searching over LDA hyper-parameter that is the number of topics, best accuracy has been reached for 400 topics for an accuracy value of 30%.

<br>

* Unsupervized methods : Kmeans clustering
<br> This sheme detailed below is based on vectorized (digitalized) POST clustering. Similar POSTS may belongs the same cluster. TAGs extraction from any cluster is detailed next.
<center><img src="./P6_Kmeans.png" alt="Drawing" style="width: 200px;"/></center>

<br>Tags extraction is based of distance from a POST to Kmeans cluster centroid. All elements in the cluster are ordered based on distance from cluster centroid. A neighborhood is defined for the POST. Highests TF-IDF value for each neighbour lead to select related Token. Those tokens are filtered from TAGs database for suggested TAGs.


<center><img src="./P6_KmeansTAGExtraction.png" alt="Drawing" style="width: 200px;"/></center>

<br> Searching over Kmeans hyper-parameter that is the number of clusters, best result is shown for 400 clusters for an accuracy value of 16%. Best accuracy value is also reached for 400 clusters, to be compared to LDA results with 400 topics (see above). A possible interpretation is that Kmeans centroid represents the main topic and sub-topics are reached as you moove away from centroid.

<center><img src="./P6_KmeansResult.png" alt="Drawing" style="width: 200px;"/></center>

<br>It is shownon the shame below that accuracy decrease when POST distance from centroid increase.
<center><img src="./P6_KmeansResult2.png" alt="Drawing" style="width: 200px;"/></center>


<h3>API on JSON format:</h3>
<hr>

<img src="./P5_MarketSegmentation_API.png" alt="Drawing" style="width: 200px;"/>




<br>
<h3>Software artefacts</h3>
<hr>
<br>
