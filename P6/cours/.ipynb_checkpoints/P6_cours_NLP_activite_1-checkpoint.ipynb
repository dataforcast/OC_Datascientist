{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NOTEBOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"./figures/LogoOpenclassrooms.png\">\n",
    "<font size=\"4\">\n",
    "<p>\n",
    "Cette activité est réalisée dans le cadre du cours ``Analysez vos données textuelles`` diffusé en MOOC par\n",
    "</p> **<font color='blus'>Openclassrooms</font>**.<p>\n",
    "</p>    \n",
    "<p></p>\n",
    "<p></p>\n",
    "Le but de cette activité est de nettoyer le texte et de créer un jeu de données d’entraînement en vue de créer un moteur de résumé automatique. Voici les données que vous utiliserez  : \n",
    "    \n",
    "    https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ\n",
    "    \n",
    "## Contexte\n",
    "\n",
    "Les données brutes représentent un corpus d’articles CNN récupérés par des chercheurs pour leurs expérimentations. L’objectif est de récupérer les features des documents et les highlights (résumés courts) associés concaténés, en vue d’entraîner un potentiel modèle de création de résumé d’articles.\n",
    "\n",
    "* Article Argentina coach Alejandro Sabella believes Lionel Messi’s habit of throwing up during games is because of nerves. The Barcelona star has vomited on the pitch during several games over the last few seasons and appeared to once again during Argentina’s last warm-up match against Slovenia on Saturday….\n",
    "\n",
    "\n",
    "* Highlight Argentina coach Sabella believes Messi’s habit of being sick during games is down to nerves.\n",
    "\n",
    "## Consigne\n",
    "Vous devrez effectuer les opérations de traitement suivantes sur le texte, pas forcément dans cet ordre\n",
    "\n",
    "* Créer des paires de document (article, highlights)\n",
    "* Suppression de la ponctuation\n",
    "* Séparation en token en minuscules\n",
    "* Suppression des stopwords pour les articles\n",
    "* Calcul des fréquences et tf-idf sur les deux types de documents\n",
    "* Enregistrement du nouveau jeu de données d’entraînement pour usage ultérieur\n",
    "\n",
    "## Livrables\n",
    "Le code de création du jeu de données d’entraînement.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity \n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# This constant allows to limit the number of CNN files to be read.\n",
    "#\n",
    "# When value is fixed to None, then all files from CNN corpus will be read.\n",
    "# This option require 16GB RAM.\n",
    "#\n",
    "# For reading CNN files without any restrictions : max_read_count=None\n",
    "#-----------------------------------------------------------------------------\n",
    "max_read_count=100\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# This constant is the path in order to write NLP processed files \n",
    "# (train dataset)\n",
    "#-----------------------------------------------------------------------------\n",
    "file_path =\"./data/cnn_new\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pair of files, frequencies are recorded into dictionary ``dict_article_highlight`` as component of a tuple.\n",
    "\n",
    "Format of ``dict_article_highlight`` is as following : ``{#ID:(tokenized_article,tokenized_highlight,dict_article_freq,dict_highlight_freq)}``\n",
    "\n",
    "* ``tokenized_article`` : list of tokens issued from article\n",
    "* ``tokenized_highlight`` : list of tokens issued from highlight\n",
    "* ``dict_article_freq`` is formated as : ``{token:token_frequency}``\n",
    "* ``dict_highlight_freq`` is formated as : ``{token:token_frequency}``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/cnn/stories'\n",
    "dict_article, dict_highlight = p6_util_activity.read_cnn_corpora(data_path, p_restriction=None, read_count=max_read_count)\n",
    "\n",
    "print(len(dict_article), len(dict_highlight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequencies computing over raw articles and highlights\n",
    "\n",
    "* Frequencies are computed for both type of content (raw articles and raw highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_content = list()\n",
    "for root_name in dict_article.keys():\n",
    "    content= dict_article[root_name]\n",
    "    list_content += content\n",
    "len(list_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dictionnaire des frequences de mots pour les articles: 1\n",
      "Taille du dictionnaire des frequences de mots pour les hightlights: 1\n"
     ]
    }
   ],
   "source": [
    "import p6_util_activity\n",
    "\n",
    "dict_article_freq = p6_util_activity.compute_frequency(dict_article)\n",
    "print(\"Taille du dictionnaire des frequences de mots pour les articles: \"+str(len(dict_article_freq.keys())))\n",
    "\n",
    "dict_highlight_freq = p6_util_activity.compute_frequency(dict_highlight)\n",
    "print(\"Taille du dictionnaire des frequences de mots pour les hightlights: \"+str(len(dict_highlight_freq.keys())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot of word frequencies from raw files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot of frequencies for hilights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity\n",
    "\n",
    "p6_util_activity.dict_plot_frequency(dict_highlight_freq,\"Raw highlights\",query=\"Freq>10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot of frequencies for articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity\n",
    "\n",
    "p6_util_activity.dict_plot_frequency(dict_article_freq,\"Raw articles\",query=\"Freq>200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF computing for raw articles and highlights\n",
    "\n",
    "* TF-IDF are computed for both type of content (raw articles and raw highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity\n",
    "\n",
    "tfidf_vectorizer = p6_util_activity.get_tfidf_vectorizer(dict_article, doc_type='string')\n",
    "dict_tfidf_art = p6_util_activity.get_dict_tfidf(tfidf_vectorizer)\n",
    "print(\"Taille du dictionnaire des valeurs TFIDF des mots pour les articles bruts: \"+str(len(dict_tfidf_art.keys())))\n",
    "\n",
    "tfidf_vectorizer= p6_util_activity.get_tfidf_vectorizer(dict_highlight, doc_type='string')\n",
    "dict_tfidf_hig = p6_util_activity.get_dict_tfidf(tfidf_vectorizer)\n",
    "print(\"Taille du dictionnaire des valeurs TFIDF des mots pour les highlights bruts: \"+str(len(dict_tfidf_hig.keys())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP process for articles : **\n",
    "* removing stopwords for english language\n",
    "* removing punctuation\n",
    "* tokenizing each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import p6_util_activity\n",
    "\n",
    "for root_name in dict_article.keys():\n",
    "    article = p6_util_activity.cb_remove_punctuation(dict_article[root_name])\n",
    "    article = p6_util_activity.cb_remove_stopwords(article)\n",
    "    dict_article[root_name] = nltk.word_tokenize(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP process for highlights : **\n",
    "* removing punctuation\n",
    "* tokenizing each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import p6_util_activity\n",
    "\n",
    "for root_name in dict_highlight.keys():\n",
    "    highlight = p6_util_activity.cb_remove_punctuation(dict_highlight[root_name])\n",
    "    dict_highlight[root_name] = nltk.word_tokenize(highlight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Writing pair of cleaned files : articles and highllights**\n",
    "\n",
    "Resulting files are created into directory ``cnn_new``.\n",
    "\n",
    "They are named with same root-name then original files.\n",
    "\n",
    "Extension ``.art`` are tokenized files containing articles.\n",
    "\n",
    "Extension ``.hig`` are tokenized files containing highlights concatened contents.\n",
    "\n",
    "Files names with same root name are those for which highlights and articles do match.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity\n",
    "\n",
    "cnn_new_dir = file_path\n",
    "p6_util_activity.build_directory(cnn_new_dir)\n",
    "\n",
    "p6_util_activity.write_train_set(cnn_new_dir, dict_article, dict_highlight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read cleaned files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity\n",
    "\n",
    "dict_article, dict_highlight = p6_util_activity.read_train_set(file_path,read_count=max_read_count)\n",
    "print(len(dict_article), len(dict_highlight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequencies computing over articles and highlights issued from cleaned files\n",
    "\n",
    "* Cleaned files for articles and hilights are read.\n",
    "* Frequencies are computed for both type of content (articles and highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity\n",
    "\n",
    "dict_freq_art = p6_util_activity.compute_frequency(dict_article)\n",
    "print(\"Taille du dictionnaire des frequences des mots pour les articles traités: \"+str(len(dict_tfidf_art.keys())))\n",
    "\n",
    "dict_freq_hig = p6_util_activity.compute_frequency(dict_highlight)\n",
    "print(\"Taille du dictionnaire des fréquences des mots pour les highlights traités: \"+str(len(dict_tfidf_hig.keys())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot of word frequencies from cleaned files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot of frequencies for clean hilights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity\n",
    "\n",
    "p6_util_activity.dict_plot_frequency(dict_freq_hig,\"Cleaned highlights\",query=\"Freq>10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot of frequencies for claned articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity\n",
    "\n",
    "p6_util_activity.dict_plot_frequency(dict_freq_art,\"Cleaned articles\",query=\"Freq>50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF computing over articles and highlights\n",
    "\n",
    "* Files from cleaned articles and hilights are read.\n",
    "* TF-IDF weights are computed for both type of content (articles and highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity\n",
    "print(file_path)\n",
    "dict_article, dict_highlight = p6_util_activity.read_train_set(file_path)\n",
    "print(len(dict_article), len(dict_highlight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity\n",
    "\n",
    "vectorizer = p6_util_activity.get_tfidf_vectorizer(dict_article, doc_type='string')\n",
    "dict_tfidf_art = p6_util_activity.get_dict_tfidf(vectorizer)\n",
    "print(\"Taille du dictionnaire des valeurs TFIDF des mots pour les articles traités: \"+str(len(dict_tfidf_art.keys())))\n",
    "\n",
    "vectorizer = p6_util_activity.get_tfidf_vectorizer(dict_highlight, doc_type='string')\n",
    "dict_tfidf_hig = p6_util_activity.get_dict_tfidf(vectorizer)\n",
    "print(\"Taille du dictionnaire des valeurs TFIDF des mots pour les highlights traités: \"+str(len(dict_tfidf_hig.keys())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Test highlights predictions</font>\n",
    "\n",
    "* Target to be predicted are hightlights while input data are articles.\n",
    "* 2 predictors models are tested : ``MultinomialNB`` and ``RandomForests``\n",
    "* No models evaluation have been conducted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read cleaned files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity\n",
    "dict_article, dict_highlight = p6_util_activity.read_train_set(file_path,read_count=max_read_count)\n",
    "print(len(dict_article), len(dict_highlight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data is splitted into train and test**\n",
    "\n",
    "* 70% dataset for training model\n",
    "* 30% dataset for testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity\n",
    "dict_X_train, dict_X_test, dict_y_train, dict_y_test = p6_util_activity.split_train_test(dict_article, dict_highlight, train_ratio = 0.7)\n",
    "print(len(dict_X_train), len(dict_X_test), len(dict_y_train), len(dict_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute TFIDF matrix from ``X`` using ``TfidfTransformer`` with option ``use_idf=True`` (default option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer=TfidfVectorizer(norm=\"l2\", use_idf=True)\n",
    "X_train = vectorizer.fit_transform(dict_X_train.values())\n",
    "y_train = list(dict_y_train.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use ``MultinomialNB`` as predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(dict_X_test.values())\n",
    "y_test=list(dict_y_test.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "print(\"\\n---------- MultinomialNB : Predicted highlight for article # {} -----------------------\\n\".format(str(n)))\n",
    "print(y_pred[n])\n",
    "print(\"\\n---------- MultinomialNB : Tested highlight for article # {} --------------------------\\n\".format(str(n)))\n",
    "print(y_test[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use ``RandomForest`` as predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "nb_estimators = 20\n",
    "rfc = RandomForestClassifier(n_estimators=nb_estimators)\n",
    "rfc_model = rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "print(\"\\n---------- RandomForest : Predicted highlight for article # {} -----------------------\\n\".format(str(n)))\n",
    "print(y_pred[n])\n",
    "print(\"\\n---------- RandomForest: Tested highlight for article # {} --------------------------\\n\".format(str(n)))\n",
    "print(y_test[n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
