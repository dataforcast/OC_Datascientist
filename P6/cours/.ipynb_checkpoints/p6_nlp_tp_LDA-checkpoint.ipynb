{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset is loaded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "list_document = dataset.data\n",
    "\n",
    "len(list_document), type(list_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA model is created**\n",
    "\n",
    "Aims is to discover topics from inside document corpus.\n",
    "\n",
    "For doing so :\n",
    " * We compute text frequency to feed LDA model\n",
    " * We fixe the number of expected topics with hyper-parameter ``no_topics``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#tokenizer = nltk.RegexpTokenizer(r'[ a-zA-Z0-9]')\n",
    "#tokenized_list = tokenizer.tokenize(item.lower())\n",
    "#--------------------------------------------------------------------\n",
    "# Get text frequency (TF)\n",
    "#--------------------------------------------------------------------\n",
    "tf_vectorizer=CountVectorizer(max_features=1000, stop_words='english',ngram_range=(1,2))\n",
    "tf_csr_matrix = tf_vectorizer.fit_transform(list_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "no_topics = 15\n",
    "\n",
    "# Run LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online'\\\n",
    "                                      , learning_offset=50.,random_state=0).fit(tf_csr_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Displays topics recovered from LDA model considering top 10 more frequents words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, nb_top_words):\n",
    "    #---------------------------------------------------------------\n",
    "    # model.components_ : matrix dimension= (N_topics x K_features)\n",
    "    #---------------------------------------------------------------\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        #-----------------------------------------------------------\n",
    "        # topic.argsort() returns indices that are sorting array\n",
    "        #-----------------------------------------------------------\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-nb_top_words - 1:-1]]))\n",
    "        print()\n",
    "\n",
    "nb_top_words = 10\n",
    "display_topics(lda_model, tf_vectorizer.get_feature_names(), nb_top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply LDA again over topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------\n",
    "# Get features names from which topics are composed from\n",
    "#----------------------------------------------------------------\n",
    "list_feature_name = tf_vectorizer.get_feature_names()\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# This list will contain topics regarded as documents\n",
    "#----------------------------------------------------------------\n",
    "list_topic_document=list()\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# For each topics, only most important features names are captured.\n",
    "# This will lead of a new kind of documents: documents of topics.\n",
    "#----------------------------------------------------------------\n",
    "most_important_values=100\n",
    "for topic in lda_model.components_:\n",
    "    #--------------------------------------------------------------------------\n",
    "    # topic.argsort() returns an index array of array values sorted ascendend.\n",
    "    #--------------------------------------------------------------------------\n",
    "    index_array=topic.argsort()\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Each topic is converted as a document\n",
    "    # 'most_important_values' are most important values exracted from features names: \n",
    "    # they are found from the last values of sorted array.\n",
    "    #--------------------------------------------------------------------------\n",
    "    topic_document =' '.join([list_feature_name[i] for i in index_array[:-most_important_values - 1:-1]])\n",
    "    list_topic_document.append(topic_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------\n",
    "# Get text frequency (TF)\n",
    "#--------------------------------------------------------------------\n",
    "tf_vectorizer=CountVectorizer(max_features=10, stop_words='english',ngram_range=(1,1))\n",
    "tf_csr_matrix = tf_vectorizer.fit_transform(list_topic_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics_topics = 3\n",
    "\n",
    "# Run LDA\n",
    "lda_model_topics = LatentDirichletAllocation(n_components=no_topics_topics, max_iter=5, learning_method='online'\\\n",
    "                                      , learning_offset=50.,random_state=0).fit(tf_csr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_top_words = 10\n",
    "display_topics(lda_model_topics, tf_vectorizer.get_feature_names(), nb_top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "dict_corpus={1:\"Je suis à la maison\", 2:\"La maison est dans la prairie\", 3:\"Je suis à la plage\"}\n",
    "list_corpus = list(dict_corpus.values())\n",
    "print(\"\\n\".join(list_corpus))\n",
    "cnt_vectorizer = CountVectorizer()\n",
    "print(cnt_vectorizer.fit_transform(list_corpus).todense())\n",
    "print(cnt_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary =['je','suis','à','la','maison','est','dans','prairie','plage']\n",
    "            [  1,    1,    1,  1,     1,     0,     0,      0,       0]\n",
    "            [  0,    0,    0,  1,     1,     1,     1,      1,       0]\n",
    "            [  1,    1,    1,  1,     0,     0,     0,      0,       1]\n",
    "[0,0,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_activity\n",
    "\n",
    "\n",
    "tfidf_vectorizer= p6_util_activity.get_tfidf_vectorizer(dict_corpus, doc_type='string')\n",
    "dict_tfidf = p6_util_activity.get_dict_tfidf(tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tfidf\n",
    "list_token = list()\n",
    "for doc in dict_corpus.values():\n",
    "    list_token += doc.split()\n",
    "list_token\n",
    "tfidf_vectorizer.transform(dict_corpus.values()).todense()\n",
    "tfidf_vectorizer.vocabulary_\n",
    "tfidf_vectorizer.idf_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.transform(dict_corpus.values()).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_vectorizer=TfidfVectorizer(norm=\"l2\", use_idf=False)\n",
    "\n",
    "\n",
    "tf_vectorizer = tf_vectorizer.fit(dict_corpus.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_matrix = tf_vectorizer.transform(dict_corpus.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "list_text = \"La seconde partie de du cours de traitement de texte traite de la transformation des données textuelles\".split()\n",
    "#list_text = [\"ABCD\",\"123\",\"GHT\",\"KKK\"]\n",
    "list_ngram = list()\n",
    "for ngram in ngrams(list_text,3):\n",
    "    joined_ngram= ' '.join(str(i) for i in ngram)\n",
    "    list_ngram.append(joined_ngram)\n",
    "\n",
    "len(list_ngram)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opérez une première classification naïve de sentiments\n",
    "\n",
    "* Imdb dataset has been downloaded from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# \n",
    "#---------------------------------------------------------------------\n",
    "def feature_extract(content):\n",
    "    \"\"\"Returns a dictionary in which : \n",
    "    * keys are tokens from content given as parameter.\n",
    "    * values are always True.\n",
    "    Content is tokenized using NLTK.\n",
    "    Each content from file is encoded as a dictionary: \n",
    "    {token1:True, token2:True,...,tokenK:True}\n",
    "    \"\"\"\n",
    "    return ({ word: True for word in nltk.word_tokenize(content) })\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# \n",
    "#---------------------------------------------------------------------\n",
    "def load_data(relative_path):\n",
    "    \"\"\"Returns a list of encoded features.\n",
    "    Each element of list is an encoded content for each file.\n",
    "    An encoded content from a file has the folowwing format :\n",
    "    [{token1:True,...,tokenN:True}, tag_value]\n",
    "    tag_value may be 'pos' or 'neg'.\n",
    "    \n",
    "    \"\"\"\n",
    "    list_data = list()\n",
    "    tag='pos'\n",
    "    print(\"Reading \"+tag+\" tag files....\")\n",
    "    file_count=0\n",
    "    for file_name in os.listdir(relative_path+'/'+tag):\n",
    "        file_name = relative_path+'/'+tag+'/'+file_name\n",
    "        with open(file_name) as fp:\n",
    "            for content in fp:\n",
    "                dict_feature = feature_extract(content)\n",
    "                list_data.append([dict_feature,tag])\n",
    "        file_count += 1\n",
    "    print(\"Number of \"+tag+\" files read: \"+str(file_count))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    tag='neg'\n",
    "    print(\"Reading \"+tag+\" tag files....\")\n",
    "    file_count=0\n",
    "    for file_name in os.listdir(relative_path+'/'+tag):\n",
    "        file_name = relative_path+'/'+tag+'/'+file_name\n",
    "        with open(file_name) as fp:\n",
    "            for content in fp:\n",
    "                dict_feature = feature_extract(content)\n",
    "                list_data.append([dict_feature,tag])\n",
    "        file_count += 1\n",
    "    print(\"Number of \"+tag+\" files read: \"+str(file_count))\n",
    "\n",
    "    return list_data\n",
    "#---------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pos tag files....\n",
      "Number of pos files read: 12500\n",
      "\n",
      "Reading neg tag files....\n",
      "Number of neg files read: 12500\n"
     ]
    }
   ],
   "source": [
    "relative_path = './data/aclImdb/train'\n",
    "list_train = load_data(relative_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list_train is compounded from lists as follwing : \n",
    "\n",
    "list_train=[list_1, list_2,...,list_N]\n",
    "\n",
    "and \n",
    "\n",
    "list_1=[dict_encoded_value,category_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos',\n",
       " {'!': True,\n",
       "  '&': True,\n",
       "  \"'m\": True,\n",
       "  \"'s\": True,\n",
       "  '(': True,\n",
       "  ')': True,\n",
       "  ',': True,\n",
       "  '.': True,\n",
       "  '...': True,\n",
       "  '4': True,\n",
       "  'A': True,\n",
       "  'Appolonia': True,\n",
       "  'B': True,\n",
       "  'Baby': True,\n",
       "  'Beautiful': True,\n",
       "  'Blue': True,\n",
       "  'Computer': True,\n",
       "  'Crazy': True,\n",
       "  'Cry': True,\n",
       "  'Darling': True,\n",
       "  'Die': True,\n",
       "  'Doves': True,\n",
       "  'Go': True,\n",
       "  'I': True,\n",
       "  'It': True,\n",
       "  'Let': True,\n",
       "  'Me': True,\n",
       "  'Nikki': True,\n",
       "  'Ones': True,\n",
       "  'Purple': True,\n",
       "  'R': True,\n",
       "  'Rain': True,\n",
       "  'Star': True,\n",
       "  'Take': True,\n",
       "  'The': True,\n",
       "  'U': True,\n",
       "  'When': True,\n",
       "  'With': True,\n",
       "  'Would': True,\n",
       "  'a': True,\n",
       "  'album': True,\n",
       "  'all': True,\n",
       "  'and': True,\n",
       "  'angry': True,\n",
       "  'anthem': True,\n",
       "  'appropriate': True,\n",
       "  'are': True,\n",
       "  'as': True,\n",
       "  'ballad': True,\n",
       "  'beginning': True,\n",
       "  'best': True,\n",
       "  'both': True,\n",
       "  'cheerful': True,\n",
       "  'classic': True,\n",
       "  'climax': True,\n",
       "  'closest': True,\n",
       "  'course': True,\n",
       "  'ending': True,\n",
       "  'ever': True,\n",
       "  'for': True,\n",
       "  'fun': True,\n",
       "  'funniest': True,\n",
       "  'good': True,\n",
       "  'great': True,\n",
       "  'has': True,\n",
       "  'highly': True,\n",
       "  'is': True,\n",
       "  'it': True,\n",
       "  'makes': True,\n",
       "  'masterpiece': True,\n",
       "  'moments': True,\n",
       "  'movie': True,\n",
       "  'music': True,\n",
       "  'of': True,\n",
       "  'okay': True,\n",
       "  'on': True,\n",
       "  'one': True,\n",
       "  'out': True,\n",
       "  'party': True,\n",
       "  'perfect': True,\n",
       "  'pop': True,\n",
       "  'probably': True,\n",
       "  'recommend': True,\n",
       "  'scenes': True,\n",
       "  'somewhat': True,\n",
       "  'song': True,\n",
       "  'songs': True,\n",
       "  'soundtrack': True,\n",
       "  'starts': True,\n",
       "  'the': True,\n",
       "  'them': True,\n",
       "  'thing': True,\n",
       "  'this': True,\n",
       "  'to': True,\n",
       "  'towards': True,\n",
       "  'true': True,\n",
       "  'up-tempo': True,\n",
       "  'vaguely': True,\n",
       "  'very': True,\n",
       "  'whole': True,\n",
       "  'with': True})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_1=list_train[0]\n",
    "dict_encoded_value_1 = list_1[0]\n",
    "category_value=list_1[1]\n",
    "category_value, dict_encoded_value_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distributions\n",
    "*        +----------+-----------+\n",
    "*        | pos      |  neg      |\n",
    "* -------+----------+-----------+-------------+\n",
    "  token1 |    N11   |    N12    | P(C |token1)| Probability token1 belongs to category C\n",
    "  -------+----------+-----------+-------------+   \n",
    "  token2 |    N21   |    N22    | P(C |token2)| Probability token2 belongs to category C\n",
    "  -------+----------+-----------+-------------+\n",
    "  token3 |    N31   |    N32    | P(C |token3)| Probability token3 belongs to category C\n",
    "  -------+----------+-----------+-------------+\n",
    "            . . .   \n",
    "  -------+----------+-----------+-------------+\n",
    "  tokenK |    NK1   |    NK2    | P(C |tokenK)| Probability tokenK belongs to category C\n",
    "  -------+----------+-----------+-------------+\n",
    "         | P(X|pos) |  P(X|neg) |\n",
    "         +----------+-----------+\n",
    "\"\"\"         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(list_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display weights assigned to any feature, mean, P(feature|C), this, for any category C.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   Avoid = True              neg : pos    =     93.4 : 1.0\n",
      "                    2/10 = True              neg : pos    =     75.7 : 1.0\n",
      "                    4/10 = True              neg : pos    =     64.2 : 1.0\n",
      "                    *1/2 = True              neg : pos    =     57.0 : 1.0\n",
      "                    3/10 = True              neg : pos    =     43.6 : 1.0\n",
      "                    Boll = True              neg : pos    =     37.7 : 1.0\n",
      "                     Uwe = True              neg : pos    =     36.3 : 1.0\n",
      "                    7/10 = True              pos : neg    =     33.2 : 1.0\n",
      "                   WORST = True              neg : pos    =     27.8 : 1.0\n",
      "                    8/10 = True              pos : neg    =     27.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pos tag files....\n",
      "Number of pos files read: 12500\n",
      "\n",
      "Reading neg tag files....\n",
      "Number of neg files read: 12500\n"
     ]
    }
   ],
   "source": [
    "relative_path = './data/aclImdb/test'\n",
    "list_test = load_data(relative_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model performance evaluation from accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83076\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, list_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple from site \n",
    "\n",
    "* http://www.nltk.org/book/ch06.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus handles list of names splited into 2 categoies : male and female.\n",
    "\n",
    "* Male names are provided as a list of names.\n",
    "* Female names are provided as a list of names.\n",
    "* ``list_labeled_names`` is a list built while assigning a category for each name: [(name, category),...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "list_labeled_names = (  [(name, 'male')   for name in names.words('male.txt')] \\\n",
    "                 + [(name, 'female') for name in names.words('female.txt')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted features are shuffled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Kerri', 'female'), list)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(list_labeled_names)\n",
    "\n",
    "\n",
    "list_labeled_names[len(list_labeled_names)-1],type(list_labeled_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the relevant features to classify gender? How to encode them?\n",
    "\n",
    "# Let's consider that relevant features for gender name clasification is the last letter in the gender name.\n",
    "# Releveant features extraction lead to format features as following : {'last_letter':value}\n",
    "# where value is the last letter from a name.\n",
    "\n",
    "def gender_features_exract(word):\n",
    "    \"\"\"Returns extracted feature from word.\n",
    "    In this fuction, the last letter gender name is regarded as the feature gender name.\n",
    "    Extracted feature is returned as a dictionary {'last_letter': last_letter_value}\n",
    "    Then this lead to create the column 'last_letter' in the data model to be built.\n",
    "    \"\"\"\n",
    "    return {'last_letter': word[-1]}\n",
    "\n",
    "def gender_features_exract2(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count(a)': 1,\n",
       " 'count(b)': 0,\n",
       " 'count(c)': 0,\n",
       " 'count(d)': 0,\n",
       " 'count(e)': 0,\n",
       " 'count(f)': 1,\n",
       " 'count(g)': 0,\n",
       " 'count(h)': 0,\n",
       " 'count(i)': 1,\n",
       " 'count(j)': 0,\n",
       " 'count(k)': 0,\n",
       " 'count(l)': 0,\n",
       " 'count(m)': 0,\n",
       " 'count(n)': 1,\n",
       " 'count(o)': 1,\n",
       " 'count(p)': 0,\n",
       " 'count(q)': 0,\n",
       " 'count(r)': 1,\n",
       " 'count(s)': 1,\n",
       " 'count(t)': 0,\n",
       " 'count(u)': 0,\n",
       " 'count(v)': 0,\n",
       " 'count(w)': 0,\n",
       " 'count(x)': 0,\n",
       " 'count(y)': 0,\n",
       " 'count(z)': 0,\n",
       " 'first_letter': 'f',\n",
       " 'has(a)': True,\n",
       " 'has(b)': False,\n",
       " 'has(c)': False,\n",
       " 'has(d)': False,\n",
       " 'has(e)': False,\n",
       " 'has(f)': True,\n",
       " 'has(g)': False,\n",
       " 'has(h)': False,\n",
       " 'has(i)': True,\n",
       " 'has(j)': False,\n",
       " 'has(k)': False,\n",
       " 'has(l)': False,\n",
       " 'has(m)': False,\n",
       " 'has(n)': True,\n",
       " 'has(o)': True,\n",
       " 'has(p)': False,\n",
       " 'has(q)': False,\n",
       " 'has(r)': True,\n",
       " 'has(s)': True,\n",
       " 'has(t)': False,\n",
       " 'has(u)': False,\n",
       " 'has(v)': False,\n",
       " 'has(w)': False,\n",
       " 'has(x)': False,\n",
       " 'has(y)': False,\n",
       " 'has(z)': False,\n",
       " 'last_letter': 's'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features_exract2('François')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Building data model will lead to build contingence matrix as following: \n",
    "\n",
    "\n",
    "\n",
    "---------------------------------------\n",
    "              Female      |     Male \n",
    "---------------------------------------\n",
    "last_letter               \n",
    "\n",
    "---------------------------------------\n",
    "       s              N00      NO1\n",
    "---------------------------------------\n",
    "       e              N10      N11\n",
    "                .....  \n",
    "---------------------------------------\n",
    "       t              Nj0      Nj1\n",
    "---------------------------------------\n",
    "       a              Nn0      Nn1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_featuresets = [(gender_features_exract(n), gender) for (n, gender) in list_labeled_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = list_featuresets[500:], list_featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female', 'male']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_featuresets2 = [(gender_features_exract2(n), gender) for (n, gender) in list_labeled_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set2, test_set2 = list_featuresets2[500:], list_featuresets2[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     38.6 : 1.0\n",
      "             last_letter = 'k'              male : female =     30.8 : 1.0\n",
      "             last_letter = 'f'              male : female =     16.0 : 1.0\n",
      "             last_letter = 'p'              male : female =     11.9 : 1.0\n",
      "             last_letter = 'v'              male : female =     10.5 : 1.0\n",
      "             last_letter = 'd'              male : female =     10.3 : 1.0\n",
      "             last_letter = 'o'              male : female =      8.6 : 1.0\n",
      "             last_letter = 'm'              male : female =      8.5 : 1.0\n",
      "             last_letter = 'r'              male : female =      6.3 : 1.0\n",
      "             last_letter = 'g'              male : female =      5.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.labels()\n",
    "classifier.show_most_informative_features(n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
