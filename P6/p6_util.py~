import numpy as np
import pandas as pd
import re

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from nltk.stem import SnowballStemmer

from bs4 import BeautifulSoup

import p5_util


from nltk.stem import WordNetLemmatizer
from nltk.stem import SnowballStemmer

from sklearn.feature_extraction.text import CountVectorizer



import p5_util
import p6_util

LIST_EMBEDDING_MODE=['tfidf','bow','ngram']
#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def p6_df_standardization(df, is_stemming=False, is_lem=True) :
    """ Applies all pre-processing actions over a dataframe df given as 
    parameter.
    Returned dataframe is a cleaned dataframe.
    """

    print("\nCleaning text in-between markers <code></code> markers...")
    df["Body"] = df.Body.apply(cb_remove_marker,args=('code',))

    print("\nCleaning LXML markers...")
    df["Body"] = df.Body.apply(cb_clean_lxml)

    print("\nRemove verbs from sentences...")
    df["Body"] = df.Body.apply(cb_sentence_filter)

    print("\nFiltering alpha-numeric words from sentences...")
    df["Body"] = df.Body.apply(cb_remove_verb_from_sentence)

    print("\nRemoving stopwords...")
    df["Body"] = df.Body.apply(cb_remove_stopwords)

    if is_lem is True:
        print("\nLemmatization ...")
        lemmatizer=WordNetLemmatizer()
        df['Body']=df.Body.apply(p5_util.cb_lemmatizer,args=(lemmatizer,'lower'))

    if is_stemming is True:
        print("\nEnglish stemming ...")
        stemmer=SnowballStemmer('english')
        df['Body']=df.Body.apply(p5_util.cb_stemmer,args=(stemmer,'lower'))
    return df
#-------------------------------------------------------------------------------
#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def get_df_from_FreqDist(freqDist, query=None):
    """Returns a dataframe from a given nltk.probability.FreqDist object class.
    The returned dataframe have a single column named Freq.
    """
    if freqDist is not None:
        df = pd.DataFrame.from_dict(freqDist, orient='index')
        if 'Freq' not in df.columns:
            df.rename(columns={0:'Freq'}, inplace=True)
            df.sort_values(by = 'Freq', inplace=True, ascending=False) 
        else:
            pass
    if query is not None:
        df=df.query(query)
    else :
        pass
    return df
#-------------------------------------------------------------------------------



#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def get_list_tag_original(str_tag) :
    str_tag_original = re.sub("[^a-zA-Z0-9=++# ]", " ", str_tag )
    list_tag_original = [tag for tag in str_tag_original.split(' ') \
    if tag not in ['',]]
    return list_tag_original
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def get_list_tag_stat_tfidf(sentence, vectorizer, tag_ratio=1.0) :
    """ Returns a list of TAGS from sentence given as parameter.
    List of returned TAGs count is a ratio from sentence words count.
    
    This fuction appplies for TFIDF vectorization only. This mean, documents are
    vectors from which components values are TFIDF values of 
    terms from vocabulary.
    
    Those values are stored into csr_matrix given as parameter 
    while terms from vocabulary are stored into vectorizer.
    
    Selected tags belong both to vocabulary and words from given sentence.

    Terms from vocabulary having greater occurence value are selected as TAGs.
    
    """

    list_term_sentence = sentence.split(' ')
    #---------------------------------------------------------------------------
    # Using vectorized sentence in csrmatrix, get indexes from all features 
    # with frequencies >0 
    #---------------------------------------------------------------------------
    csrmatrix= vectorizer.transform([sentence])
    arr_index = np.where(csrmatrix.A>0)[1]

    #---------------------------------------------------------------------------
    # Using array of indexes from features contained in sentence, 
    # vocabulary terms are extracted and regarded as TAGs.
    #---------------------------------------------------------------------------
    list_tag_sentence= [tag for tag, index in vectorizer.vocabulary_.items() \
    if index in arr_index]

    #---------------------------------------------------------------------------
    # Build dictionary {TAG:TFIDF} where TAG are issued from the given row.
    #---------------------------------------------------------------------------
    dict_tfidf = dict()
    for tag_sentence in list_tag_sentence : 
        index = vectorizer.vocabulary_[tag_sentence]
        dict_tfidf[tag_sentence]= vectorizer.idf_[index]

    #---------------------------------------------------------------------------
    # Get tag_count most greater value from dict_index
    #---------------------------------------------------------------------------
    tag_count = int(len(list_term_sentence)*tag_ratio)

    df_row_tfidf = pd.DataFrame.from_dict(dict_tfidf, orient='index')

    if 0 < df_row_tfidf.shape[0] and 0< df_row_tfidf.shape[1]:
        df_row_tfidf = df_row_tfidf.rename(columns={0:'TFIDF'}, inplace=False)
        df_row_tfidf.sort_values(by=['TFIDF'],  ascending=False, inplace=True)
    else : 
        return None

    #---------------------------------------------------------------------------
    # TFIDF tag_count greatest values are returned.
    #---------------------------------------------------------------------------
    return sorted(list(df_row_tfidf.index[:tag_count]))
    
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def get_dict_vocab_frequency(vectorizer, csr_matrix):
    """Returns a dictionary {index:frequency} where : 
         * index values are indexes from vocabulary
         * frequency is the number of occurences of a term from vocabulary

       csr_matrix is used in order to compute occurences for any feature.
       A feature from csr_matrix is a term in vocabulary.

       This fuction appplies for BOW vectorization only.
    """
    
    if type(vectorizer) is not CountVectorizer:
        print("\n*** ERROR : get_dict_frequency function should apply with \
        CountVectorizer type only!")
        return None
    else: 
        pass

    #-------------------------------------------------------------------------------
    # Get list of indexes for any features
    #-------------------------------------------------------------------------------
    list_feature_index = [index  for (tag, index) \
    in vectorizer.vocabulary_.items()]

    #-------------------------------------------------------------------------------
    # Total occurence is computated for any feature.
    #-------------------------------------------------------------------------------
    arr_total_occurence = csr_matrix.toarray().sum(axis=0)

    #-------------------------------------------------------------------------------
    # Build dictionary {feature_index:occurence} for the given row
    #-------------------------------------------------------------------------------
    dict_vocabularyIndex_occurence = dict()
    for feature_index, total_occurence_value \
    in zip(list_feature_index, arr_total_occurence) :
        dict_vocabularyIndex_occurence[feature_index]=total_occurence_value

    return dict_vocabularyIndex_occurence
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def get_list_tag_stat_ngram(sentence, vectorizer, csr_matrix, tag_ratio=1.0) :
    #---------------------------------------------------------------------------
    # Checking parameters
    #---------------------------------------------------------------------------
    if type(vectorizer) is not CountVectorizer:
        print("\n*** ERROR : get_dict_frequency function should apply with \
        CountVectorizer type only!")
        return None
    else: 
        pass

    #---------------------------------------------------------------------------
    # Using vectorized sentence in csrmatrix, get indexes from all features 
    # with frequencies >0 
    #---------------------------------------------------------------------------
    csrmatrix= vectorizer.transform([sentence])
    arr_index = np.where(csrmatrix.A>0)[1]

    #---------------------------------------------------------------------------
    # Using array of indexes from features contained in sentence, 
    # vocabulary terms are extracted.
    #---------------------------------------------------------------------------
    list_tag= [tag for tag, index in vectorizer.vocabulary_.items() \
    if index in arr_index]

    #---------------------------------------------------------------------------
    # Build dictionary {TAG:TotalFreq} where TotalFreq is the sum over 
    # all rows from corpus matrix (csr_matrix) for any TAG belonging to list_tag.
    #---------------------------------------------------------------------------
    dict_frequency_row = dict()
    dict_vocab_frequency = get_dict_vocab_frequency(vectorizer, csr_matrix)
    for tag in list_tag : 
        index_tag = vectorizer.vocabulary_[tag]
        dict_frequency_row[tag] = dict_vocab_frequency[index_tag]

    #---------------------------------------------------------------------------
    # Dictionary is converted as a dataframe allowing values to be ordered.
    #---------------------------------------------------------------------------
    df_row_frequency \
    = pd.DataFrame.from_dict(dict_frequency_row, orient='index')
    
    df_row_frequency.rename(columns={0:'Frequency'}, inplace=True)
    df_row_frequency.sort_values(by=['Frequency'],  ascending=False, inplace=True)

    #---------------------------------------------------------------------------
    # Get tag_count most greater value from dict_index
    #---------------------------------------------------------------------------
    list_term_sentence = sentence.split(' ')
    tag_count = int(len(list_term_sentence)*tag_ratio)

    return sorted(list(df_row_frequency.index[:tag_count]))
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def get_list_tag_stat_bow(sentence, vectorizer, csr_matrix, tag_ratio=1.0) :
    """ Returns a list of TAGS from sentence given as parameter.
    List of returned TAGs count is a ratio from sentence words count.
    
    This fuction appplies for BOW vectorization only. This mean, documents are
    vectors from which components values are occurences or frequencies of 
    terms from vocabulary. 
    
    Terms may be N-gram type with N>=1.
    
    Those frequencies values are stored into csr_matrix given as parameter 
    while terms from vocabulary are stored into vectorizer.
    
    Selected tags belong both to vocabulary and words from given sentence.

    Terms from vocabulary having greater occurence value are selected as TAGs.
    
    """
    #---------------------------------------------------------------------------
    # Checking parameters
    #---------------------------------------------------------------------------
    if type(vectorizer) is not CountVectorizer:
        print("\n*** ERROR : get_dict_frequency function should apply with \
        CountVectorizer type only!")
        return None
    else: 
        pass

    #---------------------------------------------------------------------------
    # Get intersection between words in vocabulary and and words in list_term.
    # TAG will be assigned from vocabulary issued from vectorization.
    #---------------------------------------------------------------------------
    list_term_sentence = sentence.split(' ')
    list_tag_row = list(set(list_term_sentence) & set(vectorizer.vocabulary_))

    #---------------------------------------------------------------------------
    # Build dictionary {TAG:Freq}
    #---------------------------------------------------------------------------
    dict_frequency_row = dict()
    dict_frequency = get_dict_vocab_frequency(vectorizer, csr_matrix)
    for tag_row in list_tag_row : 
        index = vectorizer.vocabulary_[tag_row]
        dict_frequency_row[tag_row] = dict_frequency[index]

    df_row_frequency \
    = pd.DataFrame.from_dict(dict_frequency_row, orient='index')
    
    df_row_frequency.rename(columns={0:'value'}, inplace=True)
    df_row_frequency.sort_values(by=['value'],  ascending=False, inplace=True)

    #---------------------------------------------------------------------------
    # Get tag_count most greater value from dict_index
    #---------------------------------------------------------------------------
    tag_count = int(len(list_term_sentence)*tag_ratio)

    return sorted(list(df_row_frequency.index[:tag_count]))
#-------------------------------------------------------------------------------


#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def cb_clean_lxml(str_lxml, mode='lower'):
    """This function clean (remove) all LXML markers from str_lxml given as parameter.
    BeautifulSoup library is used to process LXML string.
    
    It returns a cleaned string in low case.
    
    If mode value given as parameter is neither lower nor upper, then cleaned 
    string is returned in lower mode.
    """

    if 'lower' == mode : 
        soup = BeautifulSoup(str_lxml.lower(),"lxml") 
    elif 'uper' == mode : 
        soup = BeautifulSoup(str_lxml.upper(),"lxml") 
    else : 
        soup = BeautifulSoup(str_lxml.lower(),"lxml") 
        
    return soup.get_text()

#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def cb_sentence_filter(sentence, mode='lower'):
    """Remove all patterns that are not alpha-digital words, 
    also not '=' nor '++' characters; replace then with ' ' character.    
    """
    
    sentence_filtered = re.sub("[^a-zA-Z0-9=++# ]", " ", sentence )
    if mode=='lower' :
        sentence_filtered = sentence_filtered.lower()
    elif mode=="upper" :
        sentence_filtered = sentence_filtered.upper()
    else:
        sentence_filtered = sentence_filtered.lower()

    return sentence_filtered
    
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def cb_remove_code_source_compare(item):
    """This function removes from item given as parameter source code comparaison expression 
    such as A=B or A>=cf,...
    """
    my_text = item.replace(' = ','=')
    my_text = my_text.replace(' == ','==')
    my_text = my_text.replace(' > ','>')
    my_text = my_text.replace(' < ','<')
    my_text = my_text.replace(' >= ','>=')
    my_text = my_text.replace(' <= ','<=')
    my_text = my_text.replace('  ',' ')
    
    
    my_text = re.sub("([A-Za-z0-9]+)(=+)([A-Za-z0-9]+)",' ',my_text)    
    my_text = re.sub("([A-Za-z0-9]+)(>)([A-Za-z0-9]+)",' ',my_text)    
    my_text = re.sub("([A-Za-z0-9]+)(<)([A-Za-z0-9]+)",' ',my_text)    
    my_text = re.sub("([A-Za-z0-9]+)(>=)([A-Za-z0-9]+)",' ',my_text)    
    my_text = re.sub("([A-Za-z0-9]+)(<=)([A-Za-z0-9]+)",' ',my_text)    
    return my_text
#-------------------------------------------------------------------------------    

#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def get_list_sentence_from_df(df, feature) : 
    if feature not in df.columns : 
        print("*** ERROR : no column named "+str()+" into dataframe!")
        return None

    list_sentence = list()
    for sentence in df[feature].tolist():
        #tokenized_sentence = sentence.split(' ')
        tokenized_sentence=nltk.word_tokenize(sentence)
        list_sentence.append(tokenized_sentence)    
    return list_sentence
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def cb_remove_marker(str_marked,str_marker):
    """Removes text in between markers <str_marker> and </str_marker>
    
        * Input :         
            str_marked : marked string, meaen text in between markers
            str_marker : marker compounding removed text.
    """
    str_marked = str_marked.replace('\n','')
    marker_start='<'+str_marker+'>'
    marker_end='</'+str_marker+'>'
    str_cleaned = re.sub(marker_start+"(.*)"+marker_end,"", str_marked.lower())
    return str_cleaned
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def get_list_tag_from_post(post, ml_model, max_tag=5):
    """Returns a list of most appropriate tags from a post.
    Tags are computed from a M.L model.
    
    """
    list_tag_returned = list()
    tokenized_post = post.split()
    list_tag_pred = ml_model.predict_output_word(tokenized_post)
    tag_count = 0
    for tuple_word_percent in list_tag_pred :
        if tag_count < max_tag:
            tag = "<"+str(tuple_word_percent[0])+">"
            list_tag_returned.append(tag)
            tag_count += 1

    return list_tag_returned
#-------------------------------------------------------------------------------



#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def cb_remove_verb_from_sentence(sentence):
    """Remove verbs from sentence given as parameter and returns a sentence
    without any verb
    """
    
    #---------------------------------------------------------------------------
    # All sentences are tokenized and words are tagged.
    #---------------------------------------------------------------------------
    list_tagged = list()
    tokenized_sentence = nltk.word_tokenize(sentence)
    list_tagged += nltk.pos_tag(tokenized_sentence)
    
    #-----------------------------------------------------------------------
    # List of tagged words from sentence are filtered
    #-----------------------------------------------------------------------
    list_word_filtered = list()
    for tuple_word_tag in list_tagged:
        tag = tuple_word_tag[1]
        if 'V' == tag[0] :
            pass
        else:
            list_word_filtered.append(tuple_word_tag[0])
    #-----------------------------------------------------------------------
    # Tokenized words that have been filtered are compound into a sentence
    #-----------------------------------------------------------------------
    sentence = " ".join(list_word_filtered)
    return sentence

#-------------------------------------------------------------------------------
#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def cb_remove_stopwords(item, lang='english', mode='lower') :
    """This function removes some stopwords form item given as parameter.
    Removed stopwords are issued from 'nltk.corpus.stopwords'.
    
    """
    list_word=item.split()
    item_no_stopwords_1=[ word for word in list_word if word.lower() not in stopwords.words(lang) \
                      and not str(word).isdigit()]
    item_no_stopwords=[word for word in item_no_stopwords_1 if word.lower() not in ['cnn']]
    
    item_no_stopwords=" ".join(item_no_stopwords)
    if mode == 'upper':
        return item_no_stopwords.upper()
    elif mode =='lower':
        return item_no_stopwords.lower()
    else:
        print("*** ERROR no mode defined for word! ***")
        return None
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def compute_frequency_sentence(dict_content, token_mode='split'):
    """Computes each word frequency from a dictionary of contents.
    Contents from dict_content are tokenized using split() method as default 
    mode.
    
    Other mode option is nltk to tokenize contents.
    """
    
    list_content = list()
    if token_mode == 'split' :
        for root_name in dict_content.keys():
            content= dict_content[root_name]
            tokenized_content = content.split(' ')
            list_content += tokenized_content
    elif token_mode == 'nltk' :
        for root_name in dict_content.keys():
            content= dict_content[root_name]
            tokenized_content = nltk.word_tokenize(content)
            list_content += tokenized_content
    else :
        print("*** ERROR : unknown tokenization mode= "+str(token_mode))
        return None

    freq_content = nltk.FreqDist(list_content)

    return freq_content
#-------------------------------------------------------------------------------
#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def preprocess_post(question, is_stemming=False, is_lem=True, is_stopword=True\
    ,is_stopverb=True, is_stopalfanum=True):
    """Applies to a given question all transformations in order to clean 
    text model.
        Input : 
            * question : suite of words forming a question
            * is_stemming : when True, stemming is applied on given post.
            * is_lem : when True, lemmatization is applied on given post.
            * is_stopword : when True, engish stopwords are filtered from post.
            * is_stopverb : when True, engish verbs are filtered from post.
            * is_stopalfanum : when True, non alpha-numeric characters are filtered 
            from given question.
        Output :
            * dataframe with  standardized Body column.
    """
    df = pd.DataFrame({'Body':question}, index=[0,])
   
    #print("\nCleaning text in-between markers <code></code> markers...")
    df["Body"] = df.Body.apply(cb_remove_marker,args=('code',))

    #print("\nCleaning LXML markers...")
    df["Body"] = df.Body.apply(cb_clean_lxml)

    if is_stopalfanum is True : 
        #print("\nRemove non alfa-numeric patterns")
        df["Body"] = df.Body.apply(cb_sentence_filter)

    if is_stopverb is True : 
        #print("\nFiltering sentences...")
        df["Body"] = df.Body.apply(cb_remove_verb_from_sentence)

    if is_stopword is True : 
        #print("\nRemoving stopwords...")
        df["Body"] = df.Body.apply(cb_remove_stopwords)

    if is_lem is True:
        #print("\nLemmatization ...")
        lemmatizer=WordNetLemmatizer()
        df['Body']=df.Body.apply(p5_util.cb_lemmatizer,args=(lemmatizer,'lower'))

    if is_stemming is True:
        #print("\nEnglish stemming ...")
        stemmer=SnowballStemmer('english')
        df['Body']=df.Body.apply(p5_util.cb_stemmer,args=(stemmer,'lower'))



    return df
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def get_tag_count(str_tags):
    """Returns number of tags from a string str_tag, given as parameter.
    str_tags is expected having following format: <tag1><tag2>...<tagN>
    """
    tag_count = 0
    for char_marker in str_tags:
        if char_marker == '<':
            tag_count +=1
    return tag_count
#-------------------------------------------------------------------------------


#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def taglist_stat_predict(df_corpus, document_index, embedding_mode, vectorizer\
                        , csr_matrix, p_tag_ratio):
    """Returns a predicted list of TAGS from a given row from df_corpus 
    dataframe.
    
    Prediction is based on statistics of terms over the whole corpus.

    Given dataframe contains corpus of documents.

    Each row from Dataframe contains a document from which returned TAGs are 
    predicted.

    TAGs are extracted from vocabulary handled into vectorizer operator that 
    is given as parameter.
    
    Input : 
        * df_corpus : dataframe that is a corpus represented as a Bag Of Words .
        * document_index : document identifier from corpus (dataframe) fro which 
        TAGs will be extracted.
        * vectorizer : operator that has been built in order to embbed documents
         words
        * embeding_mode : may be TF-IDF, BOW, COO(co-occurence)
        * p_tag_ratio : ratio of extracted TAGs=number_of_tags/number_of_words
        This allows to compute the number of TAGs to be returned.
    Output :
        * The list of ordered predicted TAGS
        * The list of original TAGS
        * The original document isuue from df_corpus at document_index position.
    
    """
    
    #---------------------------------------------------------------------------
    # Check parameters
    #---------------------------------------------------------------------------
    if 'Body' not in df_corpus.columns :
        print("\n*** ERROR : dataframe corpus must contain column named Body!")
        return None,None,None
    else : 
        pass

    if 'Tags' not in df_corpus.columns :
        print("\n*** ERROR : dataframe corpus must contain column named Tags!")
        return None,None,None
    else : 
        pass
        
    if embedding_mode not in LIST_EMBEDDING_MODE :
        print("\n *** ERROR : embedding mode not supported : "\
        +str(embedding_mode))
        return None,None,None
    else : 
        pass
    
    #---------------------------------------------------------------------------
    # Extract document from corpus thanks to row index.
    #---------------------------------------------------------------------------
    if document_index is not None:
        df_document = df_corpus[df_corpus.index==document_index]
    else : 
        df_document = df_corpus


    #---------------------------------------------------------------------------
    # Document is normalized
    #---------------------------------------------------------------------------
    df_document_std = p6_df_standardization(df_document)


    #---------------------------------------------------------------------------
    # Extract standardized sentence as a dictionary : {document_id:sentence}
    #---------------------------------------------------------------------------
    dict_sentence = df_document_std.Body.to_dict()
    sentence_std = dict_sentence[document_index]

    #---------------------------------------------------------------------------
    # Extract TAGs from standardized document using vectorizer that contains
    # vocabulary and statistical values upon each document word.
    # Number of TAGs in list depends on tag_ratio criteria. This criteria 
    # applies on number of words in docuement.
    #---------------------------------------------------------------------------
    if embedding_mode == 'tfidf':
        list_predicted_tag \
        = get_list_tag_stat_tfidf(sentence_std, vectorizer\
        , tag_ratio=p_tag_ratio)
    elif embedding_mode == 'bow' or embedding_mode == 'ngram':
        list_predicted_tag \
        = get_list_tag_stat_ngram(sentence_std, vectorizer\
        , csr_matrix, tag_ratio=p_tag_ratio)
    else :
        pass

    #---------------------------------------------------------------------------
    # Get formated list of original TAGs for this document
    #---------------------------------------------------------------------------
    str_original_tag=df_document.Tags[document_index]
    list_tag_original=get_list_tag_original(str_original_tag)

    #---------------------------------------------------------------------------
    # Record original document as long as original TAG list
    #---------------------------------------------------------------------------
    str_original_document = df_corpus.iloc[document_index].Body
    
    return list_predicted_tag, list_tag_original, str_original_document
    
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def get_list_cluster_tag(dict_corpus, vectorizer, cluster_labels\
, cluster_id, p_tag_ratio, is_log=True):
    """Returns a list of tags for a given cluster.
    Input : 
        * dict_corpus : dictionary : {index:document} for the whole corpus
        * vectorizer : operator used for corpus vectorization
        * cluster_labels : list of clusters labels
        * cluster_id : a cluster identifier
        * p_tag_ratio : ration of tags considering question lentgh.
    Output : 
        * list of tags from cluster given as parameter.
    """
    #---------------------------------------------------------------------------
    # Get corpus indexes for any document assigned with the given cluster;
    # This allows to access all documents from dict_corpus belonging to cluster.
    #---------------------------------------------------------------------------
    arr_cluster_index = np.where(cluster_labels==cluster_id)[0]
    
    #---------------------------------------------------------------------------
    # Initialization of list of TAGs relative to the cluster.
    #---------------------------------------------------------------------------
    list_cluster_tag = list()

    #---------------------------------------------------------------------------
    # Process any document assigned with cluster_id
    #---------------------------------------------------------------------------
    document_count = len(arr_cluster_index)
    for index in arr_cluster_index :
        document = dict_corpus[index]
        #-----------------------------------------------------------------------
        # Get TAGs from document assigned with cluster_id; 
        # we get all N-grams from document.
        # Number of TAGs is limited with p_tag_ratio.
        #-----------------------------------------------------------------------
        list_cluster_tag_ = p6_util.get_list_tag_stat_tfidf(document\
        , vectorizer, tag_ratio=p_tag_ratio)
        
        #-----------------------------------------------------------------------
        # Aggregate into a single list all TAGs from cluster.
        # These aggragated TAGs will caracterized the cluster.
        #-----------------------------------------------------------------------
        if list_cluster_tag_ is not None :
            list_cluster_tag += list_cluster_tag_

    if is_log is True:
        print("Cluster #"+str(cluster_id)+" : Number of documents= "\
        +str(document_count)+" : Done!")

    return list_cluster_tag, document_count
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def get_dict_list_cluster_tag( arr_cluster_label, dict_sof_document, vectorizer\
, p_tag_ratio):
    """Returns a dictionary formated as following : {cluster_id:list_tag}
    where :
    * cluster_id is a cluster identifier 
    * list_tag is the list of selected TAGs for cluster_id
    Input : 
        * arr_cluster_label : array containing cluster identifier for each row 
        of vectorized corpus.
        
        * dict_sof_document : standardized corpus formated 
        as following : {index:document}
        
        * vectorizer : operator used for corpus vectorization. 
        Handled dictionary from which TAGs are picked up.
        
        * p_tag_ratio : number of TAGS / number of document terms. 
        It is applied for each document belonging a cluster.
    """
    #---------------------------------------------------------------------------
    # Get all cluster identifiers from arr_cluster_label
    #---------------------------------------------------------------------------
    arr_cluster_id = np.unique(arr_cluster_label)
    
    dict_cluster_stat = dict()
    
    #---------------------------------------------------------------------------
    # For any cluster, retrieve list of TAGs
    #---------------------------------------------------------------------------
    dict_list_cluster_tag = dict()
    dict_df_freq_cluster_tag = dict()
    for cluster_id in arr_cluster_id:
        #-----------------------------------------------------------------------
        # Get, from corpus, all indexes rows assigned with cluster_id
        #-----------------------------------------------------------------------
        arr_corpus_index = np.where(arr_cluster_label==cluster_id)[0]

        #-----------------------------------------------------------------------
        # Retrieve TAGs related to cluster_id.
        # The number of returned TAGs is constrained with p_tag_ratio.
        #-----------------------------------------------------------------------
        is_activated = (0==cluster_id%10)
        list_cluster_tag, document_count \
        = get_list_cluster_tag(dict_sof_document, vectorizer\
        , arr_cluster_label, cluster_id, p_tag_ratio, is_log=is_activated)

        #-----------------------------------------------------------------------
        # For each TAG in list_cluster_tag, tags frequency is computed in order 
        # to filter those tags with greater frequency.
        #-----------------------------------------------------------------------
        freq_cluster_tag = nltk.FreqDist(list_cluster_tag)

        df_freq_cluster_tag \
        = pd.DataFrame.from_dict(freq_cluster_tag, orient="index")
                
        df_freq_cluster_tag.rename(columns={0:'Freq'}, inplace=True)
        df_freq_cluster_tag.sort_values(by=['Freq'], axis=0, ascending=False\
        , inplace=True)

        #-----------------------------------------------------------------------
        # For word clod display over a cluster
        #-----------------------------------------------------------------------
        dict_df_freq_cluster_tag[cluster_id] = df_freq_cluster_tag

        #-----------------------------------------------------------------------
        # Lets format n-gram for having a more convenenient data to be displayed as 
        # a wordcloud set: any bigram word is aggregated into a single word with a 
        # separation character.
        #-----------------------------------------------------------------------
        list_cluster_tag_formated \
        = [re.sub("[' ']", "_", ngram_tag ) \
        for ngram_tag in df_freq_cluster_tag.index]
        
        str_cluster_tag_formated = ' '.join(list_cluster_tag_formated)
        
        dict_list_cluster_tag[cluster_id] = str_cluster_tag_formated
        
        #-----------------------------------------------------------------------
        # For statistics over each cluster
        #-----------------------------------------------------------------------
        dict_cluster_stat[cluster_id] = document_count
        
        
    return dict_list_cluster_tag, dict_cluster_stat, dict_df_freq_cluster_tag
#-------------------------------------------------------------------------------


#-------------------------------------------------------------------------------
#
#-------------------------------------------------------------------------------
def get_tag_intersect_cluster_list_tag(list_question_key_words, dict_cluster_list_tag) :
    """Returns a dictionary formated as following : {cluster_id, tag_count} 
    where tag_count is issued from intersection between list_question_key_words 
    and dict_cluster_list_tag[clster_id], for any cluster_id.
    
    Input : 
        * list_question_key_words : vectorized question
        * dict_list_cluster_tag : dictionary {cluster_id, list_tag} issue from
        a clustering process.
    Output : 
        * A dictionary {cluster_id, tag_count} where,for each cluster_id,  
        tag_count is the number of elements issued from intersection between 
        list_question_key_words and list_tag.
    """

    dict_cluster_intersect_count=dict()
    for cluster_id, list_cluster_tag in dict_cluster_list_tag.items() :
        intersect_count = len(set(list_cluster_tag)&set(list_question_key_words))
        dict_cluster_intersect_count[cluster_id] = intersect_count

    return dict_cluster_intersect_count
#-------------------------------------------------------------------------------
    
