{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<!--NOTEBOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"./figures/LogoOpenclassrooms.png\">\n",
    "<font size=\"4\">\n",
    "    \n",
    "Cette étude a été réalisée dans le cadre du 6ème projet de ma formation Datascientist dispensée en MOOC par \n",
    "\n",
    "<font color='blus'>Openclassrooms / écoles Centrale-Supélec</font>.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p></p><p></p><p></p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Le problème posé :**\n",
    "\n",
    "\n",
    "*Stack Overflow est un site célèbre de question-réponses liées au développement informatique. Pour poser une question sur ce site, il faut entrer plusieurs tags de manière à retrouver facilement la question par la suite. Pour les utilisateurs expérimentés cela ne pose pas de problème, mais pour les nouveaux utilisateurs, il serait judicieux de suggérer quelques tags relatifs à la question posée.*\n",
    "\n",
    "*Amateur de Stack Overflow, qui vous a souvent sauvé la mise, vous décidez d'aider la communauté en retour. Pour cela, vous développez un système de suggestion de tags pour le site. Celui-ci prendra la forme d’un algorithme de machine learning qui assigne automatiquement plusieurs tags pertinents à une question.*\n",
    "\n",
    "\n",
    "\n",
    "**Solutions mises en oeuvre**\n",
    "\n",
    "Les solutions de suggestion de tags présentés ici se basent sur des modèles *non-supervisés* de machine learning.\n",
    "\n",
    "Les modèles mis en oeuvre : \n",
    "    * Clusterisation par K-means\n",
    "    * LDA\n",
    "    * Word2Vec\n",
    "\n",
    "Ce notebook utilise les données issues du notebook **P6_DadaAnalysis.ipynb**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unsupervised methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAGS are regarded as keywords.\n",
    "\n",
    "Aims of this section is to create data points then identify clusters from this set of data.\n",
    "\n",
    "These data-points are digital representation of the cleaned documents dataset.\n",
    "\n",
    "For each-one of the clusters, most representative terms will be identified.\n",
    "\n",
    "These terms will be considered as TAGs.\n",
    "\n",
    "Differents embeddings techniques will be used in order to create data-points : \n",
    "\n",
    "* Bag of words\n",
    "* Co-occurence\n",
    "* TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading train dataset and extract users questions from Body column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24604, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_name=\"./data/train_StackOverFlow_BodyTitleTags.csv\"\n",
    "df_sof_train=pd.read_csv(file_name)\n",
    "print(df_sof_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_sof_train['Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardize `Body` column from dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util\n",
    "ser_document_std = p6_util.p6_df_standardization(df_sof_train['Body'],is_stemming=False, is_lem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save dataframe with standardized `Body` column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "file_name=\"./data/df_document_std.dump\"\n",
    "p5_util.object_dump(ser_document_std, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load standardized corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "file_name=\"./data/df_document_std.dump\"\n",
    "ser_document_std = p5_util.object_load(file_name)\n",
    "print(ser_document_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_document_std = ser_document_std[:16359]\n",
    "print(ser_document_std.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# When building vacabulary, terms with frequency document < p_min_df are ignored.\n",
    "p_min_df = 0.001\n",
    "# When building vacabulary, terms with frequency document> p_max_df are ignored.\n",
    "p_max_df = 1.\n",
    "\n",
    "\n",
    "dict_sof_document = ser_document_std.to_dict()\n",
    "\n",
    "ngram1=2\n",
    "ngram2=2\n",
    "vectorizer=TfidfVectorizer(norm=\"l2\", use_idf=True, min_df=p_min_df, max_df=p_max_df, ngram_range=(ngram1, ngram2))\n",
    "\n",
    "csr_matrix = vectorizer.fit_transform(dict_sof_document.values())\n",
    "\n",
    "print(csr_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save vectorization with ngram (2,2) mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "file_name=\"./data/csr_matrix_tdif_ngram_2_2.dump\"\n",
    "p5_util.object_dump(csr_matrix, file_name)\n",
    "\n",
    "file_name=\"./data/vectorizer_tdif_ngram_2_2.dump\"\n",
    "p5_util.object_dump(vectorizer, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Kmeans clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load vectorization operator and CSR matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "file_name=\"./data/csr_matrix_tdif_ngram_2_2.dump\"\n",
    "csr_matrix = p5_util.object_load(file_name)\n",
    "print(csr_matrix.shape)\n",
    "\n",
    "file_name=\"./data/vectorizer_tdif_ngram_2_2.dump\"\n",
    "vectorizer = p5_util.object_load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proceed to Kmeans clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "n_cluster=100\n",
    "cluster_kmean=KMeans(n_clusters=n_cluster).fit(csr_matrix)\n",
    "\n",
    "print(len(cluster_kmean.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Kmeans clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=\"./data/cluster_kmeans_labels_100.dump\"\n",
    "p5_util.object_dump(cluster_kmean.labels_, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For each cluster build a TAG list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=\"./data/cluster_kmeans_labels_100.dump\"\n",
    "cluster_kmean_labels = p5_util.object_load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util\n",
    "p_tag_ratio=1.0\n",
    "dict_list_cluster_tag, dict_cluster_stat, dict_df_freq_cluster_tag \\\n",
    "= p6_util.get_dict_list_cluster_tag(cluster_kmean_labels, dict_sof_document, vectorizer, p_tag_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "file_name=\"./data/dict_list_cluster_tag.dump\"\n",
    "p5_util.object_dump(dict_list_cluster_tag, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot cluster distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Convert dictionary as dataframe allowing having statisticals values\n",
    "df_cluster_stat = pd.DataFrame.from_dict(dict_cluster_stat, orient='index')\n",
    "df_cluster_stat.rename(columns={0:'cluster'},inplace=True)\n",
    "\n",
    "min_cluster_count = min(df_cluster_stat.cluster)\n",
    "max_cluster_count = max(df_cluster_stat.cluster)\n",
    "\n",
    "#Get outliers clusters : mix and max values\n",
    "cluster_max_id = df_cluster_stat.query(\"cluster == \"+str(max_cluster_count), inplace=False).index[0]\n",
    "cluster_min_id = df_cluster_stat.query(\"cluster == \"+str(min_cluster_count), inplace=False).index[0]\n",
    "\n",
    "print(\"Cluster {} : elements= {} \".format(cluster_min_id, min_cluster_count))\n",
    "print(\"Cluster {} : elements= {} \".format(cluster_max_id, max_cluster_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Questions distribution among clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_stat.query(\"cluster <= 1000\", inplace=True)\n",
    "ax = df_cluster_stat.plot.bar(figsize=(7, 7), title=\"Questions distribution\", color='orange', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display cluster with lower number of elements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_plot\n",
    "df_freq_cluster_tag = dict_df_freq_cluster_tag[cluster_min_id]\n",
    "dict_freq_cluster_tag = df_freq_cluster_tag.to_dict()['Freq']\n",
    "p6_util_plot.display_word_cloud(dict_freq_cluster_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_list_cluster_tag[cluster_min_id])\n",
    "print(\"\\n\")\n",
    "#print(dict_list_cluster_tag[cluster_max_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display cluster with greater number of elements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util_plot\n",
    "df_freq_cluster_tag = dict_df_freq_cluster_tag[cluster_max_id]\n",
    "dict_freq_cluster_tag = df_freq_cluster_tag.to_dict()['Freq']\n",
    "p6_util_plot.display_word_cloud(dict_freq_cluster_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions (Posts) are vectorized.\n",
    "\n",
    "Vectorization leads to a Matrix.\n",
    "\n",
    "Transpozed matrix is clusterized : transpozed matrix columns are questions (Posts).\n",
    "\n",
    "A clustering algorythm is applied over transposed matrix. Each cluster compound a set of questions.\n",
    "\n",
    "Each cluster is assigned with some TAGs that way : \n",
    "\n",
    "* In any cluster, TAGs are picked up from vectors components.\n",
    "\n",
    "* For each one of the cluster element : \n",
    "    * Greatest components values are picked up.\n",
    "    * TAGs assigned are vocabulary terms matching with picked-up values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract a question from test data-set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_name=\"./data/test_StackOverFlow_BodyTitleTags.csv\"\n",
    "df_sof_test=pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import p6_util\n",
    "\n",
    "i_random = random.choice(range(0, df_sof_test.shape[0]))\n",
    "question= df_sof_test.Body.iloc[i_random]\n",
    "print(\"\\nQuestion : \\n\")\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question is standardized**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util\n",
    "\n",
    "df_std =p6_util.preprocess_post(question, is_stemming=False, is_lem=True, is_stopword=True\\\n",
    "    ,is_stopverb=True, is_stopalfanum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_std = df_std.to_dict()['Body'][0]\n",
    "print(post_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert standardized post into bigram terms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "post_std_bigrams = nltk.bigrams(post_std.split(' '))\n",
    "\n",
    "\n",
    "print(post_std)\n",
    "list_key_words = [ tuple_bigram[0]+'_'+tuple_bigram[1] for tuple_bigram in post_std_bigrams]\n",
    "list_key_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search for cluster question belongs to**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "file_name=\"./data/dict_list_cluster_tag.dump\"\n",
    "dict_list_cluster_tag = p5_util.object_load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util\n",
    "dict_cluster_freq = p6_util.get_tag_intersect_cluster_list_tag(post_std, dict_list_cluster_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_freq = pd.DataFrame.from_dict(dict_cluster_freq, orient='index')\n",
    "\n",
    "df_cluster_freq.rename(columns={0:'Frequency'}, inplace=True)\n",
    "df_cluster_freq.sort_values(by=['Frequency'],  ascending=False, inplace=True)\n",
    "print(\"Question has been attached to cluster = \"+str(df_cluster_freq.iloc[0]))\n",
    "dict_cluster_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load TDIF Matrix and related vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "file_name=\"./data/csr_matrix_tdif.dump\"\n",
    "csr_matrix = p5_util.object_load(file_name)\n",
    "\n",
    "print(csr_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "file_name=\"./data/vectorizer_tdif_ngram_2_2.dump\"\n",
    "vectorizer = p5_util.object_load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        message = \"Topic %d: \" % (topic_idx)\n",
    "        message += \" / \".join([feature_names[i] \\\n",
    "                               for i in topic.argsort()[:-no_top_words - 1:-1] \\\n",
    "                               if i<len(feature_names)])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic %d:\" % (topic_idx)\n",
    "        message += \" / \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        print(message)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA with full standardized dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "no_topics = 100\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5\\\n",
    "                                , learning_method='online', learning_offset=50.,random_state=0).fit(csr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_top_words = 1\n",
    "display_topics(lda, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "p_eps=0.1\n",
    "dbscan = DBSCAN(eps=p_eps,min_samples=350, n_jobs=3)\n",
    "\n",
    "if 'cluster' in df_sof.columns:\n",
    "    del(df_sof['cluster'])\n",
    "pred_dbscan =  dbscan.fit_predict(df_customers)\n",
    "\n",
    "import numpy as np\n",
    "np.unique(pred_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN custering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSR matrix is converted into dataframe in order to feed cluster algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_sof = pd.DataFrame(csr_matrix.toarray())\n",
    "print(df_sof.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "cluster_start = 1\n",
    "cluster_end = 10\n",
    "dict_kmeans = p5_util.kmeans_scan_inter_inertia(df_sof, cluster_start, cluster_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Kmeans clustering scan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "\n",
    "fileName = './data/dict_tfidf_clustering_kmeans.dump'\n",
    "p5_util.object_dump(dict_kmeans, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util_plot\n",
    "import p5_util\n",
    "is_reloaded = False\n",
    "if is_reloaded is True:\n",
    "    fileName = './data/dict_tfidf_clustering_kmeans.dump'\n",
    "    dict_kmeans = p5_util.object_load(fileName)\n",
    "    print(\"Records into dict_kmeans= \"+str(len(dict_kmeans)))\n",
    "\n",
    "x_label_0 = 'Features : Nb. clusters'\n",
    "x_label_1 = 'Features : Nb. clusters'\n",
    "cluster_start = list(dict_kmeans.keys())[0]\n",
    "cluster_end = list(dict_kmeans.keys())[len(dict_kmeans)-1]\n",
    "print(cluster_start, cluster_end)\n",
    "p5_util_plot.plot_kmeans_interInertia(dict_kmeans, cluster_start, cluster_end\\\n",
    "                             ,[x_label_0, x_label_1]\\\n",
    "                             ,p_rows=1, p_cols=2, p_figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dict_kmeans[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding corpus with Word2vec model\n",
    "\n",
    "It is expected that ``word2vec`` learns relationships between words automatically.\n",
    "\n",
    "* ``df_sof_train['Body']`` represents the corpus with which ``word2vec`` model is feeded.\n",
    "* Once feed, ``word2vec`` will build a vocabulary, mean, a list of words (features) from which any word from corpus is vectorized.\n",
    "* Matrix [Corpus words x Features] is stored in ``word2vec_model.wv.vectors``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util\n",
    "list_sentence = p6_util.get_list_sentence_from_df(df_sof_train, 'Body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec_model = Word2Vec(list_sentence, min_count=2, size=100, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word2vec_model.wv.vocab))\n",
    "len(word2vec_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.save(\"word2vec_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_name=\"./data/test_StackOverFlow_BodyTitleTags.csv\"\n",
    "df_sof_test=pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A post is randomly selected from test dataset.\n",
    "* Then post is ipre-processed in order to feed M.L. model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import p6_util\n",
    "\n",
    "i_random = random.choice(range(0, df_sof_test.shape[0]))\n",
    "body= df_sof_test.Body.iloc[i_random]\n",
    "title= df_sof_test.Title.iloc[i_random]\n",
    "post = body+title\n",
    "post=title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post=title\n",
    "df_post = p6_util.preprocess_post(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tag counts is computed from test.\n",
    "* Computed tags are returned with same number and same format then tags from test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util\n",
    "tag_count=p6_util.get_tag_count(df_sof_test.Tags.iloc[i_random])\n",
    "\n",
    "list_computed_tag = p6_util.get_list_tag_from_post(df_post.Body.iloc[0], word2vec_model, max_tag=tag_count)\n",
    "print(\"Computed tags = \"+\"\".join(list_computed_tag))\n",
    "print(\"\")\n",
    "print(\"Tags from test = \"+df_sof_test.Tags.iloc[i_random])\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Posted question : \\n\")\n",
    "print(\"Title : \"+str(title))\n",
    "print(\"\")\n",
    "print(\"Body: \"+str(body))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2D projection of Word2Vec embedded vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec.load(\"word2vec_model\")\n",
    "\n",
    "len(word2vec_model.wv.vocab.keys()), len(word2vec_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute matrix for any word into corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load cleaned train-dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "file_name='./data/df_sof_train.dump'\n",
    "df_sof_train=p5_util.object_load(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get SOF corpus from dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p6_util\n",
    "list_corpus = p6_util.get_list_sentence_from_df(df_sof_train, 'Body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_corpus=list()\n",
    "i=0\n",
    "for list_word in list_corpus:\n",
    "    list_tokenized_corpus += list_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_tokenized_corpus[1:]), len(list_tokenized_corpus)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "len(list_tokenized_corpus)\n",
    "word0 = list_tokenized_corpus[0]\n",
    "word1 = list_tokenized_corpus[1]\n",
    "\n",
    "#arr = word2vec_model[word0].reshape(1,-1)\n",
    "#arr = word2vec_model[word0]\n",
    "#print(arr.shape)\n",
    "#arr1= word2vec_model[word1].reshape(1,-1)\n",
    "arr1 = word2vec_model[word1]\n",
    "#print(arr1.shape)\n",
    "arr = np.vstack((arr,arr1))\n",
    "print(arr.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word2vec_model[\"10\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word = list_tokenized_corpus[0]\n",
    "matrix=word2vec_model[word]\n",
    "for word in list_tokenized_corpus[1:]:\n",
    "    matrix = np.vstack((matrix,word2vec_model[word]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D t-SNE transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load TDIF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "file_name=\"./data/csr_matrix_tdif.dump\"\n",
    "csr_matrix = p5_util.object_load(file_name)\n",
    "\n",
    "print(csr_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get sample from CSR matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "arr = np.zeros(100)\n",
    "array = csr_matrix.toarray()\n",
    "#array_index_sample = random.sample(list(csr_matrix.shape), 10)\n",
    "\n",
    "arr_index = np.random.randint(0,array.shape[0],1000)\n",
    "array_sample = csr_matrix.toarray()[arr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_sample.shape, array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute for embedded vocabulary t-SNE projection with different perplexity values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "if True:\n",
    "    #X_std=word2vec_model.wv.vectors\n",
    "    X_std = array_sample\n",
    "    my_list_tsne_perplexity=[var for var in range(5,55,5)]\n",
    "    dict_tsne_result = p5_util.tsne_2D_process_perplexity(X_std, tsne_iter=1000, list_tsne_perplexity=my_list_tsne_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot points issued from t-SNE transformations for each perplexity value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util_plot\n",
    "if True :\n",
    "    nb_col = 4     \n",
    "    p5_util_plot.plot_2D_dict_tsne_result(dict_tsne_result, nb_col, ratio=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute vocabulary t-SNE transformation for a fixed perplexity value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "\n",
    "X_std=word2vec_model.wv.vectors\n",
    "print(X_std.shape)\n",
    "perplexity=10\n",
    "my_list_tsne_perplexity=[perplexity,]\n",
    "dict_tsne_result = p5_util.tsne_2D_process_perplexity(X_std, tsne_iter=300, list_tsne_perplexity=my_list_tsne_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_tsne_result[perplexity]),X_std.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot points issued from t-SNE embedded vocabulary transformation with words related to each point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import p5_util_plot\n",
    "\n",
    "nb_col = 1\n",
    "dict_tsne_result_part={perplexity:dict_tsne_result[perplexity]}\n",
    "p_annotation = np.array(list(word2vec_model.wv.vocab.keys()))\n",
    "p5_util_plot.plot_2D_dict_tsne_result(dict_tsne_result_part, nb_col, ratio=0.05, annotation=p_annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_pca=pd.DataFrame(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p3_util_plot\n",
    "z__ = p3_util_plot.df_pca_all_plot(df_pca, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "nb_components=600\n",
    "pca = PCA(n_components=nb_components)\n",
    "pca.fit(X_std)\n",
    "X_pca = pca.transform(X_std)\n",
    "print(X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save reduced PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "file_name=\"./data/pca_600_tdif.dump\"\n",
    "p5_util.object_dump(X_pca, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p3_util_plot\n",
    "p_ratio=0.1\n",
    "title = str(int(p_ratio*100))+\" % Embedded TF-IDF vocabulary\"\n",
    "pca_=p3_util_plot.df_pca_components_plot(df_pca, None, nb_components=2, param_title=title, ratio=p_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "list(word2vec_model.wv.vocab.keys())\n",
    "df_vocab=pd.DataFrame(list(word2vec_model.wv.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p3_util_plot\n",
    "\n",
    "p3_util_plot.df_pcs2_plot(df_pca, pca_)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
