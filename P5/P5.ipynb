{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NOTEBOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"./figures/LogoOpenclassrooms.png\">\n",
    "<font size=\"4\">\n",
    "<p>\n",
    "Cette étude a été réalisée dans le cadre du 5ème projet de ma formation Datascientist dispensée en MOOC par \n",
    "**<font color='blus'>Openclassrooms / écoles Centrale-Supélec</font>**.\n",
    "</p>    \n",
    "\n",
    "<p>\n",
    "Ce notebook présente un modèle de ségmentation de clients d'un site d'achats en ligne. \n",
    "</p>\n",
    "<p>\n",
    "Ce modèle se base sur les données fournies par le site :\n",
    "</p>\n",
    "<p>\n",
    "https://archive.ics.uci.edu/ml/datasets/Online+Retail\n",
    "</p>\n",
    "<p>\n",
    "</p>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blus'> Synthèse du 12/07/2018</font>\n",
    "\n",
    "* Modèle simplifié (sans référence aux produits) : réduction de dimension par Algo t-SNE\n",
    "\n",
    "    * Variables qualitatives\n",
    "    * Variables quantitatives\n",
    "    * Aggregation des variables dans la mm structure.\n",
    "\n",
    "* Outliers : élimination.\n",
    "\n",
    "* Redondance : voire les corrélations.\n",
    "\n",
    "* Méthodes pour le clustering. Comparer avec des données.\n",
    "    * Kmeans \n",
    "    * GMM\n",
    "    * DBSCAN\n",
    "\n",
    "* Méthodes de prédiction\n",
    "    * Random Forest\n",
    "    * SVM linéaire\n",
    "\n",
    "\n",
    "## <font color='blue'>Analyse exploratoire des données</font>\n",
    "* A traiter: statistiques des features retenues\n",
    "## <font color='blue'>Score RFM</font>\n",
    "* Score RFM : nouvelle feature dans le dataset\n",
    "* Suppression des features issues du process RFM\n",
    "* Suppression du pays\n",
    "## <font color='blue'>PCA</font>\n",
    "* Analyse syntaxique de la description  des produits.\n",
    "* Représetation 2D ou 3D des clusters\n",
    "* Application du t-SNE pour réductiond de dimension\n",
    "* Application du DBSAN pour le clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blus'> Synthèse du 05/07/2018</font>\n",
    "\n",
    "* Parties du dataset\n",
    "\n",
    "* Classes d'un client : table d'une transaction a une table client.==> groupby\n",
    "\n",
    "* Variables vont caractérisier les clients.\n",
    "\n",
    "* Description : faire des expressions régulieres pour détcter des motifs pertinents. Predre des mots clés spécifiques et construire des variables à partir de ces mots clés.\n",
    "\n",
    "* Fréquence de mots clés : atteindre un seuil.\n",
    "\n",
    "* Identifiant de deux mots similaires.\n",
    "\n",
    "* Outliers : éliminer.\n",
    "\n",
    "* Redondance : voire les corrélations.\n",
    "\n",
    "* Essayer d'autres méthodes pour le clustering. Comparer avec des données.\n",
    "\n",
    "\n",
    "## <font color='blue'>Analyse exploratoire des données</font>\n",
    "* A traiter: statistiques des features retenues\n",
    "## <font color='blue'>Score RFM</font>\n",
    "* Score RFM : nouvelle feature dans le dataset\n",
    "* Suppression des features issues du process RFM\n",
    "* Suppression du pays\n",
    "## <font color='blue'>PCA</font>\n",
    "* Analyse syntaxique de la description  des produits.\n",
    "* Représetation 2D ou 3D des clusters\n",
    "* Application du t-SNE pour réductiond de dimension\n",
    "* Application du DBSAN pour le clustering\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blus'> Synthèse du 21/06/2018</font>\n",
    "## <font color='blue'>Questions</font>\n",
    "* Classification + Prédiction?\n",
    "* Séries temporelles?\n",
    "* Séparation jeux de données : entraînement / test : modifier les critères de séparation des 2 sets?\n",
    "  * horizon de temps pour le train. Entrainement sur les 10 premiers mois et prédire sur les deux mois suivants.\n",
    "* **Variables qui identifient le comportement d'achat des clients :**\n",
    "    * Pays\n",
    "    * Identifiant de la marchandise achetée\n",
    "    * Volume de la marchandise\n",
    "    * Chiffre d'affaires\n",
    "    * Période d'achat dans l'année\n",
    "* Dataleakage total=unit__price*unit : ce n'est pas du data-leakage car multplication de deux valeurs\n",
    "* Invoice date:  permet de retirer des informations sur le compotement du client.\n",
    "* Classer des le premeir achaat : la frequence ne permet pas de faire une classification pour un nouveau client. Une fois l'achat fait, on classfie le client pour pouvoir fire de la pub en ligne.\n",
    "* Les clients dans cette bas efont très peu d'achat. Verifier par un graphe.\n",
    "* Un client pourrait changer de classe.\n",
    "* Pays autres que UK : certains pays n'ont pas Amazone.\n",
    "* Descriiption: donne des informations sur les articles. Frequence des mots, conjonctions a éliminer. Voire la lib <font color='blue'>**nltk**</font> qui gère du texte. Enlever les mots de la description pour iosseler le produit. Permet de classer les types de produits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import p3_util\n",
    "import p3_util_plot\n",
    "import p5_util\n",
    "import p5_util_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blues'>Acquisition des données</font>\n",
    "\n",
    "On s'est assuré qu'i n'y avait qu'un seul onglet dans le fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = './data/OnlineRetail.xlsx'\n",
    "t0 = time.time()\n",
    "df_invoice = pd.read_excel(fileName)\n",
    "t1 = time.time()\n",
    "print(\"Elapsed time = %1.2f\" %(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice_save = df_invoice.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Data cleaning</font>\n",
    "\n",
    " * Les lignes contenant les customers ID a Nan sont purgées.\n",
    " * Les lignes contenant des variables aberrantes sont purgées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice = df_invoice_save.copy()\n",
    "print(df_invoice.shape)\n",
    "df_invoice.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description des variables quantitatives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drop rows containing Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_before = df_invoice.shape[0]\n",
    "print(df_invoice.shape)\n",
    "df_invoice.dropna(axis=0, how='any', inplace=True)\n",
    "print(df_invoice.shape)\n",
    "rows_after = df_invoice.shape[0]\n",
    "print(\"NUll values : Percent of dropped rows = %1.2F\" %(np.abs(rows_before-rows_after)*100/rows_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drop quantity <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_before = df_invoice.shape[0]\n",
    "df_invoice = df_invoice[(df_invoice['Quantity']>0)]\n",
    "rows_after = df_invoice.shape[0]\n",
    "print(\"CustomerID <= 0 : Percent of dropped rows = %1.2F\" %(np.abs(rows_before-rows_after)*100/rows_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drop CustomerID <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice = df_invoice[df_invoice['CustomerID'] > 0]\n",
    "print(df_invoice.shape)\n",
    "\n",
    "df_invoice = df_invoice[df_invoice['UnitPrice']>0]\n",
    "print(df_invoice.shape)\n",
    "\n",
    "df_invoice = df_invoice[df_invoice['Quantity']>0]\n",
    "print(df_invoice.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verification de l'existance de valeurs nulles dans chacune des colonnes et chacune des lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification sur toutes les lignes (axis=1) puis somme des résultats en colonne (axis=0)\n",
    "df_invoice.isnull().sum(axis=1).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice_save = df_invoice.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Data preparation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice = df_invoice_save.copy()\n",
    "df_invoice.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Distribution des pays dans le dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p3_util_plot\n",
    "dict_to_display = dict()\n",
    "for country in df_invoice.Country.unique() :\n",
    "    dict_to_display[country] = df_invoice[df_invoice['Country']==country].shape[0]\n",
    "\n",
    "\n",
    "title=\"Distribution de l\\'origine des transactions\"\n",
    "xlabel = \"Pays\"\n",
    "ylabel = \"Nb de transactions\"\n",
    "p3_util_plot.df_display_dict_hist(dict_to_display, title, xlabel, ylabel, fontSize=12, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Filtrage des pays les moins représentés**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_countries_keep = ['EIRE','France','Germany','United Kingdom']\n",
    "list_countries_keep = ['United Kingdom']\n",
    "\n",
    "df_invoice_new = pd.DataFrame()\n",
    "for country in list_countries_keep : \n",
    "    df_invoice_new = df_invoice_new.append(df_invoice[df_invoice['Country']==country], ignore_index=True)\n",
    "\n",
    "print(df_invoice_new.shape)\n",
    "rows_after = df_invoice_new.shape[0]\n",
    "print(\"Countries filtering : Percent of dropped rows = %1.2F\" %(np.abs(rows_before-rows_after)*100/rows_before))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice = df_invoice_new.copy()\n",
    "df_invoice_new = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>For any invoice, Total price is added in feature *Total*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Total' in df_invoice.columns :\n",
    "    del(df_invoice['Total'])\n",
    "\n",
    "df_invoice['Total'] = df_invoice.Quantity * df_invoice.UnitPrice\n",
    "df_invoice = df_invoice[df_invoice['Total']>0]\n",
    "df_invoice.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Oultliers in feature *Total*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util_plot\n",
    "\n",
    "p5_util_plot.p5_df_subplot(df_invoice[['Quantity','UnitPrice','Total']],\\\n",
    "                           type_plot='box', is_outliers_removed = True, tuple_reshape=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Purge des outliers total **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p3_util\n",
    "zmin, zmax = p3_util.df_boxplot_min_max(df_invoice , 'Total')\n",
    "\n",
    "print(df_invoice.shape)\n",
    "df_invoice = df_invoice[df_invoice['Total']>=zmin]\n",
    "\n",
    "print(df_invoice.shape)\n",
    "\n",
    "df_invoice = df_invoice[df_invoice['Total']<=zmax]\n",
    "print(df_invoice.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Représentation des variables qualitatives</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>RFM scoring for each customer</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Invoice date are converted into seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice['InvoiceDate'] = df_invoice['InvoiceDate'].apply(p5_util.p5_convert_timestamp_to_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice_save = df_invoice.copy()\n",
    "df_invoice.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice = df_invoice_save.copy()\n",
    "df_invoice.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice.CustomerID.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Values from feature CustomerID are casted into integer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice.CustomerID = df_invoice.CustomerID.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** For each customer, RFM score is computed from followings features: <font color=blue>InvoiceNo, InvoiceDate, Total</font> **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_now = df_invoice.InvoiceDate.max()\n",
    "print(\"Day now = {}\".format(day_now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RFM = pd.DataFrame(df_invoice.groupby('CustomerID').agg({'InvoiceNo': lambda x: len(x)}))\n",
    "df_RFM['Recency'] = df_invoice.groupby('CustomerID').agg({'InvoiceDate': lambda x: (day_now - x.max())})\n",
    "df_RFM['Monatary'] = df_invoice.groupby('CustomerID').agg({'Total': lambda x: x.sum()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RFM.rename(columns={'InvoiceNo':'Frequency'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Scoring Frequency, Monetary and Recency from quantile values limmits **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit of quantile values are computed from \n",
    "df_rfm_quantiles = df_RFM.quantile(q=[0.25,0.5,0.75])\n",
    "\n",
    "df_rfm_quantiles.reset_index(inplace=True)\n",
    "\n",
    "df_rfm_quantiles.rename(index={0:'Q1',1:'Q2',2:'Q3'},inplace=True)\n",
    "\n",
    "del(df_rfm_quantiles['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rfm_quantiles.loc['Q1','Frequency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** For applying same scoring function, <font color='blue'>*Recency*</font> values are applied with opposite sign.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RFM['neg_recency'] = df_RFM['Recency'].apply(lambda x: -x)\n",
    "df_rfm_quantiles['neg_recency'] = df_rfm_quantiles['Recency'].apply(lambda x: -x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification des clients en score RFM\n",
    "df_RFM['R_score'] = df_RFM.neg_recency.apply(p5_util.func_rfm_scoring, args=(df_rfm_quantiles,'neg_recency'))\n",
    "df_RFM['F_score'] = df_RFM.Frequency.apply(p5_util.func_rfm_scoring, args=(df_rfm_quantiles,'Frequency'))\n",
    "df_RFM['M_score'] = df_RFM.Monatary.apply(p5_util.func_rfm_scoring, args=(df_rfm_quantiles,'Monatary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RFM.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Aggregation du scoring FRM **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RFM['RFM'] = df_RFM['R_score'].map(str)+df_RFM['F_score'].map(str)+df_RFM['M_score'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RFM.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RFM_restricted = df_RFM[['CustomerID','RFM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RFM_restricted.sample()\n",
    "df_invoice.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'RFM' in df_invoice.columns :\n",
    "    del(df_invoice['RFM'])\n",
    "df_RFM_restricted.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_invoice, df_RFM_restricted, how='left', on=['CustomerID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Features used for RFM computation are droped; this avoid data-leak**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged[['Description','Quantity','UnitPrice','CustomerID','RFM']]\n",
    "\n",
    "df_merged.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** First analysis : we do not take into account Description; CustomerID is processed as row index **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.index= df_merged.CustomerID\n",
    "df_invoice_1 = df_merged[['Quantity','UnitPrice','RFM']]\n",
    "\n",
    "df_invoice_1.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Customers segmentation issue from clustering</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** One hot encoding du score RFM **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "encoder = preprocessing.OneHotEncoder() \n",
    "try :\n",
    "    encoded_RFM = encoder.fit_transform(df_invoice_1.RFM.values.reshape(-1,1))\n",
    "except ValueError as valueError :\n",
    "    print(\"\\n*** Erreur encodage : {}\".format(valueError))\n",
    "encoded_RFM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Standardisation des données **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# ------------------------------------------------\n",
    "# Quantitative values are extracted from dataframe\n",
    "# ------------------------------------------------\n",
    "X_quantitative_std = df_invoice_1[['Quantity','UnitPrice']].values\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Quantitative values are casted as float\n",
    "# ------------------------------------------------\n",
    "X_quantitative_std = X_quantitative_std.astype(float)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Data scaling\n",
    "# ------------------------------------------------\n",
    "std_scale = preprocessing.StandardScaler().fit(X_quantitative_std)\n",
    "X_quantitative_std = std_scale.transform(X_quantitative_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Aggrégation des données **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "X_std = scipy.sparse.hstack((X_quantitative_std, encoded_RFM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_corr = pd.DataFrame(X_std.toarray()).corr()\n",
    "df_corr.shape\n",
    "_z = sns.heatmap(df_corr, annot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les features obtenues par le score RFM sont décorrélées les unes des autres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Dimension shift </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>PCA Analysis</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p3_util_plot\n",
    "\n",
    "df_std = pd.DataFrame(X_std.toarray())\n",
    "z__ = p3_util_plot.df_pca_all_plot(df_std, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variance du modèle est expliquée par l'ensemble des features. \n",
    "\n",
    "Une réduction de dimension sur la base de l'ACP n'est pas pertinente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Dimension shift with t-SNE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tSNE allows to build separate groups of data points based on probability points of same group \n",
    "belongs to same neighborhood.**\n",
    "\n",
    "**This is also an easy way to apply clustering based on this dimension reduction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sampling of 3000 random data points and 30 data points**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nb_rows = X_std.toarray().shape[0]\n",
    "random_index_sample = np.random.randint(0,nb_rows,30)\n",
    "random_index = np.random.randint(0,nb_rows,3000)\n",
    "\n",
    "list_random_index_sample = list(random_index_sample)\n",
    "list_random_index = list(random_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Verification de l'intersection vide entre la liste des samples et de la liste des 3000 data points **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_intersec = set(list_random_index_sample).intersection(list_random_index)\n",
    "print(\"\\n Intersection : nombre d'éléments dans les des deux listes = {0}\".format(len(set_intersec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sampling arrays are extracted from index list **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_std.toarray().shape)\n",
    "nb_rows = X_std.toarray().shape[0]\n",
    "X_std_sample = X_std.toarray()[random_index]\n",
    "X_std_sample_2 = X_std.toarray()[random_index_sample]\n",
    "print(X_std_sample.shape,X_std_sample_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** t-SNE is applied for dimension shifting **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "my_list_tsne_perplexity=[var for var in range(5,55,5)]\n",
    "dict_tsne_result = p5_util.tsne_2D_process_perplexity(X_std_sample, tsne_iter=3000, list_tsne_perplexity=my_list_tsne_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util_plot\n",
    "nb_col = 4     \n",
    "p5_util_plot.plot_2D_dict_tsne_result(dict_tsne_result, nb_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Data-shift for samples of 30 data points </font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_std_projected_2 = manifold_embedd.fit_transform(X_std_sample_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Clustering over a sample</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Among all tSNE results, compute Kmeans clustering and get cluster scores</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Display all Kmeans scores **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "\n",
    "n_cluster_start = 50\n",
    "n_cluster_end = 100\n",
    "n_cluster_optimum,best_perplexity,score_max = p5_util.scan_best_kmean_cluster_from_tsne_result(dict_tsne_result\\\n",
    "                                                                                               , n_cluster_start\\\n",
    "                                                                                               , n_cluster_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n *** Kmeans : Optimal cluster count= {0} / Silhouette = {1:0.3F} / tSNE Perplexity = {2}\\n\".format(n_cluster_optimum, score_max, best_perplexity))\n",
    "n_cluster_kmean = n_cluster_optimum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Compute and display optimal clustering **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclusters = n_cluster_optimum\n",
    "X_std_projected = dict_tsne_result[best_perplexity]\n",
    "\n",
    "cluster_kmean = KMeans(n_clusters=nclusters).fit(X_std_projected)\n",
    "centers_kmean = cluster_kmean.cluster_centers_\n",
    "preds_kmean_sample = cluster_kmean.predict(X_std_projected)\n",
    "my_title = str(nclusters)+\" Kmeans clusters\"\n",
    "p3_util_plot.clustering_plot(X_std_projected, preds_kmean_sample, nclusters, title=my_title\\\n",
    "                             , X_center=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_title = str(nclusters)+\" Kmeans clusters with extra sampling\"\n",
    "p3_util_plot.clustering_plot(X_std_projected, preds_kmean_sample, nclusters, title=my_title\\\n",
    "                             , X_center=X_std_projected_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "\n",
    "n_cluster_start = 50\n",
    "n_cluster_end = 100\n",
    "n_cluster_gmm_optimum,best_perplexity_gmm,score_max_gmm = p5_util.scan_best_gmm_cluster_from_tsne_result(dict_tsne_result\\\n",
    "                                                                                            , n_cluster_start\\\n",
    "                                                                                            , n_cluster_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n *** GMM : Optimal cluster count= {0} / Silhouette = {1:0.3F} / tSNE Perplexity = {2}\\n\".format(n_cluster_gmm_optimum,score_max_gmm,best_perplexity_gmm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>DBSCAN over sample</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p3_util_plot\n",
    "\n",
    "labels_trained_dbscan, labels_test_dbscan = p3_util_plot.clustering_dbscan_plot_and_metrics(X_std_sample_train\\\n",
    "                                                                                            ,X_std_sample_test\\\n",
    "                                                                                            ,parameter_eps=0.3\\\n",
    "                                                                                            ,parameter_min_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dict_dbscan_scoring = dict()\n",
    "\n",
    "for dbscan_eps in np.logspace(-5, 1, 100).astype(np.float32) :\n",
    "    dbscan = DBSCAN(eps=dbscan_eps, min_samples=5).fit(X_std_sample)\n",
    "    clustering_dbscan = dbscan.fit_predict(X_std_sample)\n",
    "    #dict_dbscan_scoring[] = clustering_dbscan.labels_\n",
    "    #dbscan_nclusters = len(set(clustering_dbscan.labels_)) - (1 if -1 in clustering_dbscan.labels_ else 0)\n",
    "    dbscan_nclusters = len(set(clustering_dbscan)) - (1 if -1 in clustering_dbscan else 0)\n",
    "\n",
    "    print(\"DBSCAN : estimation du nombre de clusters : {0} dbscan_eps = {1:1.6F}\".format(dbscan_nclusters, dbscan_eps))\n",
    "\n",
    "    if dbscan_nclusters <= 1 :\n",
    "        print(\"\\nERROR : DBSCAN cluster bruité!\\n\")\n",
    "    else :\n",
    "        dict_dbscan_scoring[dbscan_nclusters] = silhouette_score(X_std_sample, clustering_dbscan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_max = 0\n",
    "cluster_optimum = 0\n",
    "for n_cluster in dict_dbscan_scoring.keys() :\n",
    "    score = dict_dbscan_scoring[n_cluster]\n",
    "    print (\"DBSCAN : cluster count = {0} / Average silhouette score is : {1:0.3F}\".format(n_cluster, score))\n",
    "    if score > score_max : \n",
    "        score_max = score\n",
    "        n_cluster_optimum = n_cluster\n",
    "\n",
    "print(\"\\n *** Optimal cluster count= {0} / Silhouette = {1:0.3F}\\n\".format(n_cluster_optimum, score_max))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Target variable depending from best clustering process</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dict_tsne_result[best_perplexity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "cluster_kmean = KMeans(n_clusters=n_cluster_kmean).fit(X)\n",
    "\n",
    "# Predict the cluster for each data point\n",
    "preds_kmean = cluster_kmean.predict(X)\n",
    "\n",
    "#Find the cluster centers\n",
    "centers_kmean = cluster_kmean.cluster_centers_\n",
    "print(preds_kmean.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blus'>Classement des items Description</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_save = df_merged.copy()\n",
    "df_invoice_1_save = df_invoice_1.copy()\n",
    "X_std_save = X_std.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice = df_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Using NLTK to create additional features from *Description*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import text\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Punctuation exclude characters '-' and '_'\n",
    "\n",
    "# Each-one of the values from Description feature is cleaned from punctuation \n",
    "#--------------------------------------------------------------------\n",
    "#\n",
    "#--------------------------------------------------------------------\n",
    "def get_my_punctuation():\n",
    "    my_punctuation = str()\n",
    "    list_punctuation_excluded = ['-','_',' ']\n",
    "    for char in string.punctuation:\n",
    "        if char in list_punctuation_excluded:\n",
    "            pass\n",
    "        else:\n",
    "            my_punctuation += char\n",
    "    return my_punctuation\n",
    "#--------------------------------------------------------------------\n",
    "    \n",
    "#--------------------------------------------------------------------\n",
    "#\n",
    "#--------------------------------------------------------------------\n",
    "def cb_remove_punctuation(item, list_char_remove) :\n",
    "    item_no_punctuation = [ char for char in item.lower() if char not in list_char_remove ]\n",
    "    item_no_punctuation = \"\".join(item_no_punctuation)\n",
    "    return item_no_punctuation\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "#\n",
    "#--------------------------------------------------------------------\n",
    "def cb_remove_stopwords(item) :\n",
    "    list_word = item.split()\n",
    "    item_no_stopwords = [ word for word in list_word if word.lower() not in stopwords.words('english') ]\n",
    "    item_no_stopwords = \" \".join(item_no_stopwords)\n",
    "    return item_no_stopwords.upper()\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "#\n",
    "#--------------------------------------------------------------------\n",
    "def cb_stemmer(item, stemmer):\n",
    "    list_word = item.split()\n",
    "    stemmed_item = [ stemmer.stem(word.lower()) for word in list_word ]\n",
    "    stemmed_item = \" \".join(stemmed_item)\n",
    "    return stemmed_item.upper()\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "#\n",
    "#--------------------------------------------------------------------\n",
    "def cb_lemmatizer(item, lemmatizer):\n",
    "    list_item = item.split()\n",
    "    lemmatized_item = [ lemmatizer.lemmatize(word.lower()) for word in list_item ]\n",
    "    lemmatized_item = \" \".join(lemmatized_item)\n",
    "    return lemmatized_item.upper()\n",
    "#--------------------------------------------------------------------\n",
    "    \n",
    "#--------------------------------------------------------------------\n",
    "#\n",
    "#--------------------------------------------------------------------\n",
    "def cb_clean_numeric_word_in_item(item) :\n",
    "    list_word = item.split()\n",
    "    list_cleaned = [ word for word in list_word if not (word.isnumeric())]\n",
    "    new_item = ' '.join(list_cleaned)\n",
    "    return new_item\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "#\n",
    "#--------------------------------------------------------------------\n",
    "def cb_clean_list_word(item, list_no_word):\n",
    "    list_word = item.lower().split()\n",
    "    #list_no_word is converted as lower characters\n",
    "    no_word_lower = ' '.join(list_no_word).lower()\n",
    "    list_no_word = no_word_lower.split()\n",
    "    list_cleaned = [ word for word in list_word if word.lower() not in list_no_word ]\n",
    "    new_item = ' '.join(list_cleaned)\n",
    "    return new_item.upper()\n",
    "#--------------------------------------------------------------------    \n",
    "    \n",
    "    \n",
    "#--------------------------------------------------------------------\n",
    "#\n",
    "#--------------------------------------------------------------------\n",
    "def get_ser_set_len(ser):\n",
    "    return len(set([ item for item in ser ]))\n",
    "#--------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's consider all items into *Description* feature as a corpus.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove any punctuation from any *Description* feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_invoice.Description.iloc[0] +=\",!.&~\"\n",
    "item = df_invoice.Description.iloc[0]\n",
    "print(item)\n",
    "\n",
    "my_punctuation = get_my_punctuation()\n",
    "df_invoice.Description = df_invoice.Description.apply(cb_remove_punctuation,args=(my_punctuation,) )\n",
    "\n",
    "item = df_invoice.Description.iloc[0]\n",
    "print(item)\n",
    "print(get_ser_set_len(df_invoice.Description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We remove stopwords in orde to extract words with most information.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice.Description = df_invoice.Description.apply(cb_remove_stopwords)\n",
    "print(get_ser_set_len(df_invoice.Description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice_save = df_invoice.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice = df_invoice_save.copy()\n",
    "df_invoice.sample(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fdist = text.FreqDist(my_Text)\n",
    "list_most_common = fdist.most_common(1000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list_most_common[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization of *Description* values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df_invoice.Description = df_invoice.Description.apply(cb_lemmatizer,args=(lemmatizer,))\n",
    "print(get_ser_set_len(df_invoice.Description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming of *Description* values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "df_invoice.Description = df_invoice.Description.apply(cb_stemmer,args=(stemmer,))\n",
    "print(get_ser_set_len(df_invoice.Description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean all numeric word from item Description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice.Description = df_invoice.Description.apply(cb_clean_numeric_word_in_item)\n",
    "print(get_ser_set_len(df_invoice.Description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean repetitives keyword from item Description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_ser_set_len(df_invoice.Description))\n",
    "list_no_words=['SET','PACK']\n",
    "df_invoice.Description = df_invoice.Description.apply(cb_clean_list_word, args=(list_no_words,))\n",
    "print(get_ser_set_len(df_invoice.Description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *df_invoice.Description* is regarded as a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer( min_df=1, max_df=.8, ngram_range=(1,1))\n",
    "matrix_weights = vectorizer.fit_transform(df_invoice.Description)\n",
    "print(matrix_weights.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weights = np.asarray(matrix_weights.mean(axis=0))\n",
    "print(weights.shape)\n",
    "\n",
    "#vectorizer.get_feature_names()\n",
    "weights = np.asarray(matrix_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "hstack_matrix = scipy.sparse.hstack((X_std, matrix_weights))\n",
    "hstack_matrix.shape,matrix_weights.shape, encoded_RFM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_cluster=6\n",
    "cluster_kmean = KMeans(n_clusters=n_cluster).fit(hstack_matrix)\n",
    "\n",
    "# Predict the cluster for each data point\n",
    "preds_kmean = cluster_kmean.predict(hstack_matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Calculate the mean silhouette coefficient for the number of clusters chosen\n",
    "print(silhouette_score(hstack_matrix, preds_kmean))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import p5_util\n",
    "n_cluster_start = 3\n",
    "n_cluster_end = 3\n",
    "dict_cluster_scoring = p5_util.kmean_scan_score(hstack_matrix, n_cluster_start, n_cluster_end) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Building *nltk.Text* object</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True :\n",
    "    list_items = list()\n",
    "    for item in df_invoice.Description:\n",
    "        list_items.append(item)\n",
    "\n",
    "if False :\n",
    "    my_Text = text.Text(new_item)\n",
    "else :\n",
    "    my_Text = text.Text(list_items)\n",
    "    print(len(list_items))\n",
    "    set_items = set(list_items)\n",
    "    print(len(set_items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Text = text.Text(df_invoice.Description)\n",
    "fdist = nltk.FreqDist(my_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common()[-10:-1],fdist.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextes d'utilisation d'un mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Text.concordance(\"SET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Text.similar(\"SET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_Text.similar(\"KEEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,15))\n",
    "fdist.plot(10000, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='blue'>Valeurs de la feature *Description* qui contiennent le mot *SET*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "#\n",
    "#----------------------------------------------------\n",
    "def is_word_in_item(item, word) :\n",
    "    if word in item :\n",
    "        return item\n",
    "    else :\n",
    "        return None\n",
    "#----------------------------------------------------\n",
    "#----------------------------------------------------\n",
    "#\n",
    "#----------------------------------------------------\n",
    "def clean_numeric_word_in_item(item) :\n",
    "    list_word = item.split(' ')\n",
    "    for word in list_word:\n",
    "        if word.isnumeric():\n",
    "            list_word.remove(word)\n",
    "        else :\n",
    "            pass\n",
    "    \n",
    "    # Rebuild word from list\n",
    "    new_item = \"\"\n",
    "    for word in list_word :\n",
    "        new_item+=word\n",
    "        new_item+=\" \"\n",
    "    # Remove trailer character\n",
    "    new_item = new_item[:-1] \n",
    "    \n",
    "    return new_item\n",
    "#----------------------------------------------------\n",
    "result_SET = None\n",
    "result_SET = df_invoice.Description.apply(is_word_in_item, args=('MEDIUM',))\n",
    "result_SET = df_invoice.Description.apply(clean_numeric_word_in_item)\n",
    "\n",
    "result_SET.dropna(how='any', inplace=True)\n",
    "\n",
    "result_SET.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Using random Forests for RFM score prediction classification</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification used into previous step is used for targeting model.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample = preds_kmean.copy()\n",
    "hstack_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(hstack_matrix, y_sample, test_size=0.1)\n",
    "rfc = RandomForestClassifier(n_estimators=500)\n",
    "rfc_model = rfc.fit(X_train, y_train)\n",
    "y_pred = rfc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Précision de la prédiction par random forest: {0:1.3F}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Using linear SVC for prediction</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn  import svm\n",
    "svm_model = svm.LinearSVC()\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Précision de la prédiction par Linear SVC: {0:1.3F}\".format(svm_model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Using computed model for prediction </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
