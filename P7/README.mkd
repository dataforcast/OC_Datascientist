# <center><h1>Dogs images classification regarding to breed</h1></center>
This project carried out on behalf of the Master 2 Centrale Sup√©lec / Openclassrooms training in datasience
<hr>

<br>
<h2><font size=5>Abstract</font></h2>
<hr>
<font size=4>
This project presents the realization of an algorithm to detect the breed of a dog from an image.
</font>

<br>
<font size=4>Two methods are presented :</font>

<br>
<pre>
    **1. The Bag of features method applied to Machine Learning algorithms**
                    
        1.a SIFT descriptors are extracted from each image from a train-dataset. All images are represented as a set of these descriptors. 
        
        1.b The same descriptors may have different  numerical representation. For this reason, descriptors are grouped into clusters, using an appropriate cluster algorithm.
        
        1.c Bag of features representation of an image consists in counting, per cluster, occurencies of features belonging to this image.
        
        1.d A machine learning algorithm is trained with this Bag Of Features representation of train-dataset.
        
        1.e Computation model issued from this process allows to predict a breed when an image is submitted to the model.
        
        1.f Performances of a such model is evaluated thanks to an accuracy measurement.
        
        1.g Results are exposed for a range of  Bayesian and not Bayesiens supervized Machine Learning algorithms.
    
</pre>

<br>
<pre>
    **2. Deep Learning algorithms **
    <br>
    

        2.a Multi-layer Perceptron network is built and trained using Keras API.
        
        2.b Convolutional Neural Network is built and trained using Keras API.
        
        2.c Pre-trained VGG16 neural network is used in order to predict breed from a Dog Image.
        
        2.d Transfer-learning method is applied to VGG16 pre-trained neural network, freezing the learn of convolutional layers.
        
        2.e Transfer-learning method is applied to VGG16 pre-trained neural network, forcing the learn of convolutional layers.

</pre>

<br><br><br>
<h2>Project artefacts</h2>
<hr>

<br>

* <font size=4>Project artefacts are located under <a href="URL">http://bit.ly/FBangui_Datascience_CNN</font></a>
<br>

* <font size=4>These slides present the overall approach of the study : <a href="URL">
        OC_Datascientist/P7/report/report/Openclassrooms_ParcoursDatascientist_P7-V1.pdf</font></a>
<br>

* <font size=4>Jupyter notebook : <a href="URL">http://bit.ly/CNN_JupyterNotebook</font></a>
<br>

* <font size=4>Python source code : <a href="URL">http://bit.ly/CNN_Python</font></a>
<br>

<br>

<font size=4>Description of tasks to be accomplished along with asumptions are schematized below :</font> 

<center><img src="./report/P7_TasksDescription.png" alt="Drawing" style="width: 200px;"/></center>
<br>
<font size=4>2 classifiers types are benchmarked : Machine Learning classifiers and Deep Learning classifiers.</font>
<br>
<font size=4>Exploratory analysis will lead to define the M.L. parameters and to pose the asumptions.</font>

<br>
<font size=4>Track of this analysis has been recorded in the notebooks *FilterDescription.ipynb* and *ClusteringDedscriptors.ipynb*.</font>
<br>
<font size=4>Scheme below describes software architecture, designed for both parametrization and deployment.</font>


<font size=4>A model may be able to be rebuilt and deployed in a flexible way.E.G new breed may be able to be added in the data model as well as new dogs images from an existing breed.</font>
<br>

<font size=4>Also, it might be possible to add a new clasifier in the model while using existing ones. It should also be possible to run multipme classifiers for same image and select the result based on all results provided by differents classifiers.</font>
<br>

<font size=4>Software implementation of classifiers presented on scheme below allow to achieve such flexibility.</font>
<br>

<center><img src="./report/P7_SoftwareArchitecture.png" alt="Drawing" style="width: 200px;"/></center>

  * <font size=4>*P7_DataBreed* is a data model builder. This is a Python class in which attributes handle classifiers model parameters. Such parameters are issued from data exploratory analysis task.</font>
 
  * <font size=4>*User* is allowed to fixe models parameters in order to control and test classifiers behavior. </font>

  * <font size=4>Utilities files, *p3_util, p5_util, p5_util* are python sources files used by *P7_DataBreed*. These files implement functions issued from other previous projects. </font>

  * <font size=4>*File System* is used by *P7_DataBreed* in order to access images to be processed. Due to ressources limitations, images are loaded and processed one per one. *P7_DataBreed* may be upgraded in order to support many other data sources, such as databases or Blockchains.</font>

  * <font size=4>*P7_DataBreed* handles two classes of classifiers : Artificial neuron networks (ANN) and machine learning classfiers (ML).</font>
  
      *  <font size=4>Machine learnings classifiers support a data model based on SIFT features extraction.</font>
      *  <font size=4>Artificial neurons network classifiers are of two types : Multi layer perceptron (MLP) and Convolutional network layers (CNN).</font>

<br>
<h2>Problem type</h2>
<hr>
<font size=4>The problem posed is formulated as that of classifying dogs according to their breed. Due to the fact that a dog belongs to a unique breed, this problem is typed as a multi-class classification.</font>

<center><img src="./report/P7_PbType.png" alt="Drawing" style="width: 200px;"/></center>

<br>
<font size=4>Picture belows shows a sample of images bank. Images have differents background, may have more then one subject per image.</font>

<center><img src="./report/P7_ImagesSample.png" alt="Drawing" style="width: 200px;"/></center>

<br>
<font size=4>Dogs are represented according to different points of view, profile, face, from near distance ...</font>

<br>
<h2>1. Using machine learning algorithms for classification<h2>
<hr>

<h3><font size=4>1.1 Exploratory analysis</font></h3>
<br>

  * <font size=4> This step is conducted in order to dicsover best filters allowing efficient features extraction. Filters are discrete convolutional kernels applied to images pixels.
</font>
<br>
  * <font size=4> Aim of applying filters is to extract SIFT descriptors in the most efficient way.
</font>
<br>

* <font size=4>Images are resized in order all images from bank to have the same shape. Then they are convert into greys scales. Once done, a set of filters is applied. Plot below shows the size distribution for the bank of images.</font>



<center><img src="./report/P7_Filters1.png" alt="Drawing" style="width: 200px;"/></center>

* <font size=4>Filtering process is exposed on sheme below. Images are resized in order all images from bank to have the same shape. Then they are convert into grey scales. Once done, a set of combination of filters is applied. Multiple combinations have been explored and some results are displayed on the figure below.
</font>

<center><img src="./report/P7_FilterCombination.png" alt="Drawing" style="width: 200px;"/></center>

* <font size=4> Filters are applied with followings expectations: </font>
    * <font size=3>*Gaussian* filter is applied in order to render  background more smooth.
    </font>
    * <font size=3>*Median* filter allows to remove pixels irregularities.</font> 
    * <font size=3>*Edgeonly* filter allows to highlight the pixels forming the edge of an object. </font> 



<br>
<h3><font size=4>1.2 Features extraction with SIFT algorithm</font></h3>
<br>
<font size=4>SIFT algorithm is used to extract features from an image. This algorithm leads to identify key points (KP), each of them forming a vector in 128 dimensions.</font> 

<br>
<font size=4>Components of KP over the 128 dimensions are <u>KP descriptors</u>.</font> 

<br>
<font size=4>KP are points over images that have invariant properties considering scaling and geometric transformations. Then it is a natural way to think that similar key points will characterize similat breeds and, by the way, characterize a breed.</font>
<br>

<font size=4>Result of such process using combination of {*Grey, Median, Edge*} filters is showned below : </font>


<center><img src="./report/P7_FeaturesExtraction1.png" alt="Drawing" style="width: 200px;"/></center>
<br>
<font size=4>It can be observe that : </font>

* <font size=4>Some areas expose a high density of key points, some ot them located out of targeted objects, mean, dogs.</font>
* <font size=4>Also, some areas expose a low density of key points, some ot them located out of targeted objects.</font>
* <font size=4>The picture above also shows how the key points of two dogs of the same breed can match each other.</font>

<br>
<font size=4>Filtering those low and high density areas may lead to keep more efficiently expected key points. For doing so, image is splitted into equal parts. Filter criteria is fixed based on Q1 and Q3 quantile of KP empiric distribution.</font>
<br>
<center><img src="./report/P7_KeyPointDistribution.png" alt="Drawing" style="width: 200px;"/></center>

<br>
<font size=4>This KP filtering process shows below an example of matching KP between two dogs of the same breed. </font>
<br>
<center><img src="./report/P7_KeyPointFilterMatching.png" alt="Drawing" style="width: 200px;"/></center>

<br>
<font size=4>Most of the key points from issued from low and high densities areas have been removed. Key points on edges of the two dogs are more likely to match each-other.</font>
<br>
<br>

<font size=4>This process, however, suffers from a disadvantage. Many key points are inside the target object. Key points within the target object are not relevant to dog characterization. They can come from different directions of lighting.</font>

<br>
<font size=4>A simplifying hypothesis is posed: <u>dogs are characterized by their shape</u>. Due to this hypothesis, pixels filtering process should lead to key points to be located over dog edges.</font>


<br>
<font size=4>Picture below shows the combination of applied filters (on the left side of the picture) that leads to extract key points with SIFT algorithm (upper side of the picture) and how  key points on edges of the two dogs are more likely to match each-other.</font>

<br>
<center><img src="./report/P7_KeyPointFilterMatching2.png" alt="Drawing" style="width: 200px;"/></center>




<br>
<br>
<h3><font size=4> 1.3 Dimensions reduction  & data visualization</font></h3>

<br>
<font size=4>
The use of PCA shows, on the plot below, that 90% of the variance is captured with 60 first dimensions over 128 dimensions.
</font>

<br>
<font size=4>
This means that results will not be significantly changed if the dimension of KP matrix is reduced to more than 50%.
this information can be useful to exploit if the memory resources are a critical point.</font>
<center><img src="./report/P7_KeyPointPCA.png" alt="Drawing" style="width: 200px;"/></center>

<br>

<h3><font size=4> 1.4 Bag of features using clusters</font></h3>
<font size=4>
It has been noted previously that different KPs may have different numerical representations (so differents descriptors) while being similars in terms of representing an image characteristic. Similarity fit well with clustering.</font>
<font>

<br>
<font size=4>
KP descriptors are clusterized. Each cluster has a centroid (in case of K-means) or parameters that represents the center of the cluster (GMM). This center is regarded as a feature for any image.
<font>

<br>
<font size=4>
Algorithm for transforming an image into a Bag Of features is as following :
<font>

* <font size=4>For any image, SIFT KP along with their descriptors are extracted.</font>
* <font size=4>Once extracted, any descriptor from this image is assigned a cluster.</font>
* <font size=4>For image, an histogram of clusters is built increasing the count number for any assigned cluster.</font>
* <font size=4>At the end, Bag Of Features for any image is represented as an historgram of clusters.</font>

<br>
<font size=4>
Figure below shows result with the used of *Gaussian Mixture Model (GMM)* clustering over descriptors issued from a trained dataset.
</font>

<center><img src="./report/P7_GMMClustering.png" style="width: 200px;"/></center>
<br>
<font size=4>
Applying  GMM clustering with hyper-parameters ranging from 2 to 20 clusters and with an exhaustive search across covariance type = *{full, diag, tied, spherical}* provides the plot below.
</font>
 
<br>
<font size=4>
The elbow criterion used in conjunction with the silhouette coefficient leads to select 3 clusters for the Bag Of Features.
</font>

<br>
<h3><font size=4> 1.5 Machine Learing model building oberview </font></h3>
<font size=4>
Scheme below provides the global process overview applied in order to benchmark Machine Learning (M.L.) algorithms.
</font>

<center><img src="./report/P7_MLBuilding.png" style="width: 200px;"/></center>
<br>
<font size=4>The steps of the overall process break down as follows:</font>
<pr>
<br>
> <font size=4>A) Filters (also named convolution kernels) are applied over all 460 images and SIFT descriptors are extracted. The result is a matrix of `31K` rows and `128` columns.<font >

> <font size=4>B) Using GMM algorithm, each image is expanded as a Bag Of Features. This results in a array having `460` rows and `3` colmns. <font >

> <font size=4>C) Dataset is splitted over 368 points for training and 93 points for testing. <font >


> <font size=4>D) 4 types of M.L. algorithms are benchmarked : Random Forest, SVM, Gaussian Naive Bayse, Bernouilli Naive Bayse. Train dataset (*X_train, y_train*) is used to train those algorithms. This leads to 4 trained M.L. algorithms.

> <font size=4>E) All types of M.L. algorithms are tested using (*X_test, y_test*) dataset. Results are  then exposed

<br>

</pr>
<h3><font size=4> 1.6 Machine Learning benchmarks results vs binary classification</font></h4>
<font size=4>
<center><img src="./report/P7_MLBinary.png" style="width: 200px;"/></center>

<font size=4> Breeds for test have been selected in order dogs to have shape not too fare from each-other.</font></h4>
<font size=4>
<center><img src="./report/P7_MLBinaryResult.png" style="width: 200px;"/></center>

<font size=4>
With 2 classes, performances show better results then a ramdom classifier, that should provide an average accuracy of 50%. Linear SVC (support vector machine classifier with linear kernel) provides the best results.
</font>

<br>
<h3><font size=4> 1.7 Machine Learning benchmarks vs multi-classes classification</font></h3>
<font size=4>
</font>
<center><img src="./report/P7_MLMulticlass.png" style="width: 200px;"/></center>
<center><img src="./report/P7_MLMulticlassResult.png" style="width: 200px;"/></center>
<br>

<font size=4>
With more then 2 classes, performances drop dramaticaly. Random Forest shows the best score. Nevertheless, Random Forest does not performs better then a random classifier that will provide an average of 33% results.
</font>
<br>
<font size=4>

<br>
<h3><font size=4> 1.8 Why such model does not work so well?</font></h3>
<font size=4>In order to have an intuitive idea of KP representation, t-SNE as well as ISOMAP have been applied and 2D as well as 3D dimensions (for t-SNE) results have been plotted below.</font>

<br>
<font size=4>In the higher space, t-SNE defines similar points as belonging to the same neighborhood. Such neighborhood is defined thanks to normal distributions. Same considerations is done with lower space. To preserve distributions in both spaces, divergence in between distributions is minimized.</font>

<br>
<font size=4>
"Distance" used between the two distributions is the divergence of `Kullback Leibler`.
</font>


<br>
<center><img src="./report/P7_KeyPoint2D3D.png" alt="Drawing" style="width: 200px;"/></center>

<font size=4>For both algorithms, result shows that 2D data are easily separable with a straight line. 
</font>
<br>
<font size=4>It can be deduced that, in a plan, classification of Key Points is not complexe, very simple. This mean a bias value that is significantly high while variance value is low. In other words, this model with high bias and low variance is not able to keep into account the complexity of this classification problem.
<font>

<br>
<font size=4>
This high estimation of the bias is mainly due to hypothesis that have been formulated : 
<font>
<br>

  * <font size=4>dogs color have not been taken in account. Grey scale transformation may have lead of loss of relevant informations required to classsify more then 2 breeds.</font>
  * <font size=4>dogs are characterized with their shape only, along with, ket points are stuck.</font>

<br>


<br>
<font size=4>This poor results may come also from additional sources : </font>

*   <font size=4>Descriptors issued from SIFT KP extraction do not characterize well enough dogs images. This method may be efficient with images containing simple and repetitive geometric patterns. </font>
*   <font size=4>Convolutional filters that has been applied may have remove informations required to efficently classify more then 2 breeds.</font>

</font>
<br>
<h3><font size=4> 1.9 Should we expect better results with other options related to Key Points issued from SIFT?</font></h3>
<font size=4>This is a relevant question, considering that a combination of filters leaving Key Points not only on dogs edges, but also inside the edge area may feed model with more relevant information.</font>
<br>
<font size=4>Combination of filters is shown on the picture below leading to the expected results, in terms of SIFT key points distribution.</font>
<br>
<center><img src="./report/P7_KeyPointDistribution.png" alt="Drawing" style="width: 200px;"/></center>

<font size=4>Results on graph below, <u>for binary classification</u>, shows that accuracy is not improved significantly, despite the fact the total number number of Key Points has been increased from 15K units (key points located on edge only) to more then 25K units.</font>

<br>
<center><img src="./report/P7_MLBinaryResult2.png" alt="Drawing" style="width: 200px;"/></center>

**What about Key points filtering while splitting images?**
<font size=4>Results on graph below, <u>for binary classification</u>, shows that accuracy has not increased. The total number number of Key Points has been decreased to 8K units, half value of the reference model (15K). The number of images, due to splitting, has jumped to 1231, against 400 images for the reference model.</font>
<br>
<center><img src="./report/P7_MLBinaryResult3.png" alt="Drawing" style="width: 200px;"/></center>


<font size=4>**The results of this benchmark lead us to explore other ways to obtain better classifiers.** </font>
<font size=4>This is the purposes of the next part of this study. </font>

<br><br><br>
<br>
<h2>2. Using Neural Networks (NN) architectures for classification</h2>
<hr>

<font size=4>4 types of architectures are considered this part : </font>
<br>


  * <font size=4>Multi-Layers Perceptron (MLP) </font>
  * <font size=4>A convolutional neural network (CNN) built with Keras </font>
  * <font size=4>A partial tuned pre-trained convolutional neural network </font>
  * <font size=4>A fine tuned tuned pre-trained convolutional neural network </font>

<br>
<h3>2.1 Multi-layers Perceptron (MLP)</h3>

<h3>2.1.1 Brief description of MLP architecture</h3>
<br>
<font size=4>MLP is a kind of neural network architecture. Most of features described for MLP are also available for other neural networks architectures. It will be focused on differences when necessary.</font>

<br>
<font size=4>Layers are structures containings a set of neurons drawned as N_1,N_2 on picture below. First layer is input the input layer and last one is the output layer, the place where results come out. For the classification problem it is faced here, each neuron in last layer is made responsible of a dog breed while each neuron in input layer is assigned with 1 pixel from the image feeding this layer.</font>

<br>
<font size=4>Such architecture is named *feed-forward* because any output from a layer is feeding inputs of the next layer <u>and of the next layer only</u>.   </font>

<br>
<font size=4>For MLP, layers in-between are fully connected layers, means, considering two consecutives layers, any neuron from previous layer are connected to any neuron of next layer.</font>

<center><img src="./report/P7_NN_MLP.png" style="width: 200px;"/></center>

<br>
<font size=4>A neuron, a perceptron (2nd neuron of the 2nd layer), is described on the lower part of the figure above. A neuron receives input values *X* from any neuron from the input layer and, using weights values *w* related to any neuron-to-neuron connection, it computes a linear combination of weights, input values and a biases, named *b*. Bias is a variable value assigned to any neuron.  Result of this linear combination is then composed with a  function, the *Activation Function*, named Sigma and drawned with the greek symbol on the figure above.</font>
 
<br>
<font size=4>Historicaly, activation function inside a neuron named perceptron, is a threshold function. Activation function used for the benchmark is the *Softmax* function. <u>By abuse of language, this neuron is still named Perceptron</u>.</font>


<br>
<font size=4>The goal of such architecture is to find proper values of weights and biases in every neurons in a such way results in last layer produce a distribution of expected breeds classification. Weights and bias are computed into each neuron in a such way that, <u>step after step</u>, a cost function, written as a metric between the true results and the expected results, becomes lowest as possible. </font>

<br>
<font size=4>For achieving that, cost function require to have a particular shape and also, weights and bias are expressed in a relation with the gradient of the cost function. </font>


<br>
<font size=4>It's all about the algorithms used : *gradient descent* and *back propagation* algorithm shown on picture below. </font>
<br>
<center><img src="./report/P7_NN_BackPropagation.png" style="width: 200px;"/></center>

<br>

  * <font size=4>Last equation 4) says how to update weights and biases once gradient of the cost function is known. Gradient is noted with nabla greek symbol. </font>

  * <font size=4>Equation 3) tell us how to compute gradient of the cost function, once error over X is computed </font>

  * <font size=4>Equation 2) tell us how to compute error over X in the current layer when error over X is known on next layer. This is the sens of expression *back propagation*. When X values are propagated from lower layers toward upper layers, errors are back-propagated from upper layer to lower layer in order to, step by step, decrease the cost function.</font>

  * <font size=4>Endly, equation 1) tell us how to compute error over X in the last layer. This is the point where back-propagation starts.</font>

<font size=4>An expression of the cost function Cost(A) is provided in 1). For multi-classification models, cross-entropy function is used where `p` is the distribution when all images are properly classified, A the distribution issued to the prediction results.</font>

<br>

<h3>2.1.2 Multi-layers Perceptron results</h3>
<br>
<font size=4>3 (denses) layers havee been used for MLP for a total of 77 335 043 parmeters. </font> 
<font size=4>On the loss curves, loss function from train dataset curve decrease weirdly while loss function from test dataset remains "weirdly" stable after 10 epochs.</font>
<center><img src="./report/P7_MLP_Result1.png" style="width: 200px;"/></center>

<br>
<font size=4>Behaviour of accuracy curves reflect the loss curves behaviors. With train dataset, MLP is able to learn until reach of nearly 90% of properly classified images.</font> 
 
<br>
<font size=4>With inputs from test dataset, accuracy does not increases after 10 epochs. With weights and biases values issued from frain dataset, MLP is not able to generalize its behaviour with new inputs. This behavior is named as <u>over-fitting issue</u>.</font>

<br>
<h3>2.1.3 Regularized Multi-layers Perceptron results</h3>

<br>
<font size=4>Over-fitting issue is the insight of too much complexity of classifiers. It may be addressed with regularization. L2 regularization leads to decrease weights values into each neuron before gradient descent update. Randomly drop of neurons is another way to decrease the neuron network  complexity.</font>

<center><img src="./report/P7_MLP_Result2.png" style="width: 200px;"/></center>

<br>
<font size=4>On first raw on the plot above, loss curves and accuracy curves both issued from train and test dataset inputs show that loss function decrease more smoothly while accuracy is not improved. Over-fit problem is not mitigated. </font>

<br>
<font size=4>On second raw on the plot above, drop of 50% neurons lead to reduce over-fitting problem.
As a counterpart, learning tends to slow down. </font>

<br>
<h3>2.2 Convolutional Neurons Network (CNN)</h3>

<h3>2.2.1 A brief description of CNN architecture</h3>
<br>
<font size=4>Convolutional Neurons Networks is another type of neuron networks specially used for text processing or images classifications.</font>

<br>
<font size=4>Most of the concepts exposed for MLP, such as activation functions, loss functions, gradient descent, backpropagaation are still available.</font>

<br>
<font size=4>As for  MLP, such networks holds with an input layer and an output layers with the same role as MLP first and last layers. Main differences come from hidden layers, as shown on scheme below.</font>


<center><img src="./report/P7_CNN_Architecture1.png" style="width: 200px;"/></center>

<br>
<font size=4>Hidden layers are sub-divised in convolutional layers and fully connected layers. </font>

  * <font size=4>*Convolutional layers* are in charge to create features just like we did with filters for SIFT keypoints extraction. </font>
  * <font size=4>*Dense layers* are in charge to classify inputs features coming from convolutional layers, just like last layer from MLP does. Dense layers are named so because all neurons from a layer are connected with all other neurons in adjacent layers. Such layers are "costly" considering parameters required to define them.</font>

  * <font size=4>In between convolutional layers, *pooling layers* are inserted. Those layers have the ability to operate a spatial reduction of representation of the pixels. It "summarizes" informations assigned to an area of the image, rendering representation of these areas more "abstract".</font>

<font size=4>More details are given over convolutional layers below.</font>

<center><img src="./report/P7_CNN_Architecture2.png" style="width: 200px;"/></center>
<br>
<font size=4>As per MLP, one neuron into input layer manages the luminous intensity of one pixel from an input image.</font>

<br>
<font size=4>On the scheme above, first convolutional layer manages 64 convolutional kernels, one convolutional kernel per plan. Activity of first plan is detailed, represented with three neurons. Each neuron of this plan is connected to some neurons in the input layer, mean, connected with a spatial area of the input image. Weights and biases in any neuron of the same plan have same values. <u>Weights in a plan are values of a convolutional kernel</u>. 
<br>

<font size=4>This lead to the fact that any plan from a convolutional layer extract the same unique feature in an image. It results that neurons in a plan hold a map of the same features. Such plans iare also named *feature maps*</font>

<font size=4>Assigning 64 convolutional kernels (mean 64 filters) in the first convolutional layer allow to extract from an image 64 different characteristics.</font>

<br>
<font size=4>Backpropagation algorithm along with gradeint descent algorithm leads to compute, step by step, proper filters (convolutional kernels) in order to extract features from an image.</font>

<br>
<font size=4>Defining more filters after a pool reduction leads to extract features mapped on specific and biggest areas, issued from a combination of smaller detailed features. This is all about building abstraction of features. </font>

<br>
<font size=4>Once features have been extracted in a sufficient abstract manner, then dense layers are in charge to properly classify combinations of abstrcat dogs features in order to match with a breed. </font>


<h3>2.2.2 Results from a built CNN architecture</h3>
<br>
<font size=4>Keras is a framework, an abstraction layer of functionalities built over tensorflow, allowing to build neural networks (as well CNN) and also to download and use pre-trained neuron networks.</font>

<center><img src="./report/P7_CNN_Result1.png" style="width: 200px;"/></center>

<font size=4>Built CNN is a 8 layers network with more then 18 millions parameters.</font>

<br>
<font size=4>Network overfit comparing accuracy performances between inputs from trained dataset and test dataset.</font>

<center><img src="./report/P7_CNN_Result2.png" style="width: 200px;"/></center>

<font size=4>Results above shows effect of regularization while droping 50% neurins in the classification layer. Loss function decreases more dynamicaly as well as accuracy improves and over-fiting is improved.</font>
<br>

<h3>2.2.3 Results from a pre-trained CNN architecture</h3>
<br>
<font size=4>VGG-16 network has been used in order to test this network performance against datatset of 3 breeds we've used in this study. VGG-16 has been trained over 1000 breeds of dogs. This 16 layers network has been used "off the shelve". Each image from dataset has been submitted to the network and classification result has been picked.</font>
<br>
<center><img src="./report/P7_VGG16_OffShelveResult.png" style="width: 200px;"/></center>

<font size=4>Results on the picture above shows that accuracy depends on breeds. This network is defined with more then 138 millions parameters, 8 times more parameters then built CNN.</font>

<h3>2.2.4 Results from a pre-trained CNN architecture with partial tuning</h3>
<br>
<font size=4>It has been investigated if, using transfer learning, results may be improved, keeping convolutional layers and replacing classification layers. Convolutional layers have weights and biases already trained over 1000 breeds.</font>
<br>
<center><img src="./report/P7_VGG16_PartialTuningResult.png" style="width: 200px;"/></center>
<br>
<font size=4>Doing so, without any speficic action over this network, accuracy is not improved considering previous results.</font>

<h3>2.2.5 Results from a pre-trained CNN architecture with fine tuning</h3>
<br>
<font size=4>It has been investigated if, using transfer learning, results may be improved, keeping convolutional layers and replacing classification layers. Weights and biaises from convolutional layers have been used with their intial value and continued to be trained with trained dataset. Classification layers from VGG16 have been replaced and trained. This results with a CNN of more then 27 millions parameters.</font>
<br>
<center><img src="./report/P7_VGG16_FineTuningResult.png" style="width: 200px;"/></center>
<br>
<font size=4>Results shows while re-training all weights and biases from network, lead to the best results gained with VGG-16 pretrained network.</font>


<h2>3. Benchmark global results</h2>

<font size=4>All results have been compouned in a single graph, showned below.</font>
<br>

<center><img src="./report/P7_AllResult.png" style="width: 200px;"/></center>
<font size=4>This benchamrk has been performed on a computer with 16GB RAM and 8 cores Intel core i7. This resources have limited the number of breeds to be used for benchmarking the dataset.</font>

<br>
<font size=4>There is a gap between M.L. classifiers based on SIFT features and neural networks. With such data model, M.L. classifiers are not able to perform correctly with more then 2 breeds.</font>

<br>
<font size=4>M.L with such data-model may be considered for binary classification. Such data-model uses very few ressources, less then 1500 parameters, for an accuracy of around 60%, considering VGG16-pretrained model with closed to 140 millions parameters. </font>

<br>
<font size=4>Classifiers based on artificial neural networks may be still improved and should be benchmarked against more breeds and so, more dogs images. As exposed earlier, the built CNN network with 6 convolutional layers and 2 full connected layers hold more then 18 millions parameters while the whole train dataset represents 5 millions pixels. In such conditions, network captures noise from images during training set then, is not able generalize with new images from test dataset. </font>

<br>
<font size=4>Beyond techniques such as the L2 regularization of the cost function or the random drop of neurons, the use of more data, such as the number of dog images per breece, or the data augmentation, is necessary to increase network performances. To do this, more resources such as GPU and memory capabilities are required. To do this, cloud computing is a relevant option.</font>





