{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import P7_DataBreed\n",
    "import p5_util\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "from  sklearn import model_selection\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "filename= './data/df_breed_image.dump'\n",
    "if False :\n",
    "    df = p5_util.object_load(filename)\n",
    "    df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df['image'] = df['image'].apply(lambda ser : ser.resize((200,200)))\n",
    "filename= './data/df_breed_image.dump'\n",
    "p5_util.object_dump(df ,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "if True :\n",
    "    oP7_DataBreed = p5_util.object_load('./data/oP7_DataBreed.dump')\n",
    "    oP7_DataBreed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a sample dataset :\n",
    "\n",
    "Sampling is leaded by 2 parameters :\n",
    "* `breed_count` : number of breeds into the dataset\n",
    "* `image_per_breed_count` : for each breed, number of image into sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import P7_DataBreed\n",
    "\n",
    "oP7_DataBreed=P7_DataBreed.P7_DataBreed('./data/Images')\n",
    "oP7_DataBreed.std_size=None\n",
    "oP7_DataBreed.is_squarred=False\n",
    "oP7_DataBreed.is_kp_filtered=False\n",
    "is_splitted=True\n",
    "oP7_DataBreed.split_ratio=(3,3)\n",
    "oP7_DataBreed.load()\n",
    "breed_count=3\n",
    "image_per_breed_count=100\n",
    "list_breed_sample=['n02107142-Doberman','n02115641-dingo','n02113978-Mexican_hairless']\n",
    "#list_breed_sample=['n02107142-Doberman']\n",
    "oP7_DataBreed.list_breed_sample = list_breed_sample\n",
    "oP7_DataBreed.sampling(breed_count, image_per_breed_count)\n",
    "\n",
    "oP7_DataBreed.show()\n",
    "oP7_DataBreed.show_breed_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute following cell in order to update object version with no change in attributes**\n",
    "\n",
    "* A backup instance of object is created. This backup instance is empty.\n",
    "* If object instance is not defined then it is read from dmped file.\n",
    "* object instance is copied back into backup instance\n",
    "* Backup instance is deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import P7_DataBreed\n",
    "if True :\n",
    "    oP7_DataBreed = P7_DataBreed.update_object_save(oP7_DataBreed, is_saved=False,is_new_attribute=True)\n",
    "    oP7_DataBreed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Data-model**\n",
    "\n",
    "DataFrame with columns as images, breed names and labels is built.\n",
    "\n",
    "Labels are issued from encoding breed names. \n",
    "\n",
    "Labels are then used for training Keras network. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = oP7_DataBreed.df_build()\n",
    "df.label.unique(),df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataframe is saved**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "\n",
    "filename= './data/df_breed_image.dump'\n",
    "p5_util.object_dump(df,filename)\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image truncated against image resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_pil_image = df['image']\n",
    "ser_label = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original image sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image = ser_pil_image.iloc[0]\n",
    "print(pil_image.size)\n",
    "pil_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resized image\n",
    "\n",
    "Resize does not lead to loss of information due to truncation.\n",
    "<br>\n",
    "Resizing allows to consume less resources into CNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "pil_image_square = pil_image.resize((224,224))\n",
    "pil_image_square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncated image as square"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import P7_DataBreed\n",
    "square=(204,204)\n",
    "pil_image_truncated = P7_DataBreed.pil_square(ser_pil_image.iloc[0], square=square)\n",
    "print(np.array(pil_image_truncated).shape)\n",
    "#arr_keras_X = arr_keras_X.reshape(1,arr_keras_X.shape[0],arr_keras_X.shape[1],arr_keras_X.shape[2])\n",
    "pil_image_truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building train and test sample, Keras compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square = None\n",
    "resize = (224,224)\n",
    "test_size=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import P7_DataBreed\n",
    "\n",
    "arr_keras_image_train, arr_keras_image_test, arr_label_train, arr_label_test \\\n",
    "= P7_DataBreed.p7_keras_X_train_test_build(ser_pil_image, ser_label, test_size=test_size, square=square, resize=resize)\n",
    "\n",
    "arr_keras_image_train.shape, arr_keras_image_test.shape, arr_label_train.shape, arr_label_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dump train and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "filename='./data/arr_keras_X_y_train_test.dump'\n",
    "p5_util.object_dump((arr_keras_image_train, arr_keras_image_test, arr_label_train, arr_label_test),filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Sequential VGG16 network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read dumped file for train and test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import p5_util\n",
    "filename='./data/arr_keras_X_y_train_test.dump'\n",
    "(arr_X_train,arr_X_test, arr_y_train, arr_y_test) = p5_util.object_load(filename)\n",
    "print(arr_X_train.shape,arr_X_test.shape,arr_y_train.shape,arr_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building VGG16 Keras layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st convolution layer is added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "my_VGG16 = Sequential()  # Création d'un réseau de neurones vide \n",
    "\n",
    "# Ajout de la première couche de convolution, suivie d'une couche ReLU\n",
    "w=arr_X_train.shape[1]\n",
    "h=arr_X_train.shape[2]\n",
    "c=arr_X_train.shape[3]\n",
    "print(w,h,c)\n",
    "my_VGG16.add(Conv2D(64, (3, 3), input_shape=(w, h, c), padding='same', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_VGG16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd convolution layer is added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout de la deuxième couche de convolution, suivie  d'une couche ReLU\n",
    "my_VGG16.add(Conv2D(64, (3, 3), padding='same', activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling layer is added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout de la première couche de pooling\n",
    "my_VGG16.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_labels = arr_y_train.shape[0]\n",
    "print(\"Number of labels for training process : \"+str(nb_labels))\n",
    "my_VGG16.add(Flatten())  # Conversion des matrices 3D en vecteur 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st Dense layer is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_VGG16.add(Dense(nb_labels, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd Dense layer is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout de la deuxième couche fully-connected, suivie d'une couche ReLU\n",
    "my_VGG16.add(Dense(nb_labels, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3th Dense layer is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Ajout de la dernière couche fully-connected qui permet de classifier\n",
    "my_VGG16.add(Dense(nb_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16 compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "#my_VGG16.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "my_VGG16.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Sequential model is dumped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "p5_util.object_dump(my_VGG16,'./data/VGG16Seq.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_VGG16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading VGG166 Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/VGG16Seq.dump\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 802816)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 270)               216760590 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 270)               73170     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 270)               73170     \n",
      "=================================================================\n",
      "Total params: 216,945,650\n",
      "Trainable params: 216,945,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "if True :\n",
    "    VGG16Seq = p5_util.object_load('./data/VGG16Seq.dump')\n",
    "    VGG16Seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/oP7_DataBreed.dump\n",
      "\n",
      " \n",
      "Path to data directory ........ : ./data/Images\n",
      "Number of original breeds ..... : 120\n",
      "Total number of images ........ : 20580\n",
      "Standard images size .......... : None\n",
      "SIFT Image descriptors count .. : 0\n",
      "Sampling : breeds count ....... : 3\n",
      "Sampling : images per breed ... : 100\n",
      "X train size .................. : 0\n",
      "y train size .................. : 0\n",
      "X test size ................... : 0\n",
      "y test size ................... : 0\n",
      "Clusters models  .............. : dict_keys([])\n",
      "Current cluster model  ........ : \n",
      "Bag of features dataframe ..... : (0, 0)\n",
      "Encoded labels from dataset ... : ()\n",
      "Number of breeds in sample .... : 3\n",
      "Image splitted ................ : False\n",
      "Key point descriptors ......... : (128,)\n",
      "Classifier name ............... : \n",
      "Supported classifiers ......... : []\n",
      "Number of restricted images ... : 0\n",
      "Splitted parts ................ : (3, 3)\n",
      "Dataframe images descriptors .. : 0 / Index([], dtype='object')\n",
      "KP filtering .................. : False\n",
      "Squarred images ............... : False\n",
      "Nb of breeds into sampling .... : 3\n",
      "Random image sampling ......... : False\n",
      "\n",
      "\n",
      "Breed directory= n02107142-Doberman              Breed name= Doberman\n",
      "Breed directory= n02115641-dingo                 Breed name= dingo\n",
      "Breed directory= n02113978-Mexican_hairless      Breed name= Mexican_hairless\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "import P7_DataBreed\n",
    "if True :\n",
    "    oP7_DataBreed = p5_util.object_load('./data/oP7_DataBreed.dump')\n",
    "    oP7_DataBreed.show()\n",
    "    oP7_DataBreed.breed_show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for images classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/arr_keras_X_y_train_test.dump\n",
      "(270, 224, 224, 3) (30, 224, 224, 3) (270, 1) (30, 1)\n",
      "(270, 224, 224, 3)\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "filename='./data/arr_keras_X_y_train_test.dump'\n",
    "(arr_X_train,arr_X_test, arr_y_train, arr_y_test) = p5_util.object_load(filename)\n",
    "print(arr_X_train.shape,arr_X_test.shape,arr_y_train.shape,arr_y_test.shape)\n",
    "\n",
    "#for X_train, y_train in zip(list_X_train, list_y_train):\n",
    "print(arr_X_train.shape)\n",
    "history = VGG16Seq.fit(arr_X_train, arr_y_train, epochs=10, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "is_score_dumped=True\n",
    "if is_score_dumped is True :\n",
    "    filename = './data/dict_cls_score.dump'\n",
    "    dict_cls_score = p5_util.object_load(filename)\n",
    "else:\n",
    "    dict_cls_score = dict()\n",
    "dict_classifier = dict()\n",
    "\n",
    "dict_cls_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = VGG16Seq.predict(arr_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= './data/result_VGG16Seq.dump'\n",
    "p5_util.object_dump(y_pred,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure of accuracy\n",
    "\n",
    "Result is stored into `arr_result` where :\n",
    "* rows : number of observations from test dataset\n",
    "* columns : probabibilty for each observation from train dataset.\n",
    "\n",
    "For each row from `arr_result`, greater probability value is searched for. \n",
    "<br>\n",
    "The result of search provides index of label into `list_index`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "filename= './data/result_vgg16.dump'\n",
    "p5_util.object_load(y_pred,filename)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# List of indexes greater probabilities: greater values for each row from arr_result.\n",
    "list_index = [np.where(y_pred[i] == max(y_pred[i]))[0][0] for i in range(0, len(y_pred))]\n",
    "\n",
    "#print(arr_result)\n",
    "print(len(arr_y_train))\n",
    "print(list_index)\n",
    "list_result = [arr_y_train[list_index[i]][0] for i in range(0,len(list_index))]\n",
    "\n",
    "score_keras = len(set(list_result).intersection(arr_y_test.flatten()))/len(list(arr_y_test))\n",
    "print(\"*** INFO : Result accuracy = {0:1.1F}%\".format(score_keras*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, score_vgg16_seq) = VGG16Seq.evaluate(arr_X_test, arr_y_test, verbose=True)\n",
    "\n",
    "score_vgg16_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import p5_util\n",
    "import p6_util_plot\n",
    "is_dumped=True\n",
    "if is_dumped is True:\n",
    "    filename = './data/dict_cls_score.dump'\n",
    "    dict_cls_score = p5_util.object_load(filename)\n",
    "else :\n",
    "    pass\n",
    "\n",
    "dict_cls_score['VGG16 Seq'] = score_vgg16_seq\n",
    "dict_benchmark_result = dict_cls_score.copy()\n",
    "\n",
    "df_result = pd.DataFrame.from_dict( dict_benchmark_result, orient='index')\n",
    "df_result.reset_index(inplace=True)\n",
    "df_result.rename(columns={'index':'Classifier',0:'Score'}, inplace=True)\n",
    "df_result\n",
    "nb_images = oP7_DataBreed._sampling_breed_count*oP7_DataBreed._sampling_image_per_breed_count\n",
    "nb_images = oP7_DataBreed.df_pil_image_kpdesc.shape[0]\n",
    "if oP7_DataBreed.is_kp_filtered :\n",
    "    title = \"Benchmark classifiers accuracy / GMM clustering / \"+str(nb_images)+\" filtered splitted images / \"+str(oP7_DataBreed.sampling_breed_count)+\" breeds\"\n",
    "else :\n",
    "    title = \"Benchmark classifiers accuracy / GMM clustering / \"+str(nb_images)+\" splitted images / \"+str(oP7_DataBreed.sampling_breed_count)+\" breeds\"\n",
    "\n",
    "p6_util_plot.ser_item_occurency_plot(df_result.Classifier, df_result.Score*100, item_count=None, title=title,\\\n",
    "                                    p_reverse=False,p_x_title='Classifiers', p_y_title='Accuracy')\n",
    "\n",
    "#### Classifier API\n",
    "p5_util.object_dump(dict_cls_score,filename)\n",
    "\n",
    "dict_cls_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oP7_DataBreed.show()\n",
    "oP7_DataBreed.show_breed_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
