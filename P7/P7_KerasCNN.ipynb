{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NOTEBOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"./figures/LogoOpenclassrooms.png\">\n",
    "<font size=\"4\">\n",
    "    \n",
    "Cette étude a été réalisée dans le cadre du 6ème projet de ma formation Datascientist dispensée en MOOC par \n",
    "\n",
    "<font color='blus'>Openclassrooms / écoles Centrale-Supélec</font>.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p></p><p></p><p></p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Le problème posé :**\n",
    "\n",
    "# <font color='blus'>Indexation d'images</font>\n",
    "\n",
    "Vous êtes bénévole pour l'association de protection des animaux de votre quartier. C'est d'ailleurs ainsi que vous avez trouvé votre compagnon idéal, Snooky. Du coup, vous vous demandez ce que vous pouvez faire en retour pour aider l'association.\n",
    "\n",
    "Vous apprenez, en discutant avec un bénévole, que leur base de données de pensionnaires commence à s'agrandir et qu'ils n'ont pas toujours le temps de référencer les images des animaux qu'ils ont accumulées depuis plusieurs années. Ils aimeraient donc réaliser un index de l’ensemble de la base de données d’images qu’ils possèdent, pour classer les chiens par races.\n",
    "\n",
    "**<font color='blus'>Les données</font>**\n",
    "\n",
    "Les bénévoles de l'association n'ont pas eu le temps de réunir les différentes images des pensionnaires dispersées sur leurs disques durs. Pas de problème, vous développerez un algorithme en utilisant le Stanford Dogs Dataset pour entraîner votre algorithme.\n",
    "\n",
    "**<font color='blus'>Votre mission</font>**\n",
    "\n",
    "En tant que Data Scientist, l'association vous demande de réaliser un algorithme de détection de la race du chien sur une photo, afin d'accélérer leur travail d’indexation.\n",
    "\n",
    "**<font color='blus'>Contraintes</font>**\n",
    "\n",
    "Lors de ce projet, vous mettrez en œuvre deux approches.\n",
    "\n",
    "* Une approche classique : il s’agit de pre-processer des images avec des techniques spécifiques (e.g.whitening, equalisation, filtre linéaire/laplacien/gaussien, éventuellement modifier la taille des images), puis d’extraire des features (e.g. texture, corners, edges et SIFT detector). Il faut ensuite réduire les dimensions, soit par des approches classiques (e.g. PCA, k-means) soit avec une approche par histogrammes et dictionary learning (bag-of-words appliqué aux images), puis appliquer des algorithmes de classification standards.\n",
    "\n",
    "\n",
    "\n",
    "* Lors de l’analyse exploratoire, vous regarderez si les features extraites et utilisées en classification sont prometteuses en utilisant des méthodes de réduction de dimension pour visualiser le dataset en 2D. Cela vous permettra d’affiner votre intuition sur les différents traitements possibles, sans que cela ne se substitue à des mesures de performances rigoureuses.\n",
    "\n",
    "\n",
    "\n",
    "* Une approche s’appuyant sur l’état de l’art et l’utilisation de CNN (réseaux de neurones convolutionnels). Compte tenu de la taille et de la complexité du dataset, et de la puissance de calcul à votre disposition, il est très difficile d’obtenir de bonnes performances (pour ça, essayez MNIST). Aussi, est-il recommandé d’utiliser le transfer learning, c’est-à-dire utiliser un réseau déjà entraîné, et le modifier pour répondre à votre problème. Une première chose obligatoire est de ré-entraîner les dernières couches pour prédire les classes qui vous intéressent seulement. Il est également possible d’adapter la structure (supprimer certaines couches par exemple) ou de ré-entraîner le modèle avec un très faible learning rate pour ajuster les poids à votre problème (plus long) et optimiser les performances.\n",
    "\n",
    "\n",
    "***Ce notebook est dédié à la conception et à l'étude d'un réseau CNN VGG16 non pré-entraîné***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bangui/.local/lib/python3.6/site-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import P7_DataBreed\n",
    "import p5_util\n",
    "\n",
    "from  sklearn import model_selection\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blus'>2. Data is loaded</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/arr_keras_X_y_train_test.dump\n",
      "(414, 224, 224, 3) (47, 224, 224, 3) (414, 3) (47, 3)\n",
      "Number of classes= 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import p5_util\n",
    "\n",
    "filename='./data/arr_keras_X_y_train_test.dump'\n",
    "(X_train,X_test, y_train, y_test) = p5_util.object_load(filename)\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "nClasses = y_train.shape[1]\n",
    "print(\"Number of classes= \"+str(nClasses))\n",
    "if False :\n",
    "    dimData = np.prod(X_train.shape[1:])\n",
    "    print(dimData)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], dimData)\n",
    "    X_test  = X_test.reshape(X_test.shape[0], dimData)\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/VGG16Seq_sgd.dump\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 54, 54, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 46656)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              46657000  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 3003      \n",
      "=================================================================\n",
      "Total params: 46,846,435\n",
      "Trainable params: 46,846,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "if True :\n",
    "    VGG16Seq = p5_util.object_load('./data/VGG16Seq_sgd.dump')\n",
    "\n",
    "    VGG16Seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blus'>1. Building CNN VGG16 network</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution layers arre added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 224 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "=================================================================\n",
      "Total params: 38,720\n",
      "Trainable params: 38,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "VGG16Seq = Sequential()  # Création d'un réseau de neurones vide \n",
    "\n",
    "# Ajout de la première couche de convolution, suivie d'une couche ReLU\n",
    "w=X_train.shape[1]\n",
    "h=X_train.shape[2]\n",
    "c=X_train.shape[3]\n",
    "print(w,h,c)\n",
    "VGG16Seq.add(Conv2D(64, (3, 3), input_shape=(w, h, c), padding='same', activation='relu'))\n",
    "VGG16Seq.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "VGG16Seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters computation through convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv 1 : parameters= 1792\n",
      "Conv 2 : parameters= 36928\n",
      "Total parameters = 38720\n"
     ]
    }
   ],
   "source": [
    "ntotal = 0\n",
    "# 1st convolution layer with 64 filters, each filter size is (64*64), 3 channels, 1 biais added per filter : \n",
    "n_param = 64*(3*3)*3+64\n",
    "ntotal += n_param\n",
    "print('Conv 1 : parameters= '+str(n_param))\n",
    "\n",
    "# 2nd convolution layer with 64 filters, each filter size is (64*64), 64 channels, 1 biais added per filter : \n",
    "n_param = 64*(3*3)*64+64\n",
    "ntotal += n_param\n",
    "print('Conv 2 : parameters= '+str(n_param))\n",
    "\n",
    "# 1st Pooling image sizes is divided per 2\n",
    "print('Total parameters = '+str(ntotal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooling: parameters= 3211264\n"
     ]
    }
   ],
   "source": [
    "#Pooling\n",
    "n_param = 224*224*64\n",
    "print('Pooling: parameters= '+str(n_param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd convolution layer is added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "=================================================================\n",
      "Total params: 112,576\n",
      "Trainable params: 112,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16Seq.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "# Ajout de la deuxième couche de convolution, suivie  d'une couche ReLU\n",
    "VGG16Seq.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "VGG16Seq.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "VGG16Seq.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "VGG16Seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters computation through convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv 1 : parameters= 1792\n",
      "Conv 2 : parameters= 36928\n",
      "Total parameters = 77440\n"
     ]
    }
   ],
   "source": [
    "# Cnvolution layer 3 : with 64 filters, each filter size is (64*64), 3 channels, 1 biais added per filter : \n",
    "n_param = 64*(3*3)*3+64\n",
    "ntotal += n_param\n",
    "print('Conv 1 : parameters= '+str(n_param))\n",
    "\n",
    "# 2nd convolution layer with 64 filters, each filter size is (64*64), 64 channels, 1 biais added per filter : \n",
    "n_param = 64*(3*3)*64+64\n",
    "ntotal += n_param\n",
    "print('Conv 2 : parameters= '+str(n_param))\n",
    "\n",
    "# 1st Pooling image sizes is divided per 2\n",
    "print('Total parameters = '+str(ntotal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3th convolution layer is added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 54, 54, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 64)        0         \n",
      "=================================================================\n",
      "Total params: 186,432\n",
      "Trainable params: 186,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Ajout de la deuxième couche de convolution, suivie  d'une couche ReLU\n",
    "VGG16Seq.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "VGG16Seq.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "VGG16Seq.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "VGG16Seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling layer is added "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st Dense layer is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16Seq.add(Flatten())  # Conversion des matrices de pixels 3D en vecteur 1D"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "VGG16Seq.add(Dense(4096, activation='relu'))\n",
    "#VGG16Seq.add(Dropout(0.5))\n",
    "VGG16Seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd Dense layer is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 54, 54, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 46656)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              46657000  \n",
      "=================================================================\n",
      "Total params: 46,843,432\n",
      "Trainable params: 46,843,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16Seq.add(Dense(1000, activation='relu'))\n",
    "#VGG16Seq.add(Dropout(0.5))\n",
    "VGG16Seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3third Dense layer is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 54, 54, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 46656)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              46657000  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 3003      \n",
      "=================================================================\n",
      "Total params: 46,846,435\n",
      "Trainable params: 46,846,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Ajout de la dernière couche fully-connected qui permet de classifier\n",
    "VGG16Seq.add(Dense(nClasses, activation='softmax'))\n",
    "VGG16Seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a saved CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/VGG16Seq_sgd.dump\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 54, 54, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 46656)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              46657000  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 3003      \n",
      "=================================================================\n",
      "Total params: 46,846,435\n",
      "Trainable params: 46,846,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "if True :\n",
    "    VGG16Seq = p5_util.object_load('./data/VGG16Seq_sgd.dump')\n",
    "\n",
    "    VGG16Seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16 compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=5e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsprop = optimizers.RMSprop(lr=1e-4)\n",
    "\n",
    "#my_VGG16.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#my_VGG16.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "VGG16Seq.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Sequential model is dumped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 54, 54, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 46656)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1000)              46657000  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 3003      \n",
      "=================================================================\n",
      "Total params: 46,846,435\n",
      "Trainable params: 46,846,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "p5_util.object_dump(VGG16Seq,'./data/VGG16Seq_sgd.dump')\n",
    "\n",
    "VGG16Seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blus'>3. CNN evaluation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(414, 224, 224, 3)\n",
      "Train on 414 samples, validate on 47 samples\n",
      "Epoch 1/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 1.0975 - acc: 0.3768 - val_loss: 1.0968 - val_acc: 0.4681\n",
      "Epoch 2/50\n",
      "414/414 [==============================] - 38s 92ms/step - loss: 1.0944 - acc: 0.5193 - val_loss: 1.0911 - val_acc: 0.4681\n",
      "Epoch 3/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 1.0898 - acc: 0.4444 - val_loss: 1.0824 - val_acc: 0.4255\n",
      "Epoch 4/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 1.0815 - acc: 0.4348 - val_loss: 1.0604 - val_acc: 0.5532\n",
      "Epoch 5/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 1.0536 - acc: 0.5217 - val_loss: 1.0039 - val_acc: 0.5745\n",
      "Epoch 6/50\n",
      "414/414 [==============================] - 38s 92ms/step - loss: 1.0122 - acc: 0.4614 - val_loss: 1.6811 - val_acc: 0.2766\n",
      "Epoch 7/50\n",
      "414/414 [==============================] - 38s 91ms/step - loss: 1.0893 - acc: 0.4952 - val_loss: 0.9289 - val_acc: 0.6170\n",
      "Epoch 8/50\n",
      "414/414 [==============================] - 38s 91ms/step - loss: 0.9499 - acc: 0.5459 - val_loss: 0.8573 - val_acc: 0.6809\n",
      "Epoch 9/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 0.8578 - acc: 0.6135 - val_loss: 0.8118 - val_acc: 0.6383\n",
      "Epoch 10/50\n",
      "414/414 [==============================] - 39s 95ms/step - loss: 0.8411 - acc: 0.5894 - val_loss: 0.8919 - val_acc: 0.5319\n",
      "Epoch 11/50\n",
      "414/414 [==============================] - 38s 91ms/step - loss: 0.8093 - acc: 0.6063 - val_loss: 0.7681 - val_acc: 0.6809\n",
      "Epoch 12/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 0.7503 - acc: 0.6546 - val_loss: 0.7678 - val_acc: 0.6809\n",
      "Epoch 13/50\n",
      "414/414 [==============================] - 36s 88ms/step - loss: 0.7733 - acc: 0.6691 - val_loss: 0.7405 - val_acc: 0.6383\n",
      "Epoch 14/50\n",
      "414/414 [==============================] - 38s 91ms/step - loss: 0.7170 - acc: 0.6667 - val_loss: 0.7856 - val_acc: 0.5745\n",
      "Epoch 15/50\n",
      "414/414 [==============================] - 40s 96ms/step - loss: 0.6928 - acc: 0.7029 - val_loss: 0.6425 - val_acc: 0.7447\n",
      "Epoch 16/50\n",
      "414/414 [==============================] - 38s 93ms/step - loss: 0.7684 - acc: 0.6691 - val_loss: 0.6815 - val_acc: 0.8085\n",
      "Epoch 17/50\n",
      "414/414 [==============================] - 38s 92ms/step - loss: 0.6652 - acc: 0.7005 - val_loss: 0.6408 - val_acc: 0.7447\n",
      "Epoch 18/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 0.6444 - acc: 0.7029 - val_loss: 0.8061 - val_acc: 0.6383\n",
      "Epoch 19/50\n",
      "414/414 [==============================] - 38s 92ms/step - loss: 0.6468 - acc: 0.7126 - val_loss: 0.6470 - val_acc: 0.7660\n",
      "Epoch 20/50\n",
      "414/414 [==============================] - 39s 93ms/step - loss: 0.5797 - acc: 0.7729 - val_loss: 1.0340 - val_acc: 0.6383\n",
      "Epoch 21/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 0.7055 - acc: 0.6981 - val_loss: 0.7293 - val_acc: 0.7447\n",
      "Epoch 22/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 0.6132 - acc: 0.7295 - val_loss: 0.6486 - val_acc: 0.7447\n",
      "Epoch 23/50\n",
      "414/414 [==============================] - 38s 92ms/step - loss: 0.5209 - acc: 0.8019 - val_loss: 0.7165 - val_acc: 0.7021\n",
      "Epoch 24/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 0.5235 - acc: 0.7585 - val_loss: 0.7239 - val_acc: 0.7021\n",
      "Epoch 25/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 0.4660 - acc: 0.8068 - val_loss: 0.7776 - val_acc: 0.7021\n",
      "Epoch 26/50\n",
      "414/414 [==============================] - 38s 91ms/step - loss: 0.5909 - acc: 0.7633 - val_loss: 1.1318 - val_acc: 0.6170\n",
      "Epoch 27/50\n",
      "414/414 [==============================] - 40s 96ms/step - loss: 0.4753 - acc: 0.7874 - val_loss: 0.6837 - val_acc: 0.7021\n",
      "Epoch 28/50\n",
      "414/414 [==============================] - 37s 91ms/step - loss: 0.4232 - acc: 0.8527 - val_loss: 0.7338 - val_acc: 0.7660\n",
      "Epoch 29/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 0.4069 - acc: 0.8213 - val_loss: 0.6998 - val_acc: 0.7021\n",
      "Epoch 30/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 0.4693 - acc: 0.8043 - val_loss: 0.8039 - val_acc: 0.5957\n",
      "Epoch 31/50\n",
      "414/414 [==============================] - 37s 91ms/step - loss: 0.3736 - acc: 0.8913 - val_loss: 0.9466 - val_acc: 0.7234\n",
      "Epoch 32/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 0.9481 - acc: 0.8019 - val_loss: 1.1336 - val_acc: 0.2128\n",
      "Epoch 33/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 1.1109 - acc: 0.3140 - val_loss: 1.1058 - val_acc: 0.3830\n",
      "Epoch 34/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 1.0999 - acc: 0.3478 - val_loss: 1.1122 - val_acc: 0.2128\n",
      "Epoch 35/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 1.1062 - acc: 0.3671 - val_loss: 1.1180 - val_acc: 0.2128\n",
      "Epoch 36/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 1.0810 - acc: 0.3768 - val_loss: 1.0419 - val_acc: 0.5957\n",
      "Epoch 37/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 1.0533 - acc: 0.5145 - val_loss: 1.0466 - val_acc: 0.3617\n",
      "Epoch 38/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 0.9945 - acc: 0.5121 - val_loss: 0.9483 - val_acc: 0.5319\n",
      "Epoch 39/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 1.0324 - acc: 0.4493 - val_loss: 0.9572 - val_acc: 0.4894\n",
      "Epoch 40/50\n",
      "414/414 [==============================] - 37s 88ms/step - loss: 0.9271 - acc: 0.5459 - val_loss: 0.9112 - val_acc: 0.5532\n",
      "Epoch 41/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 0.8559 - acc: 0.6135 - val_loss: 0.8732 - val_acc: 0.6383\n",
      "Epoch 42/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 0.8264 - acc: 0.6329 - val_loss: 1.0078 - val_acc: 0.4681\n",
      "Epoch 43/50\n",
      "414/414 [==============================] - 38s 92ms/step - loss: 0.8000 - acc: 0.6280 - val_loss: 0.8160 - val_acc: 0.5957\n",
      "Epoch 44/50\n",
      "414/414 [==============================] - 37s 90ms/step - loss: 0.7954 - acc: 0.6256 - val_loss: 1.0424 - val_acc: 0.5319\n",
      "Epoch 45/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 0.7522 - acc: 0.6739 - val_loss: 0.7661 - val_acc: 0.6809\n",
      "Epoch 46/50\n",
      "414/414 [==============================] - 38s 93ms/step - loss: 0.6684 - acc: 0.7198 - val_loss: 1.1797 - val_acc: 0.5106\n",
      "Epoch 47/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 1.4958 - acc: 0.4928 - val_loss: 1.0926 - val_acc: 0.2979\n",
      "Epoch 48/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 1.0662 - acc: 0.5266 - val_loss: 1.0335 - val_acc: 0.5745\n",
      "Epoch 49/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 1.0585 - acc: 0.4976 - val_loss: 1.0098 - val_acc: 0.6383\n",
      "Epoch 50/50\n",
      "414/414 [==============================] - 37s 89ms/step - loss: 1.0100 - acc: 0.5942 - val_loss: 0.9838 - val_acc: 0.6383\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "history = VGG16Seq.fit(X_train, y_train, epochs=50, batch_size=90,verbose=1,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "[test_loss, test_acc] = VGG16Seq.evaluate(X_test, y_test)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))\n",
    "\n",
    "#Plot the Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['loss'],'r',linewidth=1.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=1.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves ',fontsize=16)\n",
    " \n",
    "#Plot the Accuracy Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['acc'],'r',linewidth=1.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=1.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p5_util\n",
    "filename = './data/vgg16_cnn.dump'\n",
    "p5_util.object_dump(VGG16Seq,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign CNN model to `oP7_DataBreed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bangui/.local/lib/python3.6/site-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p5_util.object_load : fileName= ./data/oP7_DataBreed.dump\n",
      "p5_util.object_load : fileName= ./data/vgg16_cnn.dump\n",
      "\n",
      " \n",
      "Path to data directory ........ : ./data/Images\n",
      "Number of original breeds ..... : 3\n",
      "Total number of images ........ : 461\n",
      "Standard images size .......... : (224, 224)\n",
      "SIFT Image descriptors count .. : 0\n",
      "Number of images in sample .... : 0\n",
      "Sampling : breeds count ....... : 3\n",
      "Sampling : images per breed ... : 0\n",
      "X train size .................. : (368, 3)\n",
      "y train size .................. : (368, 3)\n",
      "X test size ................... : (93, 3)\n",
      "y test size ................... : (93, 3)\n",
      "Clusters models  .............. : dict_keys(['GMM'])\n",
      "Current cluster model  ........ : GMM\n",
      "Bag of features dataframe ..... : (461, 3)\n",
      "Encoded labels from dataset ... : (461, 3)\n",
      "Number of breeds in sample .... : 3\n",
      "Image splitted ................ : False\n",
      "Key point descriptors ......... : (30949, 128)\n",
      "Classifier name ............... : Random Forest\n",
      "Supported classifiers ......... : ['Bernoulli NB', 'Random Forest', 'L.R. + multinomial', 'Gaussian NB', 'Linear SVC', 'KR Classifier']\n",
      "Number of restricted images ... : 0\n",
      "Splitted parts ................ : (1, 1)\n",
      "Dataframe images descriptors .. : 461 / Index(['desc', 'breed', 'kp', 'size', 'split_image', 'image_id'], dtype='object')\n",
      "KP filtering .................. : False\n",
      "Squarred images ............... : False\n",
      "Nb of breeds into sampling .... : 0\n",
      "Random image sampling ......... : True\n",
      "Assigned filters identifiers .. : [3, 2, 11, 6]\n",
      "\n",
      "Assigned filters list ......... : \n",
      "Identifier : 3   Filter= pil_2gray\n",
      "Identifier : 2   Filter= p7_filter_median\n",
      "Identifier : 11   Filter= pil_edge_only\n",
      "Identifier : 6   Filter= pil_gaussian\n",
      "\n",
      "Assignable filters list ....... : \n",
      "Identifier : 0   Filter= pil_square\n",
      "Identifier : 1   Filter= pil_edge\n",
      "Identifier : 2   Filter= p7_filter_median\n",
      "Identifier : 3   Filter= pil_2gray\n",
      "Identifier : 4   Filter= pil_autocontrast\n",
      "Identifier : 5   Filter= pil_equalize\n",
      "Identifier : 6   Filter= pil_gaussian\n",
      "Identifier : 7   Filter= pil_gradient\n",
      "Identifier : 8   Filter= pil_laplacien_kernel_4\n",
      "Identifier : 9   Filter= pil_laplacien_kernel_8\n",
      "Identifier : 10   Filter= pil_low_pass\n",
      "Identifier : 11   Filter= pil_edge_only\n",
      "\n",
      "Images processed count ........ : 461\n",
      "List of selected clusters ..... : []\n",
      "PCA components ................ : None\n",
      "\n",
      "Neural Network supported models ... : \n",
      "mlp\n",
      "cnn\n",
      "\n",
      "Activated Neural Network model .. : cnn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import p5_util\n",
    "import P7_DataBreed\n",
    "\n",
    "filename= './data/vgg16_cnn.dump'\n",
    "if False :\n",
    "    p5_util.object_dump(mlp_model_drop, filename)\n",
    "    oP7_DataBreed.mlp_model = mlp_model_drop\n",
    "else : \n",
    "    oP7_DataBreed = p5_util.object_load('./data/oP7_DataBreed.dump')\n",
    "    oP7_DataBreed.nn_model = p5_util.object_load(filename)\n",
    "    \n",
    "\n",
    "oP7_DataBreed.nn_model_name = 'cnn'\n",
    "p5_util.object_dump(oP7_DataBreed, './data/oP7_DataBreed.dump')\n",
    "oP7_DataBreed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
