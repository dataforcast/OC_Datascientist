{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import adanet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) 3\n"
     ]
    }
   ],
   "source": [
    "import adanet\n",
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = (tf.keras.datasets.mnist.load_data())\n",
    "print(y_train.shape, y_train[10])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import p8_util\n",
    "import p8_util_config\n",
    "datadir = './data'\n",
    "\n",
    "filename_dataset=datadir+'/arr_keras_X_y_train_test.dump'\n",
    "x_train, x_test, y_train, y_test, nClasses, feature_shape = p8_util.load_dataset(filename_dataset)\n",
    "print(x_train.shape, x_test.shape, feature_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_KEY = \"images\"\n",
    "\n",
    "\n",
    "def generator(images, labels):\n",
    "  \"\"\"Returns a generator that returns image-label pairs.\"\"\"\n",
    "\n",
    "  def _gen():\n",
    "    for image, label in zip(images, labels):\n",
    "      yield image, label\n",
    "\n",
    "  return _gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, label):\n",
    "  \"\"\"Preprocesses an image for an `Estimator`.\"\"\"\n",
    "  # First let's scale the pixel values to be between 0 and 1.\n",
    "  image = image / 255.\n",
    "  # Next we reshape the image so that we can apply a 2D convolution to it.\n",
    "  image = tf.reshape(image, [28, 28, 1])\n",
    "  # Finally the features need to be supplied as a dictionary.\n",
    "  print(\"\\n*** preprocess_image() : label shape= {}\".format(label.shape))\n",
    "  features = {FEATURES_KEY: image}\n",
    "  return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_color_image(image, label):\n",
    "  \"\"\"Preprocesses an image for an `Estimator`.\"\"\"\n",
    "  # First let's scale the pixel values to be between 0 and 1.\n",
    "  #image = image / 255.\n",
    "  # Next we reshape the image so that we can apply a 2D convolution to it.\n",
    "  #image = tf.reshape(image, [28, 28, 1])\n",
    "  # Finally the features need to be supplied as a dictionary.\n",
    "  print(\"\\n*** preprocess_color_image() : label shape= {}\".format(label.shape))\n",
    "  features = {FEATURES_KEY: image}\n",
    "  return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(partition, training, batch_size):\n",
    "  \"\"\"Generate an input_fn for the Estimator.\"\"\"\n",
    "\n",
    "  def _input_fn():\n",
    "    if True :\n",
    "        if partition == \"train\":\n",
    "          dataset = tf.data.Dataset.from_generator(\n",
    "              generator(x_train, y_train), (tf.float32, tf.int32), ((28, 28), ()))\n",
    "        else:\n",
    "          dataset = tf.data.Dataset.from_generator(\n",
    "              generator(x_test, y_test), (tf.float32, tf.int32), ((28, 28), ()))\n",
    "    else :\n",
    "        if partition == \"train\":\n",
    "          dataset = tf.data.Dataset.from_generator(\n",
    "              generator(x_train, y_train), (tf.float32, tf.int32), ((224, 224, 3), ()))\n",
    "        else:\n",
    "          dataset = tf.data.Dataset.from_generator(\n",
    "              generator(x_test, y_test), (tf.float32, tf.int32), ((224, 224, 3), ()))\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    if training:\n",
    "      dataset = dataset.shuffle(10 * batch_size, seed=RANDOM_SEED).repeat()\n",
    "    if False :\n",
    "        dataset = dataset.map(preprocess_image).batch(batch_size)\n",
    "    else :\n",
    "        dataset = dataset.map(preprocess_color_image).batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels\n",
    "\n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of classes.\n",
    "NUM_CLASSES = 10\n",
    "RANDOM_SEED = 42\n",
    "# We will average the losses in each mini-batch when computing gradients.\n",
    "loss_reduction = tf.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
    "\n",
    "# A `Head` instance defines the loss function and metrics for `Estimators`.\n",
    "head = tf.contrib.estimator.multi_class_head(\n",
    "    NUM_CLASSES, loss_reduction=loss_reduction)\n",
    "\n",
    "# Some `Estimators` use feature columns for understanding their input features.\n",
    "feature_columns = [\n",
    "    tf.feature_column.numeric_column(FEATURES_KEY, shape=[28, 28, 1])\n",
    "]\n",
    "\n",
    "# Estimator configuration.\n",
    "config = tf.estimator.RunConfig(\n",
    "    save_checkpoints_steps=50000,\n",
    "    save_summary_steps=50000,\n",
    "    tf_random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "#@title Parameters\n",
    "LEARNING_RATE = 0.001  #@param {type:\"number\"}\n",
    "TRAIN_STEPS = 5000  #@param {type:\"integer\"}\n",
    "BATCH_SIZE = 64  #@param {type:\"integer\"}\n",
    "if False :\n",
    "    estimator = tf.estimator.LinearClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        n_classes=NUM_CLASSES,\n",
    "        optimizer=tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE),\n",
    "        loss_reduction=loss_reduction,\n",
    "        config=config)\n",
    "\n",
    "    results, _ = tf.estimator.train_and_evaluate(\n",
    "        estimator,\n",
    "        train_spec=tf.estimator.TrainSpec(\n",
    "            input_fn=input_fn(\"train\", training=True, batch_size=BATCH_SIZE),\n",
    "            max_steps=TRAIN_STEPS),\n",
    "        eval_spec=tf.estimator.EvalSpec(\n",
    "            input_fn=input_fn(\"test\", training=False, batch_size=BATCH_SIZE),\n",
    "            steps=None))\n",
    "    print(\"Accuracy:\", results[\"accuracy\"])\n",
    "    print(\"Loss:\", results[\"average_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNNBuilder(adanet.subnetwork.Builder):\n",
    "  \"\"\"Builds a CNN subnetwork for AdaNet.\"\"\"\n",
    "\n",
    "  def __init__(self, learning_rate, max_iteration_steps, seed):\n",
    "    \"\"\"Initializes a `SimpleCNNBuilder`.\n",
    "\n",
    "    Args:\n",
    "      learning_rate: The float learning rate to use.\n",
    "      max_iteration_steps: The number of steps per iteration.\n",
    "      seed: The random seed.\n",
    "\n",
    "    Returns:\n",
    "      An instance of `SimpleCNNBuilder`.\n",
    "    \"\"\"\n",
    "    self._learning_rate = learning_rate\n",
    "    self._max_iteration_steps = max_iteration_steps\n",
    "    self._seed = seed\n",
    "\n",
    "  def build_subnetwork(self,\n",
    "                       features,\n",
    "                       logits_dimension,\n",
    "                       training,\n",
    "                       iteration_step,\n",
    "                       summary,\n",
    "                       previous_ensemble=None):\n",
    "    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n",
    "    images = list(features.values())[0]\n",
    "    print(images)\n",
    "    kernel_initializer = tf.keras.initializers.he_normal(seed=self._seed)\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    input_shape=(1,28,28))(\n",
    "            images)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(\n",
    "        units=64, activation=\"relu\", kernel_initializer=kernel_initializer)(\n",
    "            x)\n",
    "\n",
    "    # The `Head` passed to adanet.Estimator will apply the softmax activation.\n",
    "    logits = tf.keras.layers.Dense(\n",
    "        units=10, activation=None, kernel_initializer=kernel_initializer)(\n",
    "            x)\n",
    "\n",
    "    # Use a constant complexity measure, since all subnetworks have the same\n",
    "    # architecture and hyperparameters.\n",
    "    complexity = tf.constant(1)\n",
    "\n",
    "    return adanet.Subnetwork(\n",
    "        last_layer=x,\n",
    "        logits=logits,\n",
    "        complexity=complexity,\n",
    "        persisted_tensors={})\n",
    "\n",
    "  def build_subnetwork_train_op(self,\n",
    "                                subnetwork,\n",
    "                                loss,\n",
    "                                var_list,\n",
    "                                labels,\n",
    "                                iteration_step,\n",
    "                                summary,\n",
    "                                previous_ensemble=None):\n",
    "    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n",
    "\n",
    "    # Momentum optimizer with cosine learning rate decay works well with CNNs.\n",
    "    learning_rate = tf.train.cosine_decay(\n",
    "        learning_rate=self._learning_rate,\n",
    "        global_step=iteration_step,\n",
    "        decay_steps=self._max_iteration_steps)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, .9)\n",
    "    # NOTE: The `adanet.Estimator` increments the global step.\n",
    "    return optimizer.minimize(loss=loss, var_list=var_list)\n",
    "\n",
    "  def build_mixture_weights_train_op(self, loss, var_list, logits, labels,\n",
    "                                     iteration_step, summary):\n",
    "    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n",
    "    return tf.no_op(\"mixture_weights_train_op\")\n",
    "\n",
    "  @property\n",
    "  def name(self):\n",
    "    \"\"\"See `adanet.subnetwork.Builder`.\"\"\"\n",
    "    return \"simple_cnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "class SimpleCNNGenerator(adanet.subnetwork.Generator):\n",
    "  \"\"\"Generates a `SimpleCNN` at each iteration.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, learning_rate, max_iteration_steps, seed=None):\n",
    "    \"\"\"Initializes a `Generator` that builds `SimpleCNNs`.\n",
    "\n",
    "    Args:\n",
    "      learning_rate: The float learning rate to use.\n",
    "      max_iteration_steps: The number of steps per iteration.\n",
    "      seed: The random seed.\n",
    "\n",
    "    Returns:\n",
    "      An instance of `Generator`.\n",
    "    \"\"\"\n",
    "    self._seed = seed\n",
    "    self._dnn_builder_fn = functools.partial(\n",
    "        SimpleCNNBuilder,\n",
    "        learning_rate=learning_rate,\n",
    "        max_iteration_steps=max_iteration_steps)\n",
    "\n",
    "  def generate_candidates(self, previous_ensemble, iteration_number,\n",
    "                          previous_ensemble_reports, all_reports):\n",
    "    \"\"\"See `adanet.subnetwork.Generator`.\"\"\"\n",
    "    seed = self._seed\n",
    "    # Change the seed according to the iteration so that each subnetwork\n",
    "    # learns something different.\n",
    "    if seed is not None:\n",
    "      seed += iteration_number\n",
    "    return [self._dnn_builder_fn(seed=seed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmppmaksmsf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0522 18:13:26.182114 139625172219712 estimator.py:1760] Using temporary folder as model directory: /tmp/tmppmaksmsf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmppmaksmsf', '_tf_random_seed': 42, '_save_summary_steps': 50000, '_save_checkpoints_steps': 50000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7efc97840a58>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 18:13:26.183158 139625172219712 estimator.py:201] Using config: {'_model_dir': '/tmp/tmppmaksmsf', '_tf_random_seed': 42, '_save_summary_steps': 50000, '_save_checkpoints_steps': 50000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7efc97840a58>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 18:13:26.184676 139625172219712 estimator_training.py:185] Not using Distribute Coordinator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 18:13:26.185400 139625172219712 training.py:610] Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 50000 or save_checkpoints_secs None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 18:13:26.186097 139625172219712 training.py:698] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 50000 or save_checkpoints_secs None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** preprocess_color_image() : label shape= ()\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 18:13:26.279169 139625172219712 estimator.py:1111] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 18:13:26.281203 139625172219712 iteration.py:250] Building iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building subnetwork 'simple_cnn'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0522 18:13:26.291087 139625172219712 iteration.py:337] Building subnetwork 'simple_cnn'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext:0\", shape=(?, 28, 28), dtype=float32, device=/device:CPU:0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 28, 28]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9f72d27cf659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     eval_spec=tf.estimator.EvalSpec(\n\u001b[1;32m     27\u001b[0m         \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         steps=None))\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"average_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[1;32m    469\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[1;32m    610\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;31m# Distributed case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\u001b[0m in \u001b[0;36mrun_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_hooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m         saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     eval_result = listener_for_eval.eval_result or _EvalResult(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/adanet/core/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decorate_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             saving_listeners=saving_listeners)\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished training Adanet iteration %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1152\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1154\u001b[0;31m           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m   1155\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/adanet/core/estimator.py\u001b[0m in \u001b[0;36m_adanet_model_fn\u001b[0;34m(self, features, labels, mode, params)\u001b[0m\n\u001b[1;32m   1452\u001b[0m           \u001b[0mprevious_ensemble_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprevious_ensemble_summary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           \u001b[0mprevious_ensemble_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprevious_ensemble_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m           skip_summaries=True)\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;31m# Variable which allows us to read the current iteration from a checkpoint.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/adanet/core/iteration.py\u001b[0m in \u001b[0;36mbuild_iteration\u001b[0;34m(self, iteration_number, ensemble_candidates, subnetwork_builders, features, mode, labels, previous_ensemble_summary, previous_ensemble_spec, skip_summaries, rebuilding)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuilder_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             previous_ensemble=previous_ensemble)\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0msubnetwork_specs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubnetwork_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         if not self._placement_strategy.should_build_ensemble(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/adanet/core/ensemble_builder.py\u001b[0m in \u001b[0;36mbuild_subnetwork_spec\u001b[0;34m(self, name, subnetwork_builder, iteration_step, summary, features, mode, labels, previous_ensemble)\u001b[0m\n\u001b[1;32m    537\u001b[0m           \u001b[0mscoped_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m           trainable_vars=[]):\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0msubnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_subnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m       \u001b[0msubnetwork_var_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_new_trainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore_var_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-8a5fb1c491ae>\u001b[0m in \u001b[0;36mbuild_subnetwork\u001b[0;34m(self, features, logits_dimension, training, iteration_step, summary, previous_ensemble)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             images)\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m     input_spec.assert_input_compatibility(\n\u001b[0;32m-> 1591\u001b[0;31m         self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   1592\u001b[0m     \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 28, 28]"
     ]
    }
   ],
   "source": [
    "#@title Parameters\n",
    "LEARNING_RATE = 0.05  #@param {type:\"number\"}\n",
    "TRAIN_STEPS = 5000  #@param {type:\"integer\"}\n",
    "BATCH_SIZE = 64  #@param {type:\"integer\"}\n",
    "ADANET_ITERATIONS = 2  #@param {type:\"integer\"}\n",
    "\n",
    "max_iteration_steps = TRAIN_STEPS // ADANET_ITERATIONS\n",
    "estimator = adanet.Estimator(\n",
    "    head=head,\n",
    "    subnetwork_generator=SimpleCNNGenerator(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        max_iteration_steps=max_iteration_steps,\n",
    "        seed=RANDOM_SEED),\n",
    "    max_iteration_steps=max_iteration_steps,\n",
    "    evaluator=adanet.Evaluator(\n",
    "        input_fn=input_fn(\"train\", training=False, batch_size=BATCH_SIZE),\n",
    "        steps=None),\n",
    "    adanet_loss_decay=.99,\n",
    "    config=config)\n",
    "\n",
    "results, _ = tf.estimator.train_and_evaluate(\n",
    "    estimator,\n",
    "    train_spec=tf.estimator.TrainSpec(\n",
    "        input_fn=input_fn(\"train\", training=True, batch_size=BATCH_SIZE),\n",
    "        max_steps=TRAIN_STEPS),\n",
    "    eval_spec=tf.estimator.EvalSpec(\n",
    "        input_fn=input_fn(\"test\", training=False, batch_size=BATCH_SIZE),\n",
    "        steps=None))\n",
    "print(\"Accuracy:\", results[\"accuracy\"])\n",
    "print(\"Loss:\", results[\"average_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
